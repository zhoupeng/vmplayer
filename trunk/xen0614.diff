diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/extras/mini-os/domain_config new/xen-3.3.0/extras/mini-os/domain_config
*** xen-3.3.0/extras/mini-os/domain_config	2008-08-22 05:49:07.000000000 -0400
--- new/xen-3.3.0/extras/mini-os/domain_config	2009-06-12 10:15:19.000000000 -0400
*************** kernel = "mini-os.gz"
*** 13,18 ****
--- 13,21 ----
  # Initial memory allocation (in megabytes) for the new domain.
  memory = 32
  
+ #ifconfig
+ vif = [ 'ip=192.168.1.252' ]
+ 
  # A name for your domain. All domains must have different names.
  name = "Mini-OS"
  
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/extras/mini-os/include/xen/xen.h new/xen-3.3.0/extras/mini-os/include/xen/xen.h
*** xen-3.3.0/extras/mini-os/include/xen/xen.h	2008-08-22 05:49:10.000000000 -0400
--- new/xen-3.3.0/extras/mini-os/include/xen/xen.h	2009-06-01 09:42:26.000000000 -0400
*************** DEFINE_XEN_GUEST_HANDLE(xen_pfn_t);
*** 91,96 ****
--- 91,100 ----
  #define __HYPERVISOR_sysctl               35
  #define __HYPERVISOR_domctl               36
  #define __HYPERVISOR_kexec_op             37
+ /* for record and replay op*/
+ #define __HYPERVISOR_my_hyper_op           38
+ #define __HYPERVISOR_replay_op               39
+ 
  
  /* Architecture-specific hypercall definitions. */
  #define __HYPERVISOR_arch_0               48
Binary files xen-3.3.0/extras/mini-os/mini-os and new/xen-3.3.0/extras/mini-os/mini-os differ
Binary files xen-3.3.0/.hg/00changelog.i and new/xen-3.3.0/.hg/00changelog.i differ
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/.hg/requires new/xen-3.3.0/.hg/requires
*** xen-3.3.0/.hg/requires	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/.hg/requires	2009-06-01 10:55:36.000000000 -0400
***************
*** 0 ****
--- 1,2 ----
+ revlogv1
+ store
Binary files xen-3.3.0/.Makefile.swp and new/xen-3.3.0/.Makefile.swp differ
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/stubdom/include/xen/xen.h new/xen-3.3.0/stubdom/include/xen/xen.h
*** xen-3.3.0/stubdom/include/xen/xen.h	2008-08-22 05:49:10.000000000 -0400
--- new/xen-3.3.0/stubdom/include/xen/xen.h	2009-06-01 09:42:26.000000000 -0400
*************** DEFINE_XEN_GUEST_HANDLE(xen_pfn_t);
*** 91,96 ****
--- 91,100 ----
  #define __HYPERVISOR_sysctl               35
  #define __HYPERVISOR_domctl               36
  #define __HYPERVISOR_kexec_op             37
+ /* for record and replay op*/
+ #define __HYPERVISOR_my_hyper_op           38
+ #define __HYPERVISOR_replay_op               39
+ 
  
  /* Architecture-specific hypercall definitions. */
  #define __HYPERVISOR_arch_0               48
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/libxc/MK new/xen-3.3.0/tools/libxc/MK
*** xen-3.3.0/tools/libxc/MK	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/libxc/MK	2009-06-01 17:21:08.000000000 -0400
***************
*** 0 ****
--- 1,10 ----
+ CC=gcc
+ record:record.c replay.h record.h
+ 	cp replay.h /usr/include
+ 	cp replay.h /root/xen-3.3.0/xen/include/xen
+ 	cp replay.h /lib/modules/2.6.18.8-xen/source/include
+ 	$(CC) -lxenctrl -g -o record record.c
+ 	cp record /usr/bin
+ .PHONY:clean
+ clean:
+ 	rm record
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/libxc/README new/xen-3.3.0/tools/libxc/README
*** xen-3.3.0/tools/libxc/README	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/libxc/README	2009-05-26 15:57:53.000000000 -0400
***************
*** 0 ****
--- 1,7 ----
+ xen states and events log and reexecute
+ //Function:log system states at a certain timepoint,then log event during system execution. Replay events when necessary.
+ insmod record_module.ko
+ dmesg look up MAJOR
+ mknod /dev/module c MAJOR 0
+ 
+ record domid
Binary files xen-3.3.0/tools/libxc/record and new/xen-3.3.0/tools/libxc/record differ
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/libxc/record.c new/xen-3.3.0/tools/libxc/record.c
*** xen-3.3.0/tools/libxc/record.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/libxc/record.c	2009-06-12 18:07:45.000000000 -0400
***************
*** 0 ****
--- 1,859 ----
+ #include <sys/types.h>
+ #include <sys/fcntl.h>
+ #include <sys/ioctl.h>
+ #include <sys/socket.h>
+ #include <netinet/in.h>
+ #include <assert.h>
+ #include <err.h>
+ #include <printf.h>
+ #include <stdio.h>
+ #include <unistd.h>
+ #include <sched.h>
+ #include <sys/resource.h>
+ #include <time.h>
+ #include <xenctrl.h>
+ #include <stdlib.h>
+ 
+ #include "record.h"
+ 
+ #include <xen/event_channel.h>
+ 
+ typedef uint16_t domid_t;
+ static int       recordxend;
+ int              record_fd;
+ static int       xc_handle;
+ static unsigned  dom_netif_handle;
+ //typedef uint32_t u32;
+ static netif_tx_interface_t *dom_tx_ring;
+ static netif_rx_interface_t *dom_rx_ring;
+ static unsigned last_tx_ring_rsp,last_rx_ring_rsp;
+ unsigned int guest_width;
+ 
+ static xen_pfn_t *live_m2p = NULL;
+ static xen_pfn_t *live_p2m = NULL;
+ 
+ static struct record_event_ring *event_ring;
+ static unsigned int *page_list;
+ static void *queue_area;
+ 
+ #define pfn_to_mfn(_pfn)                                            \
+    ((xen_pfn_t) ((guest_width==8)                               \
+                ? (((uint64_t *)live_p2m)[(_pfn)])                  \
+                 : ((((uint32_t *)live_p2m)[(_pfn)]) == 0xffffffffU  \
+                     ? (-1UL) : (((uint32_t *)live_p2m)[(_pfn)]))))
+ 
+ int evtfd;
+ 
+ static unsigned event_port = 0;
+ 
+ static int _evtchn_bind(int _evtfd, int idx)
+ {
+     if ( ioctl(_evtfd, EVTCHN_BIND, idx) != 0 ) {
+         return -1;
+     }
+     
+     return 0;
+ }
+ 
+ int evtchn_bind(int idx) {
+     return _evtchn_bind(evtfd, idx);
+ }
+ 
+ static int do_replay_op(replay_op_t *op) {
+   int rc;
+   printf("domid:%d,port:%d,nr_pages:%d\n",op->u.recop.dom,
+ 				  op->u.recop.u.rec_init.port,
+ 				  op->u.recop.u.rec_init.ring_mfn);
+ 
+   mlock(op,sizeof(*op));
+   asm("int $0x80"
+       : "=a" (rc)
+       : "0" (297), "b" (op));
+   munlock(op,sizeof(*op));
+   printf("rc = %d\n",rc);
+   return rc;
+ }
+ 
+ int record_init(int logging_dom, 
+ 		unsigned long event_ring_mfn, 
+ 		unsigned long page_list_mfn,
+ 		unsigned nr_pages,
+ 		unsigned cleaner) {
+     int ret;
+     replay_op_t rop;
+     //replay_record_op_t *rrop=&rop.u.recop;
+ 
+     rop.cmd=REPLAY_REC_CMD;
+     rop.u.recop.cmd=REC_INIT;
+     rop.u.recop.dom=logging_dom;  //domid
+     rop.u.recop.u.rec_init.port=0;
+     rop.u.recop.u.rec_init.ring_mfn=event_ring_mfn;  //-1
+     rop.u.recop.u.rec_init.addr=page_list_mfn;  //-1
+     rop.u.recop.u.rec_init.len=nr_pages;  //1000
+     rop.u.recop.u.rec_init.clean_algorithm=cleaner; //3
+ 
+     ret=do_replay_op(&rop);
+     if (ret<0) {
+ 	errno=-ret;
+ 	ret=-1;
+     } else {
+ 	ret=rop.u.recop.u.rec_init.port;
+     }
+     return ret;
+ }
+ 
+ int init_egress(int logging_dom) {  //egress出口
+ #define NR_REPLAY_PAGES 1000
+     int shmem_fd;
+     char fname[]="/dev/egress";
+     //unsigned int *page_list;
+     long page_list_mfn, event_ring_mfn, mfn;
+     int egress_port=0;
+     int i;
+ 
+     if ((shmem_fd=open(fname,O_RDWR|O_CREAT))<0) {
+ 	    err(1, "ERROR: error opening device");
+     }
+ 	printf("/dev/egress fd:%d\n", shmem_fd);
+ 
+     for (i=0;i<NR_REPLAY_PAGES+2;i++) {
+ 	    if(lseek(shmem_fd, 4096*(i+1)-1, SEEK_SET)<0) {
+             err(1,"lseek");
+ 	}
+ 	if(write(shmem_fd, "\0", 1)<0) {
+             err(1,"write");
+ 	}
+     }
+     queue_area=mmap(0, (NR_REPLAY_PAGES+2) * PAGE_SIZE, PROT_READ|PROT_WRITE, MAP_SHARED, shmem_fd, 0);
+     
+     if (mlock(queue_area, (NR_REPLAY_PAGES+2) * PAGE_SIZE)) {
+ 	    perror("failed to mlock replay queue pages");
+         exit(1);
+     }
+     memset(queue_area, 0, NR_REPLAY_PAGES * PAGE_SIZE);
+     wmb();
+    
+ 	//page_list = 0
+     page_list=(unsigned int *)queue_area;
+ 	//page_list_mfn = -1
+ 	//int ffd = open("/root/tt.txt", O_CREAT|O_RDWR);
+     page_list_mfn=ioctl(shmem_fd, _IOR('K', 13, int), 0);
+     if (page_list_mfn<0) {
+ 	    printf("Failed to get zeroed page.\n");
+ 	    return 0;
+     }
+     printf("pagelist  : %d\n",*page_list);
+     event_ring=(struct record_event_ring *)(((char*)queue_area)+PAGE_SIZE);
+     //assert(((unsigned long)queue_area)+PAGE_SIZE==(unsigned int)event_ring);
+     event_ring_mfn = ioctl(shmem_fd, _IOR('K', 13, int), 1);
+     if (event_ring_mfn<0) {
+ 	printf("Failed to get zeroed page.\n");
+ 	return 0;
+     }
+     
+     for (i = 0; i < NR_REPLAY_PAGES; i++) {
+ 	mfn=ioctl(shmem_fd, _IOR('K', 13, int), i+2);
+         if (mfn<0) {
+ 	    printf("Failed to get mfn from scullp device.\n");
+ 	    return 0;
+         }
+         page_list[i] = mfn;
+ 	//queue_map[mfn]=(struct record_event_buffer *)((char*)queue_area+(PAGE_SIZE*(i+2)));
+     }
+ 
+     printf("Starting monitor & sending page list to hypervisor.  (page_list[0] => MFN(%lx))\n",page_list[0]);
+     egress_port=record_init(logging_dom,
+ 			   event_ring_mfn,
+ 			   page_list_mfn, 
+ 			   NR_REPLAY_PAGES,
+ 			   QUEUE_CLEAN_NEVER);
+ 	if(egress_port == -1)
+ 			printf("acquire egress_port error\n");
+     printf("Setting event port to: %ld\n",egress_port);
+     if(evtchn_bind(egress_port)<0) {
+         err(1, "evtchn_bind");
+         fprintf(stderr, "Failed to bind %d!\n",
+                 egress_port);
+         egress_port=0;
+     }
+     return egress_port;
+ }
+ 
+ 
+ static int _evtchn_read(int _evtfd)
+ {
+     u16 v;
+     int bytes;
+ 
+     while ( (bytes = read(_evtfd, &v, sizeof(v))) == -1 )
+     {
+         if ( errno == EINTR )
+             continue;
+         /* EAGAIN was cased to return 'None' in the python version... */
+         return -errno;
+     }
+     
+     if ( bytes == sizeof(v) )
+         return v;
+     
+     /* bad return */
+     return -1;
+ }
+ 
+ int evtchn_read(void) {
+     return _evtchn_read(evtfd);
+ }
+ 
+ static void _evtchn_unmask(int _evtfd, u16 idx)
+ {
+     (void)write(_evtfd, &idx, sizeof(idx));
+ }
+ 
+ void evtchn_unmask(u16 idx) {
+     _evtchn_unmask(evtfd, idx);
+ }
+ 
+ 
+ static int _evtchn_wait(int _evtfd) {
+     int r;
+     fd_set fds;
+ 
+     fflush(stdout);
+ 
+     if(_evtfd <0) {
+     	fprintf(stderr, "_evtfd not set!\n");
+ 	return -EBADF;
+     }
+ 
+     FD_ZERO(&fds);
+    
+     FD_SET(_evtfd, &fds);
+ 
+     while(1) {
+     	r=select(_evtfd+1, &fds, NULL, NULL, NULL);
+     	if(r<0) {
+ 	    if(errno == -EINTR)
+ 	    	continue;
+ 	    else {
+ 	    	err(1, "select");
+ 		goto out;
+ 	    }
+ 	} else {
+ 	    goto out;
+ 	}
+     }
+    out:
+     return r;
+ }
+ 
+ int evtchn_wait(void) {
+     return _evtchn_wait(evtfd);
+ }
+ 
+ static struct record_event_buffer *get_next_event_buffer(void)
+ {
+     struct record_event_buffer *event_buffer;
+ 
+     unsigned long full_idx;
+     unsigned long full_mfn;
+     int chan, i;
+ 
+     /* Grab the next buffer from the full list. */
+ 
+     while(!(event_ring->full_buffer_prod > event_ring->full_buffer_cons)) {
+         if(evtchn_wait()<0)
+ 	    return 0;
+ 	
+ 	//fflush(stdout);
+ 	chan=evtchn_read();
+ 	evtchn_unmask(chan);
+ 	//replay_poke();
+     }
+ 
+     full_idx = event_ring->full_buffer_cons % RECORD_BUFFERS_PER_RING;
+     rmb();
+     full_mfn = event_ring->buffers[full_idx];
+     
+     for (i=0;i<NR_REPLAY_PAGES && page_list[i]!=full_mfn;i++);
+     if (i==NR_REPLAY_PAGES) {
+ 	printf("ERROR: Couldn't find MFN!\n");
+ 	fflush(stdout);
+ 	exit(1);
+     }
+     event_buffer = queue_area+(PAGE_SIZE*(i+2));
+ 
+     return event_buffer;
+ }
+ 
+ static int
+ printf_special_B_handler_func(FILE *f, const struct printf_info *info,
+ 		const void *const *args)
+ {
+     int x;
+     int count = 0;
+     int r;
+     const unsigned char *buf = *(const unsigned char **)args[0];
+ 
+     for(x = 0; x < info->prec; x++){
+         r = fprintf(f, "%.2x%c", buf[x], (char)info->pad);
+ 	if(r < 0)
+ 		return r;
+ 	count += r;
+     }
+     return count;
+ }
+ 
+ static int printf_special_B_arginfo_func(const struct printf_info *info,
+ 		size_t n, int *argtypes)
+ {
+     if(n > 0)
+ 	    argtypes[0] = PA_POINTER;
+     return 1;
+ }
+ 
+ static void register_printf_specials()
+ {
+     register_printf_function('B',printf_special_B_handler_func,
+ 		            printf_special_B_arginfo_func);
+ }
+ 
+ static void
+ plug_network_event_ring(domid_t dom)  //阻塞网络事件ring
+ {
+ 	struct record_ioctl_plug plug_struct;
+ 	int res;
+ 	plug_struct.dom = dom;
+ 	plug_struct.handle = dom_netif_handle;
+     res = ioctl(record_fd, RECORD_IOCTL_PLUG, &plug_struct);
+ 	if (res < 0) {
+ 		printf("dom_netif_handle:%d", dom_netif_handle);
+ 		err(1,"Failed to plug event ring");
+ 		//abort();
+ 	}
+ }
+ 
+ static void write_hypercall_result(const struct record_event *event)
+ {
+     struct hyperresult_record record;
+ 	
+ 	record.rec.type = REC_TYPE_HYPERRES;
+ 	record.call = event->u.hyperresult.call;
+ 	record.res = event->u.hyperresult.result;
+ 	record.ecx = event->u.hyperresult.result_ecx;
+ 	printf("Hypercall event\n");
+ }
+ 
+ int record_events(const struct record_event *event)
+ {
+ 	switch (event->type){
+ 		case RECORD_EVT_SCHED:
+             printf("Scheduling event\n");
+ 		case RECORD_EVT_HYPERRESULT:
+ 			printf("Hypercall event\n");
+ 			write_hypercall_result(event);
+ 			break;
+ 		case RECORD_EVT_DOMTIME:
+ 			printf("Domain time\n");
+ 			break;
+ 		case RECORD_EVT_EVTCHN_SET_PENDING:
+ 			printf("Event channnel set pending event\n");
+ 			break;
+ 	//	case RECORD_EVT_IO_EVENT:
+ 			//printf("Io event\n");
+ 			//break;
+ 		case RECORD_EVT_CHECKPOINT:
+             printf("Checkpoint event\n");
+ 			break;
+ 		//case RECORD_EVT_UNPAUSE:  //NEED_UNPAUSE
+ 			//printf("Unpause dom event\n");
+ 			//break;
+ 		case RECORD_EVT_RDTSC:
+ 			printf("rdtsc event\n");
+ 			break;
+         case RECORD_EVT_HCALLBACK:
+ 			printf("Callback event\n");
+ 			break;
+ 		default:
+ 			printf("unknown event\n");
+ 	
+ 	}
+     return 1;
+ }
+ 
+ static inline unsigned long read_cr3()
+ {
+     unsigned long cr3;
+ 	__asm__ __volatile__(
+ 					"mov %%cr3, %0" : "=r" (cr3):);
+ 
+ 	printf("cr3:=%lu", cr3);
+ 	return cr3;
+ }
+ 
+ unsigned long long get_time()
+ {
+ 	unsigned long long current_time;
+     struct timeval tv;
+ 
+ 	gettimeofday(&tv, 0);
+     current_time = tv.tv_sec*1000000 + tv.tv_usec;
+ 	printf("current time:%llu\n", current_time);
+ 
+ 	return current_time;
+ }
+ 
+ static int do_evtchn_op(int xc_handle, int cmd, void *arg,
+ 						                        size_t arg_size, int silently_fail)
+ {
+     int ret = -1;
+     DECLARE_HYPERCALL;
+     hypercall.op     = __HYPERVISOR_event_channel_op;
+     hypercall.arg[0] = cmd;
+     hypercall.arg[1] = (unsigned long)arg;
+ 				 
+     if ( lock_pages(arg, arg_size) != 0 ){
+         err(1,"do_evtchn_op: arg lock failed");
+         goto out;
+     }
+     if ((ret = do_xen_hypercall(xc_handle, &hypercall)) < 0 && !silently_fail)
+         err(1,"do_evtchn_op: HYPERVISOR_event_channel_op failed: %d", ret);
+         unlock_pages(arg, arg_size);
+         out:
+         return ret;
+ }
+ 
+ int xc_port_status(int xc_handle, u32 dom, int port, evtchn_status_t *status)
+ {
+    //evtchn_op_t op;
+     int ret;
+ 
+     struct evtchn_status arg = {.dom = (domid_t)dom,
+ 		                       .port = (evtchn_port_t)port } ;
+ 
+     ret = do_evtchn_op(xc_handle, EVTCHNOP_status, &arg, sizeof(arg),1);
+ 	memcpy(status, &arg,sizeof(struct evtchn_status));
+ 	if(0 == ret)
+ 		ret = arg.status;
+ 
+     return ret;
+ }
+ int xc_domain_getinfolist(int xc_handle,
+                           uint32_t first_domain,
+                           unsigned int max_domains,
+                           xc_domaininfo_t *info)
+ {
+     int ret = 0;
+     DECLARE_SYSCTL;
+ 
+     if ( lock_pages(info, max_domains*sizeof(xc_domaininfo_t)) != 0 )
+         return -1;
+ 
+     sysctl.cmd = XEN_SYSCTL_getdomaininfolist;
+     sysctl.u.getdomaininfolist.first_domain = first_domain;
+     sysctl.u.getdomaininfolist.max_domains  = max_domains;
+     set_xen_guest_handle(sysctl.u.getdomaininfolist.buffer, info);
+ 
+     if ( xc_sysctl(xc_handle, &sysctl) < 0 )
+         ret = -1;
+     else
+         ret = sysctl.u.getdomaininfolist.num_domains;
+ 
+     unlock_pages(info, max_domains*sizeof(xc_domaininfo_t));
+ 
+     return ret;
+ }
+ 
+ int xc_vcpu_getcontext(int xc_handle,
+                        uint32_t domid,
+                        uint32_t vcpu,
+                        vcpu_guest_context_any_t *ctxt)
+ {
+     int rc;
+     DECLARE_DOMCTL;
+     size_t sz = sizeof(vcpu_guest_context_any_t);
+ 
+     domctl.cmd = XEN_DOMCTL_getvcpucontext;
+     domctl.domain = (domid_t)domid;
+     domctl.u.vcpucontext.vcpu   = (uint16_t)vcpu;
+     set_xen_guest_handle(domctl.u.vcpucontext.ctxt, &ctxt->c);
+ 
+     
+     if ( (rc = lock_pages(ctxt, sz)) != 0 )
+         return rc;
+     rc = do_domctl(xc_handle, &domctl);
+     unlock_pages(ctxt, sz);
+ 
+     return rc;
+ }
+ 
+ int main(int argc, char *argv[])
+ {   /**** Usage:./record domID  ****/
+     //设置checkpoint，然后记录其他进程向环形缓冲区写入的事件
+     struct record_event evts[1024];
+     shared_info_t *live_shinfo = NULL;
+     unsigned long shared_info_frame;
+     xen_pfn_t *pfn_list;
+     xen_pfn_t *p2m_frame_list = NULL;
+ 
+     evtchn_status_t port_uses[1024];
+ 	evtchn_status_t *port_status;
+     evtchn_status_t *netif_port=0;
+ 
+     struct sockaddr_in sa;
+     struct sched_param sched_params;
+     int status;
+     int getportinfo;
+     int r;
+     struct record_ioctl_attach attach_struct;
+     int att;
+     unsigned rx_ring_mfn,tx_ring_mfn;
+     struct task_struct *currentProcess;
+     struct replay_file_header hdr;
+     static unsigned long p2m_size;
+     struct xen_domctl_getdomaininfo domaininfo[1];
+     xc_dominfo_t dominfo;
+     void *data = NULL;
+     struct timeval tv;
+     unsigned long i;
+     int evtchn_fd;
+     int xce_handle = -1;
+     int ret;
+     unsigned long long now;
+     static unsigned long max_mfn;
+     unsigned int res;
+     struct evtchn *evt;
+ 
+     vcpu_guest_context_any_t ctxt;  //vcpu context
+ 
+     guest_width = sizeof(unsigned long);
+ 
+     xen_pfn_t *live_p2m_frame_list_list = NULL;
+     xen_pfn_t *live_p2m_frame_list = NULL; //typedef unsigned long xen_pfn_t;
+ 
+     printf("event size:%d,header size:%d\n", sizeof(struct record_event), sizeof(struct replay_file_header));
+ 
+     register_printf_specials();
+ 
+     //Set process priority, PRIO_PROCESS = 0;  -20~20 
+     if(setpriority(PRIO_PROCESS, getpid(), -20))
+         err(1, "can't get high priority");
+     sched_params.sched_priority = 99;
+ 
+     if (sched_setscheduler(getpid(), SCHED_FIFO, &sched_params) < 0)
+ 	    err(1, "cannot make ourselves real time");
+     
+     int port = 8000;      //default xend port:8000
+     recordxend = socket(PF_INET, SOCK_STREAM , 0);
+     sa.sin_family = PF_INET;
+     //equal: sa.sin_addr.s_addr = inet_addr("127.0.0.1");
+     sa.sin_addr.s_addr = 0x0100007f;     //0x0100007f是127.0.0.1
+     sa.sin_port = htons(port);
+     if(connect(recordxend,(const struct sockaddr *)&sa,sizeof(sa)) < 0)
+ 	    err(1,"Connect to minixend\n");
+     else 
+ 	    printf("Connect to xend:succeed\n");
+ 
+     //get domid
+     int dom = atoi(argv[1]);
+ 	
+     xc_handle = xc_interface_open();
+     if(xc_handle < 0)
+ 	    err(1,"Failed to open xc_interface");
+ 
+     //read_cr3(); //段错误
+     xc_domain_unpause(xc_handle, dom);
+     xc_domain_pause(xc_handle, dom);
+     
+     if((record_fd = open("/root/record.txt", O_RDWR|O_CREAT)) < 0)
+ 	    err(1,"fail to open record file");
+     if(write(record_fd, &dom, sizeof(dom)) < 0)
+ 	    err(1, "write record error");
+ 
+     //get full domain info
+     ret = xc_domain_getinfolist(xc_handle, dom, 1, domaininfo);
+     if(1 != ret)
+         printf("get fulldomaininfo failed,is xend start?\n");
+ 
+     //Aim:acquire netif_port,new port is the minimum port which is free.
+     errno = 0;
+     for(port=0; errno != EINVAL; port++){
+ 
+ 	//status = xc_evtchn_status(xc_handle, dom, port);
+         //if(status < 0)
+         //    continue;
+         int pt_status;
+ 	    pt_status = xc_port_status(xc_handle, dom, port, port_uses+port);
+  
+ 		if(pt_status < 0)
+ 			continue;
+         if(pt_status > 0)
+             printf("\nport:%d, pt_status:%d\n", port, pt_status);       
+        
+ 	    switch(pt_status){
+                 case EVTCHNSTAT_closed:    //0
+ 		    //printf("closed\n");
+ 		    break;
+ 		case EVTCHNSTAT_unbound:
+ 		    //printf("unbound\n"); //1
+ 		    break;
+ 		case EVTCHNSTAT_interdomain:  /* 2: Channel is connected to remote domain. */
+ 				//evt = evtchn_from_port(d, port);
+ 		    printf("dom:%d,port:%d\n",
+ 		    port_uses[port].u.interdomain.dom,
+ 		    port_uses[port].u.interdomain.port);
+ 		    if(port_uses[port].u.interdomain.dom != 0)
+ 			err(1,"need to implement interdomain support");
+                     if(port == 4)
+ 			netif_port = port_uses + port;//获取netif_port
+ 
+ 		    break;
+ 		case EVTCHNSTAT_pirq:        //3 记录物理中断请求
+ 		    printf("virq %d\n",port_uses[port].u.virq);
+ 		    err(1,"Can not record domains with physical hardware access");
+ 		    break;
+ 		case EVTCHNSTAT_virq:        //4 记录虚拟中断请求
+ 		    printf("virq:%d\n",port_uses[port].u.virq);
+ 		    break;
+                 case EVTCHNSTAT_ipi:
+                     printf("EVTCHNSTAT_IPI\n");
+                     break;
+ 		default:
+ 		   abort();
+ 	    }
+     }
+     if(EINVAL != errno)
+         err(1,"Finding domain list");
+ 
+     int record_dev;
+     record_dev = open("/dev/module", O_RDWR);
+     if(record_dev < 0)
+ 	err(1,"Open /dev/module");
+ 
+     write(record_dev, &dom, sizeof(dom));
+ 
+     struct record_ioctl_plug ioctl_plug;
+ 
+     /* XXX this only works for backends in domain 0. */
+     if(netif_port != 0){
+ 	attach_struct.port = netif_port->u.interdomain.port;
+ 	//attach_struct.port = 2;
+ 	att = ioctl(record_dev,RECORD_IOCTL_ATTACH,&attach_struct);
+ 
+ 	if(att < 0){
+ 		printf("get network ring failed\n");
+ 		abort();
+ 	}
+ 
+ 	rx_ring_mfn = attach_struct.rx_ring_mfn;
+ 	tx_ring_mfn = attach_struct.tx_ring_mfn;
+ 	dom_netif_handle = attach_struct.handle;  //dom_netif_handle = 0
+         printf("rx_ring_mfn %x, tx_ring+mfn %x \n", rx_ring_mfn, tx_ring_mfn);
+ 
+ 	plug_network_event_ring(dom);
+ 
+ 	dom_rx_ring = xc_map_foreign_range(xc_handle,dom,
+ 				PAGE_SIZE,PROT_READ,rx_ring_mfn);
+ 	if(dom_rx_ring == NULL)
+ 		err(1,"Mapping domain rx ring\n");
+ 	dom_tx_ring = xc_map_foreign_range(xc_handle,dom,
+ 				PAGE_SIZE,PROT_READ,tx_ring_mfn);
+ 	if(dom_tx_ring == NULL)
+ 		err(1,"Mapping domain tx ring\n");
+ 
+ 	last_tx_ring_rsp = dom_tx_ring->resp_prod;
+ 	last_rx_ring_rsp = dom_rx_ring->resp_prod;
+     }
+ /* 2009.06.12 */
+     //**** Get and save context
+ 	if((xc_domain_getinfo(xc_handle, dom, 1, &dominfo)) != 1)
+ 		err(1, "unable get dom info,is domain living?");
+ 
+ 	p2m_size = xc_memory_op(xc_handle, XENMEM_maximum_gpfn, &dom) + 1;
+         printf("p2m_size:%d nr_pages:%d\n",p2m_size, dominfo.nr_pages);//520M,512M
+ 	
+ 	if(!xc_domain_getinfo(xc_handle, dom, 1,&dominfo))
+ 	     printf("Fail to get domain_info\n");
+ 
+ 	//Map the share info
+ 	shared_info_frame = dominfo.shared_info_frame;
+ 	live_shinfo = xc_map_foreign_range(xc_handle, dom, PAGE_SIZE,
+ 					PROT_READ, shared_info_frame);
+ 	printf("test2\n");
+ 	if(!live_shinfo)
+ 			err(1,"Fail to get share info");
+ 	
+ 	live_p2m_frame_list_list = xc_map_foreign_range(xc_handle, dom, PAGE_SIZE,
+ 					PROT_READ, live_shinfo->arch.pfn_to_mfn_frame_list_list);
+ 	if(!live_p2m_frame_list_list)
+ 			err(1,"live p2m frame list list");
+ 
+         printf("LIVE LIST LIST:%d\n", *live_p2m_frame_list_list);
+ 	
+ 	live_p2m_frame_list = xc_map_foreign_pages(xc_handle,
+ 					dom, PROT_READ, live_p2m_frame_list_list,
+ 					P2M_FLL_ENTRIES);
+ 
+ 	if(!live_p2m_frame_list)
+             err(1,"live p2m frame list");
+ 
+ 	live_p2m = xc_map_foreign_pages(xc_handle, dom,
+ 					PROT_READ, live_p2m_frame_list,
+ 					P2M_FL_ENTRIES);
+ 
+ 	if(!*live_p2m)
+ 			err(1,"map foreign pages");
+ 
+         /* acuqire and write out memory contents */
+         int N = 0;
+ 	unsigned int mfn;
+ 	//while(N < p2m_size)  //512M 之后出现live_p2m[N] = -1;
+ 	now = get_time();
+ 	int domchkpoint = open("/root/savedom.txt",O_RDWR | O_CREAT);
+ 	while(N < dominfo.nr_pages){
+ 			//printf("N = %d\n",N);
+ 			//printf("Live_p2m[N]=%d,%d\n",live_p2m[N], pfn_to_mfn(N));
+ 			//mfn = pfn_to_mfn(N);    //mfn == live_p2m[N]
+ 
+ 			if(data = xc_map_foreign_range(xc_handle,
+ 				dom, PAGE_SIZE,PROT_READ, live_p2m[N])){
+ 				if(data){
+                             //write(domchkpoint, data, PAGE_SIZE);
+ 					munmap(data, PAGE_SIZE);  //不munmap()，可能导致内存不足，无法写出内存页
+ 					}
+ 			}
+ 
+ 			N++;
+ 	}
+ 	now = get_time();
+ 	munlock(&dominfo, sizeof(dominfo));
+ 
+ 	xc_domain_unpause(xc_handle, dom);
+ 
+         //get vcpu context
+         printf("max vcpu:%d\n", dominfo.max_vcpu_id);
+         xc_vcpu_getcontext(xc_handle, dom, 0, &ctxt);
+ 	//replay file header
+ 
+ 	hdr.dom = dom;
+ 	hdr.flags = domaininfo[0].flags;
+ 	hdr.tot_pages = domaininfo[0].tot_pages;
+ 	hdr.shared_info_frame = domaininfo[0].shared_info_frame;
+         printf("flags:%d,tot_pages%d,shared_info_frame:%d\n",hdr.flags, hdr.tot_pages, hdr.shared_info_frame);
+ 	//hdr.network_rx_mfn = rx_ring_mfn;
+ 	//printf("hdr.network_rx_mfn:%d",hdr.network_rx_mfn);
+ 	//hdr.network_tx_mfn = tx_ring_mfn;
+         hdr.magic = 0x11224433;
+ #if 0
+ 	int evtchn = open("/dev/xen/evtchn", O_RDWR|O_NONBLOCK);
+ 	if(evtchn < 0)
+ 			err(1, "Open event channel");
+ 	
+ 	tv.tv_sec = 0;
+ 	tv.tv_usec = 1000;
+ 
+ 	fd_set inset;
+ 	FD_ZERO(&inset);
+ 	FD_SET(evtchn, &inset);    //设定被监视的句柄为evtchn
+ 	
+ 	/*while(1){    //waiting for events
+ 	    ret = select(evtchn+1, &inset, NULL, NULL, &tv);
+ 	    if(ret < 0)
+ 	        printf("select error\n");
+ 	    if(ret == 0)
+ 	        printf("waiting timeout\n");  //using timeval
+ 		else 
+ 			printf("event coming\n");
+ 
+ 		sleep(2);
+ 	}*/
+ 
+ 	/*record events*/
+ 	event_port = init_egress(dom);
+ 	printf("event port:%d\n",event_port);
+ 	if(!event_port){
+ 			printf("No port to listen\n");
+ 			exit(1);
+ 	}
+ 
+ 	evtfd = open("/root/event.txt", O_RDWR | O_CREAT);
+ 	if(evtfd < 0)
+ 	    err(1, "open record event file");
+ 
+ 	while(1){
+         struct record_event_buffer *buf;
+ 
+ 		buf = get_next_event_buffer();
+ 		if(0==buf)
+ 			err(1,"get event");
+ 		write(record_fd, buf, PAGE_SIZE);
+ 	}
+ 	
+ 	//replay file header
+ 	hdr.dom = dom;
+ 	hdr.flags = domaininfo[0].flags;
+ 	hdr.tot_pages = domaininfo[0].tot_pages;
+ 	hdr.shared_info_frame = domaininfo[0].shared_info_frame;
+ 	//hdr.network_rx_mfn = rx_ring_mfn;
+ 	//printf("hdr.network_rx_mfn:%d",hdr.network_rx_mfn);
+ 	//hdr.network_tx_mfn = tx_ring_mfn;
+         hdr.magic = 0x11224433;
+         */
+ 	//write(record_fd, &hdr, sizeof(hdr));
+ 
+     
+ 	xc_domain_unpause(xc_handle, dom);
+ 
+ 	if(0)  //用作注释
+ 	    unplug_network_event_ring(dom);
+ #endif
+         printf("now wait for events...\n");
+ 	int evt_size;/* read and record events */
+ 	while(1){
+ 		printf("wait for events\n");
+ 		sleep(2);  //won't disturb the system
+ 
+ 		evt_size = read(record_fd, evts, sizeof(evts));
+ 		if(evt_size < 0)
+ 		    err(1,"read events");
+ 		if(evt_size == 0)   //no events
+ 		    continue;
+ 
+ 		evt_size /= sizeof(evts[0]);
+ 		printf("event count = %d\n", evt_size);
+ 		for(i = 0; i < evt_size; i++){
+ 		    printf("record events,EIP%lu\n",evts[i].when.eip);
+ 		    record_events(evts+i);
+ 		}
+ 
+ 	}
+ #if 0
+ 	xce_handle = xc_evtchn_open();
+ 	if(xce_handle < 0)
+             printf("Fail to open event channel\n");
+ 	
+ 	evtchn_fd = xc_evtchn_fd(xce_handle);
+ 
+     tv.tv_sec = 0;
+     tv.tv_usec = 1000;
+     fd_set inset;
+     FD_ZERO(&inset);
+     FD_SET(evtchn_fd, &inset);    //设定被监视的句柄为evtchn_fd
+ 
+     while(1){    //waiting for events
+         ret = select(evtchn_fd+1, &inset, NULL, NULL, NULL);
+         if(ret < 0)
+             printf("select error\n");
+         if(ret == 0)
+             printf("waiting timeout\n");  //using timeval
+      
+ 		if(FD_ISSET(evtchn_fd, &inset)){
+             printf("event comes up\n");
+             xc_domain_pause(xc_handle, dom);
+             record_events(evts);
+             xc_domain_unpause(xc_handle, dom);
+         } 
+     }
+ #endif
+     close(record_fd);
+     return 0;
+ }
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/libxc/record.h new/xen-3.3.0/tools/libxc/record.h
*** xen-3.3.0/tools/libxc/record.h	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/libxc/record.h	2009-05-26 15:57:53.000000000 -0400
***************
*** 0 ****
--- 1,255 ----
+ #ifndef RECORD_H_
+ #define RECORD_H_
+ 
+ #include <stdio.h>
+ #include <sys/resource.h>
+ #include <unistd.h>
+ #include <sched.h>
+ #include <string.h>
+ #include "xg_private.h"
+ #include "replay.h"
+ 
+ #define XEN_DOMCTL_destroydomain      2
+ #define XEN_DOMCTL_pausedomain        3
+ #define XEN_DOMCTL_unpausedomain      4
+ #define XEN_DOMCTL_resumedomain      27
+ 
+ #define ENIVAL 22
+ 
+ #define DECLARE_DOMCTL struct xen_domctl domctl
+ #define RECORD_IOCTL_PLUG _IOR('R', 1, struct record_ioctl_plug)
+ #define XENMEM_maximum_gpfn         14
+ 
+ #define MAX_EVENT_NUM  sizeof(unsigned long) * sizeof(unsigned long) * 64
+ 
+ #define NR_REPLAY_PAGES 1000
+ 
+ #define wmb()	asm volatile("" ::: "memory")
+ //#define wmb() 	__asm__ __volatile__ ("lock; addl $0,0(%%esp)": : :"memory")
+ #define rmb() __asm__ __volatile__ ( "lock; addl $0,0(%%esp)" : : : "memory" )
+ 
+ //typedef unsigned long u32;
+ typedef unsigned long u32;
+ typedef u32 NETIF_RING_IDX;
+ typedef u32 memory_t;
+ #define MEMORY_PADDING
+ typedef unsigned int u16;
+ typedef signed char s8;
+ //typedef unsigned char u8;
+ typedef unsigned short s16;
+ typedef unsigned long long u64;
+ 
+ typedef unsigned char u8;
+ 
+ struct evtchn
+ {
+ #define ECS_FREE         0 /* Channel is available for use.                  */
+ #define ECS_RESERVED     1 /* Channel is reserved.                           */
+ #define ECS_UNBOUND      2 /* Channel is waiting to bind to a remote domain. */
+ #define ECS_INTERDOMAIN  3 /* Channel is bound to another domain.            */
+ #define ECS_PIRQ         4 /* Channel is bound to a physical IRQ line.       */
+ #define ECS_VIRQ         5 /* Channel is bound to a virtual IRQ line.        */
+ #define ECS_IPI          6 /* Channel is bound to a virtual IPI line.        */
+     u8  state;             /* ECS_* */
+     u8  consumer_is_xen;   /* Consumed by Xen or by guest? */
+     u16 notify_vcpu_id;    /* VCPU for local delivery notification */
+     union {
+         struct {
+             domid_t remote_domid;
+         } unbound;     /* state == ECS_UNBOUND */
+         struct {
+             u16            remote_port;
+             struct domain *remote_dom;
+         } interdomain; /* state == ECS_INTERDOMAIN */
+         u16 pirq;      /* state == ECS_PIRQ */
+         u16 virq;      /* state == ECS_VIRQ */
+     } u;
+ #ifdef FLASK_ENABLE
+     void *ssid;
+ #endif
+ };
+ 
+ /*
+ #define EVTCHNS_PER_BUCKET 128
+ #define bucket_from_port(d,p) \
+ 		    ((d)->evtchn[(p)/EVTCHNS_PER_BUCKET])  //EVTCHNS_PER_BUCKET = 128
+ #define port_is_valid(d,p)    \
+ 		    (((p) >= 0) && ((p) < MAX_EVTCHNS(d)) && \
+ 			      (bucket_from_port(d,p) != NULL))
+ #define evtchn_from_port(d,p) \
+ 		    (&(bucket_from_port(d,p))[(p)&(EVTCHNS_PER_BUCKET-1)])
+ */
+ 
+ #define EINVAL 22	/* Invalid argument */
+ #define PROT_READ	0x1		/* Page can be read.  */
+ 
+ //typedef dom0_getdomaininfo_t xc_domaininfo_t;
+ 
+ typedef struct {
+     memory_t addr;   /*  0: Machine address of packet.  */
+     MEMORY_PADDING;
+ 	u16      csum_blank:1; /* Proto csum field blank?   */
+ 	u16      id:15;  /*  8: Echoed in response message. */
+ 	u16      size;   /* 10: Packet size in bytes.       */
+ }netif_tx_request_t; /* 12 bytes */
+ 
+ typedef struct {
+ 	u16      id;     /*  0 */
+ 	s8       status; /*  2 */
+ 	u8       __pad;  /*  3 */
+ }netif_tx_response_t; /* 4 bytes */
+ 
+ #define NETIF_TX_RING_SIZE 256
+ 
+ #define EVTCHN_BIND   _IO('E', 2)
+ 
+ typedef struct {
+ 	NETIF_RING_IDX req_prod;       /*  0 */
+ 	NETIF_RING_IDX req_cons;       /*  4 */
+ 	NETIF_RING_IDX resp_prod;      /*  8 */
+ 	NETIF_RING_IDX event;          /* 12 */
+ 	union {                        /* 16 */
+ 	    netif_tx_request_t  req;
+ 	    netif_tx_response_t resp;
+ 	}ring[NETIF_TX_RING_SIZE];
+ }netif_tx_interface_t;
+ 
+ typedef struct {
+ 	u16       id;    /*  0: Echoed in response message.        */
+ }netif_rx_request_t; /* 2 bytes */
+ 
+ typedef struct {
+ 	memory_t addr;   /*  0: Machine address of packet.              */
+ 	MEMORY_PADDING;
+ 	u16      csum_valid:1; /* Protocol checksum is validated?       */
+ 	u16      id:15;  /*  8:  */
+ 	s16      status; /* 10: -ve: BLKIF_RSP_* ; +ve: Rx'ed pkt size. */
+ }netif_rx_response_t; /* 12 bytes */
+ 
+ #define NETIF_RX_RING_SIZE 256
+ typedef struct {
+ 		    /*
+ 			 *      * Frontend places empty buffers into ring at rx_req_prod.
+ 			 *           * Frontend receives event when rx_resp_prod passes rx_event.
+ 			 *                */
+ 	NETIF_RING_IDX req_prod;       /*  0 */
+ 	NETIF_RING_IDX resp_prod;      /*  4 */
+ 	NETIF_RING_IDX event;          /*  8 */
+ 	union {                        /* 12 */
+ 	    netif_rx_request_t  req;
+ 	    netif_rx_response_t resp;
+ 	}ring[NETIF_RX_RING_SIZE];
+ }netif_rx_interface_t;
+ 
+ #define RECORD_IOCTL_ATTACH _IOR('R', 3, struct record_ioctl_attach)
+ 
+ struct record{
+ 
+ };
+ 
+ struct replay_file_header {
+ 	unsigned magic;
+ 	u32 dom;
+ 	u32 flags;
+ 	u32 shared_info_mfn;
+ 	unsigned assist;
+ 	memory_t tot_pages;
+ 	memory_t shared_info_frame;
+ 	vcpu_guest_context_t ctxt;
+ };
+ 
+ struct replay_record {
+         unsigned size:6;
+         unsigned ignore:1;
+         enum {
+             REC_TYPE_HYPERRES,
+             REC_TYPE_DOMTIME,
+             REC_TYPE_SETPENDING,
+             REC_TYPE_CHKPOINT,
+             REC_TYPE_RDTSC,
+             REC_TYPE_EVTDELIVER,
+             REC_TYPE_NOTIFY,
+             REC_TYPE_CREW_EVENT,
+             REC_TYPE_CREW_CONSTRAINT,
+             REC_TYPE_BTS,
+             REC_TYPE_REGISTER_CHECK,
+         } type:4;
+ 	struct time_location when;
+ #ifdef CTX_PER_RECORD
+ 	cpu_user_regs_t ctx;
+ #endif
+ };
+ 
+ /* Hyperresults */
+ struct hyperresult_record {
+ 	struct replay_record rec;
+ 	int call;
+ 	long res;
+ 	unsigned long ecx;
+ };
+ 
+ /* Async modifications to the shared info page made by Xen */
+ struct domtime_record {
+ 	struct replay_record rec;
+ 	u64 cpu_freq;
+ 	u64 system_time;
+ 	u32 wc_sec;
+ 	u32 wc_usec;
+ 	tsc_timestamp_t tsc_timestamp;
+ 	unsigned version;
+ };
+ 
+ struct setpending_record {
+ 	struct replay_record rec;
+ 	domid_t dom;
+ 	int port;
+ };
+ /* Check points */
+ struct checkpoint_record {
+ 	struct replay_record rec;
+ 	domid_t dom;
+ 	unsigned long from, to;
+ };
+ 
+ /* TSC stuff */
+ struct rdtsc_record {
+ 	struct replay_record rec;
+ 	domid_t dom;
+ 	unsigned long low;
+ 	unsigned long high;
+ };
+ 
+ /* Event deliver */
+ struct evtdeliver_record {
+ 	struct replay_record rec;
+ };
+ 
+ struct notify_record {
+         struct replay_record rec;
+         unsigned vcpu;
+         unsigned tip;
+ };
+ 
+ struct crew_event_record {
+         struct replay_record rec;
+ 	unsigned long gpfn;
+ 	unsigned long event_count;
+ };
+ struct crew_constraint_record {
+         struct replay_record rec;
+         unsigned actor;
+ 	crew_vector_t vector;
+ 	unsigned long gpfn;
+ };
+ struct bts_state_record {
+         struct replay_record rec;
+         unsigned vcpu;
+         unsigned bts_state;
+ };
+ struct register_check_record {
+ 	struct replay_record rec;
+ 	int where;
+         unsigned long extra;
+ };
+ 
+ #endif
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/libxc/replay.h new/xen-3.3.0/tools/libxc/replay.h
*** xen-3.3.0/tools/libxc/replay.h	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/libxc/replay.h	2009-06-10 17:29:31.000000000 -0400
***************
*** 0 ****
--- 1,722 ----
+ #ifndef __REPLAY_H__
+ #define __REPLAY_H__
+ 
+ /* Maximum number of vcpus to log. */
+ #define CREW_LAST_VCPU 2
+ /* This is limited by the size available in the lower 12 bits of the
+  * p2m table.  */
+ #define CREW_MAX_ACTORS (CREW_LAST_VCPU+3)
+ 
+ #if (CREW_MAX_ACTORS > 8)
+ # error "CREW_MAX_ACTORS cannot be more than 8!" 
+ #endif
+ 
+ #define vcpu_to_actor(x) ((x)+1)
+ #define actor_to_vcpu(x) ((x)-1)
+ //#define tip_to_actor(x) ((x)+CREW_LAST_VCPU+1)
+ //#define tip_to_actor(x) (_tip_to_actor[x])
+ #define is_vcpu(x) ((x) <= CREW_LAST_VCPU)
+ #define is_proc(x) (!is_vcpu(x))
+ #define can_preempt(x) is_vcpu(x)
+ 
+ #define CREW_TIP_EVENT        20
+ #define CREW_TIP_CONSTRAINT   21
+ #define DTIME_TIP             22
+ #define BREAK_TIP             23
+ #define SPEND_TIP             24
+ #define DELIVER_TIP           25
+ #define NOTIFY_TIP            26
+ #define	BTS_UPDATE_TIP        27
+ #define UNPAUSE_TIP           28
+ #define NULL_TIP              29
+ #define SHADOW_SYNC_TIP       30
+ 
+ //#define __HYPERVISOR_replay_op  43
+ 
+ //#define	SA_INTERRUPT	0x20000000
+ 
+ #define crew_v2e(_a, _v) ((_v).event[(_a)-1])
+ 
+ typedef unsigned long u_32;
+ 
+ #define vcpu_not_current(_v)	\
+ 		(((_v)!=current) && ((_v)->processor != smp_processor_id()))
+ 
+ #define TESTING_LOG_IGNORE 0
+ #define TESTING_MINIMAL 1
+ #define TESTING_STRICTLY_MINIMAL 1
+ /* These will cause extravirt and/or replay not to work */
+ #define TESTING_DISABLE_EXTRAVIRT 1
+ #define TESTING_DISABLE_CREW 0
+ #define TESTING_FILTER_GPFNS 0
+ 
+ #if TESTING_STRICTLY_MINIMAL
+ # define s_start_clock(_x)
+ # define s_stop_clock(_x)
+ #else
+ # define s_start_clock(_x) start_clock(_x)
+ # define s_stop_clock(_x) stop_clock(_x)
+ #endif
+ 
+ #if TESTING_MINIMAL
+ # define register_check(_x...)
+ # define m_start_clock(_x)
+ # define m_stop_clock(_x)
+ #else
+ # define register_check __register_check
+ # define m_start_clock(_x) start_clock(_x)
+ # define m_stop_clock(_x) stop_clock(_x)
+ #endif
+ 
+ /*
+  * Per-domain flags (domain_flags).
+  */
+  /* Has the guest OS been fully built yet? */
+ #define _DOMF_constructed      0
+ #define DOMF_constructed       (1UL<<_DOMF_constructed)
+  /* Is this one of the per-CPU idle domains? */
+ #define _DOMF_idle_domain      1
+ #define DOMF_idle_domain       (1UL<<_DOMF_idle_domain)
+  /* Is this domain privileged? */
+ #define _DOMF_privileged       2
+ #define DOMF_privileged        (1UL<<_DOMF_privileged)
+  /* May this domain do IO to physical devices? */
+ #define _DOMF_physdev_access   3
+ #define DOMF_physdev_access    (1UL<<_DOMF_physdev_access)
+  /* Guest shut itself down for some reason. */
+ #define _DOMF_shutdown         4
+ #define DOMF_shutdown          (1UL<<_DOMF_shutdown)
+  /* Guest is in process of shutting itself down (becomes DOMF_shutdown). */
+ #define _DOMF_shuttingdown     5
+ #define DOMF_shuttingdown      (1UL<<_DOMF_shuttingdown)
+  /* Death rattle. */
+ #define _DOMF_dying            6
+ #define DOMF_dying             (1UL<<_DOMF_dying)
+ 
+ /* Replay flags */
+ #define  _DOMF_monitored   16 /* Monitor timings of return to userspace      */
+ #define  _DOMF_unplug      17 /* Unplug net ring as soon as this schedules*/ 
+ #define  _DOMF_replaying   18 /* In progress of replaying this domain */ 
+ //#define  _DOMF_HYPERCALL   19 /* Domain in process of doing a hypercall */
+ #define DOMF_monitored             (1UL<<_DOMF_monitored)
+ #define DOMF_unplug             (1UL<<_DOMF_unplug)
+ #define DOMF_replaying             (1UL<<_DOMF_replaying)
+ 
+ #define IS_REPLAYING(ddd)       (test_bit(_DOMF_replaying, &ddd->domain_flags))
+ #define IS_MONITORED(ddd)       (test_bit(_DOMF_monitored, &ddd->domain_flags))
+ #define IS_LOGGING(ddd)         (test_bit(_DOMF_monitored, &ddd->domain_flags))
+ 
+ #define RECORD_SOFTIRQ         8
+ 
+ struct record_ioctl_plug {
+ 		    unsigned dom;
+ 			    unsigned handle;
+ };
+ 
+ struct record_ioctl_attach {
+ 		    unsigned port;
+ 			    unsigned rx_ring_mfn;
+ 				    unsigned tx_ring_mfn;
+ 					    unsigned handle;
+ };
+ 
+ #define RECORD_IOCTL_ATTACH _IOR('R', 3, struct record_ioctl_attach)
+ #define RECORD_IOCTL_PLUG _IOR('R', 1, struct record_ioctl_plug)
+ #define RECORD_IOCTL_UNPLUG _IOR('R', 2, struct record_ioctl_plug)
+ 
+ typedef struct {
+     unsigned long event[CREW_MAX_ACTORS-1];
+ } crew_vector_t;
+ #define CREW_MAX_EVENT_COUNT (((unsigned long)0)-1)
+ 
+ 
+ /* General #defines */
+ #define    MAX_TIPS           32
+ #define    SYNCHRONOUS_EVENT   0
+ 
+ typedef unsigned long long pcounter_t;
+ 
+ /* A representation of a point in time.  */
+ struct time_location {
+     unsigned long eip;
+     unsigned long ecx; /* Needed for string opcodes */
+     pcounter_t perfctr;
+ };
+ 
+ static inline int time_cmp(struct time_location a, struct time_location b) {
+     if (a.perfctr<b.perfctr)
+ 	return -1;
+     if (a.perfctr>b.perfctr)
+ 	return 1;
+     if (a.eip<b.eip)
+ 	return -1;
+     if (a.eip>b.eip)
+ 	return 1;
+     if (a.ecx<b.ecx)
+ 	return -1;
+     if (a.ecx>b.ecx)
+ 	return 1;
+     return 0;  // otherwise they are ==
+ }
+ 
+ /*static inline int time_cmp(struct time_location t1, struct time_location t2) {
+     if (t1.perfctr < t2.perfctr) 
+ 	return -1;
+     if (t1.perfctr == t2.perfctr &&
+ 	t1.eip < t2.eip)
+ 	return -1;
+     if (t1.perfctr == t2.perfctr &&
+ 	t1.eip == t2.eip &&
+ 	t1.ecx < t2.ecx) 
+ 	return -1;
+     if (t1.perfctr == t2.perfctr &&
+         t1.eip == t2.eip &&
+         t1.ecx == t2.ecx)
+         return 0;
+     return 1;
+     }*/
+ 
+ /* Breakpoint request structure */
+ struct breakpoint {
+   struct time_location    time;
+   int                     dom_id;
+   int                     vcpu;
+   int                     tip_num;  // -1 for none
+ };
+ 
+ //typedef unsigned long long tsc_timestamp_t;
+ 
+ typedef struct record_event {
+    enum {
+         RECORD_EVT_NULL=0,
+         RECORD_EVT_SCHED,
+         RECORD_EVT_HYPERRESULT,
+         RECORD_EVT_DOMTIME,
+         RECORD_EVT_EVTCHN_SET_PENDING,
+         RECORD_EVT_CHECKPOINT, /* 5 */
+         RECORD_EVT_RDTSC,
+         RECORD_EVT_HCALLBACK,
+         RECORD_EVT_NOTIFY,
+         RECORD_EVT_CREW_EVENT,
+         RECORD_EVT_CREW_CONSTRAINT, /* 10 */
+         RECORD_EVT_BTS_STATE,
+         RECORD_EVT_UNPAUSE,
+         RECORD_EVT_REGISTER_CHECK,
+         RECORD_EVT_SHADOW_SYNC,
+         RECORD_EVT_MAX,
+    } type:4; /* 4 bits */
+     unsigned unpause_validate:2; /* 6 bits */
+     unsigned ignore:1; /* 7 bits */
+     unsigned user_timer:1; /* 8 bits */
+     struct time_location when; /* 17 bytes */
+ //#define REPLAY_FULLCONTEXT
+ #ifdef REPLAY_FULLCONTEXT
+     cpu_user_regs_t ctx; /* lots */
+ #endif
+     union {
+ 	struct {
+ 	    unsigned call;
+ 	    unsigned result;
+ 	    unsigned result_ecx;
+ 	} hyperresult; /* 12 bytes */
+ 	struct {
+ 	    unsigned long long cpu_freq;
+ 	    unsigned long long system_time;
+ 	    unsigned long wc_sec;
+ 	    unsigned long wc_usec;
+ 	    unsigned long long tsc_timestamp;
+ 	    unsigned version;
+ 	} domtime; /* 36 bytes */
+ 	struct {
+ 	    int port;
+             unsigned char flags;
+ 	} evtchn_set_pending; /* 5 bytes */
+ 	struct {
+ 	    unsigned long from,to;
+ 	} checkpoint; /* 8 bytes */
+ 	struct {
+ 	    unsigned long low, high;
+ 	} rdtsc; /* 8 bytes */
+         struct {
+ 	    int      tip;
+ 	} notify; /* 12 bytes */
+         struct {
+ 	    int      enable;
+ 	} bts_state; /* 12 bytes */
+ 	struct {
+ 	    unsigned long gpfn;
+             unsigned long event_count;
+ 	} crew_event; /* 8 bytes */
+ 	struct {
+ 	    unsigned actor; 
+ 	    crew_vector_t vector;
+ 	    unsigned long gpfn;
+             unsigned long cr2;
+             enum {
+                 CSRC_DELAY,
+                 CSRC_FAULT,
+                 CSRC_SHARED_INFO_GP,
+                 CSRC_SHARED_INFO_SPECIAL,
+                 CSRC_SHADOW_GRAB_GPFN,
+                 CSRC_BARRIER,
+                 CSRC_FLUSH_TLB,
+             } source:3;
+ 	} crew_constraint; /* 28 bytes + 3 bits*/
+ 	struct {
+ 	    enum {
+ 	       RCHECK_HYPERCALL,  /* 0 */
+ 	       RCHECK_PFAULT,
+ 	       RCHECK_TRAP,
+ 	       RCHECK_GP,
+ 	       RCHECK_MATH,
+ 	       RCHECK_INT3, /* 5 */
+ 	       RCHECK_DEBUG,
+ 	       RCHECK_HYPERCALL_POST,
+ 	       RCHECK_DA_EMULATION,
+ 	       RCHECK_SHADOW_MARK_OOS,
+                RCHECK_SHADOW_SYNC_LOCAL, /* 10 */
+                RCHECK_SHADOW_SYNC_ATTEMPT,
+                RCHECK_SHADOW_NORMAL_L1_PT_UPDATE,
+                RCHECK_SHADOW_NORMAL_L2_PT_UPDATE,
+                RCHECK_SHADOW_PROMOTE,
+                RCHECK_CREW_FLUSH_TLB,
+ 	    } where:4;
+ 	    unsigned long extra;
+ 	} register_check; /* 4 bytes + 4 bits */
+     } u; /* 57 bytes + ctxt size */
+ } record_event_t;
+ 
+ 
+ //#define REPLAY_LOCKDOMAIN      0
+ typedef struct {
+   /* IN parameters. */
+   unsigned short domain;
+ } replay_lockdomain_t;
+ 
+ //#define REPLAY_UNLOCKDOMAIN    1
+ typedef struct {
+   /* IN parameters. */
+   unsigned short domain;
+ } replay_unlockdomain_t;
+ 
+ typedef struct {
+   /* IN parameters. */
+   unsigned short domain;
+   int     vcpu;    // -1 for all vcpu's
+   int     enable;
+ } replay_bts_state_t;
+ 
+ typedef struct {
+     enum {
+ 	CREW_GRAB_GPFN,
+ 	CREW_RELEASE_GPFN,
+ 	CREW_GRAB_GREF,
+ 	CREW_RELEASE_GREF,
+ 	CREW_GRAB_EVENT,
+ 	CREW_RELEASE_EVENT,
+ 	CREW_RELEASE_ACTOR,
+ 	CREW_SET_VCPU_CONSTRAINT,
+ 	CREW_GRAB_BARRIER,
+ 	CREW_RELEASE_BARRIER,
+ 	CREW_FILTER_GPFN,
+     } cmd;
+     int actor;
+     int dom;
+     union {
+ 	struct {
+ 	    crew_vector_t * vector_p;
+ 	} grab_barrier;
+ 	struct {
+ 	    crew_vector_t * vector_p;
+ 	} release_barrier;
+ 	struct {
+ 	    int gpfn;
+ 	    int rw;
+ 	    crew_vector_t * vector_p;
+ 	} grab_gpfn;
+ 	struct {
+ 	    int gpfn;
+ 	    int rw;
+ 	} release_gpfn;
+ 	struct {
+ 	    int gref;
+ 	    int rw;
+ 	    crew_vector_t * vector_p;
+ 	} grab_gref;
+ 	struct {
+ 	    int gref;
+ 	    int rw;
+ 	} release_gref;
+ 	struct {
+ 	    /* event count is redundant; it's in the vector */
+ 	    crew_vector_t vector;
+ 	} grab_event;
+ 	struct {
+ 	} release_event;
+ 	struct {
+ 	    crew_vector_t vector;
+ 	} set_vcpu_constraint;
+         struct {
+ 	  unsigned long gpfn;
+         } filter_gpfn;
+     } u;
+     struct {
+ 	int dom;
+ 	int port;
+     } notify;
+ } replay_crew_op_t;
+ 
+ typedef enum {
+     QUEUE_CLEAN_LOGGED,
+     QUEUE_CLEAN_REPLAYED,
+     QUEUE_CLEAN_VERIFIED,
+     QUEUE_CLEAN_NEVER,
+     QUEUE_CLEAN_ALWAYS,
+ } clean_algorithm_t;
+ 
+ 
+ typedef struct {
+     enum {
+ 	REC_INIT,
+ 	REC_CLEAN_SWITCH
+     } cmd;
+     int dom;
+     union {
+ 	struct {
+ 	    int                port;     // notification port   (OUTPUT)
+ 	    unsigned long      ring_mfn; // mfn of the shared ring
+ 	    unsigned long      addr;     // start of the list
+ 	    unsigned long      len;      // number of mfns to read
+ 	    clean_algorithm_t  clean_algorithm;
+ 	} rec_init;
+ 	struct {
+ 	    clean_algorithm_t  clean_algorithm;
+ 	} rec_clean_switch;
+     } u;
+ } replay_record_op_t;
+ 
+ typedef struct {
+     enum {
+ 	REP_INIT,
+ 	REP_PUSH_RECORD,
+ 	REP_CONNECT
+     } cmd;
+     int dom;
+     union {
+ 	struct {
+ 	    int                port;       // notification port   (OUTPUT)
+             unsigned long      buf_vaddr;  // tip page for replay
+             unsigned long      list_vaddr; // start of the list
+             unsigned long      len;        // number of pages
+ 	} rep_init;
+ 	struct {
+ 	    record_event_t event;
+ 	} rep_push_record;
+ 	struct {
+ 	    int record_dom;
+ 	} rep_connect;
+     } u;
+ } replay_replay_op_t;
+ 
+ typedef struct {
+     unsigned long gpfn;
+     char * exec;
+     int len;
+ } replay_feedback_op_t;
+ 
+ typedef struct {
+     int dom;
+ } replay_poke_op_t;
+ 
+ typedef struct {
+   enum {
+     REPLAY_LOCKDOMAIN,
+     REPLAY_UNLOCKDOMAIN,
+     REPLAY_CREW_CMD,
+     REPLAY_BTS_UPDATE,
+     REPLAY_REC_CMD,
+     REPLAY_REP_CMD,
+     REPLAY_FEEDBACK_CMD,
+     REPLAY_POKE
+   }cmd;
+   union {
+     replay_lockdomain_t       lockdomain;
+     replay_unlockdomain_t     unlockdomain;
+     replay_crew_op_t	      crewop;
+     replay_record_op_t        recop;
+     replay_replay_op_t        repop;
+     replay_bts_state_t        bts_state;
+     replay_feedback_op_t      feedback;
+     replay_poke_op_t          poke;
+   } u;
+ } replay_op_t;
+ 
+ 
+ // Definition for per vcpu information.
+ //   we break down per vcpu to try to insure that
+ //   there won't be any contention between information in case
+ //   each vcpu ends upon a different processor...
+ struct tracer_exec_info {
+   // Event channel information if we need to notify on an event
+   long                    notify_port;
+   struct vcpu      *notify_domain;
+   
+   // Tracing information about which dom/exec_dom is being traced
+   unsigned      traced_domain;
+   unsigned      traced_vcpu;
+ 
+   pcounter_t    pcounter;
+ 
+   // Defines for the current state of the tracer  (needs work...)
+   unsigned long state;
+ #define       TI_LIVE          0xbeefcafe
+ #define       TI_DEAD          0x0
+ 
+   // Defines for what to do on an activation.
+   u_32           t_flags;
+ #define       TF_PAUSE        0
+ #define       TF_NOTIFY       1
+ 
+   // Current event's type
+   u_32           event;
+   u_32		count;
+ #define TRACE_EVENT_NONE      0
+ #define TRACE_EVENT_HYPERCALL 1
+ #define TRACE_EVENT_TBP       2
+ #define TRACE_EVENT_RDTSC     3
+   
+   union {
+     
+   } event_info;  
+ };
+ 
+ //
+ // Definition for the shared memory page.
+ //   If we want more than MAX_VCPUS, then we will need a multiple
+ //   pages.  May want to try to insure that each processor's info
+ //   maps to a different cache line?
+ //
+ // NOTE:  tip# 0 automatically gets notified for all synchronous events
+ //
+ 
+ #define MAX_VCPUS    (4088/sizeof(struct tracer_exec_info))
+ struct tracer_info_page {
+   unsigned                 tip_state;
+   #define                  TIP_LIVE          0xbeefcafe;
+   #define                  TIP_DEAD          0x0;
+ 
+   unsigned                 tip_num;
+   struct tracer_exec_info  exec_info[MAX_VCPUS];
+ };
+ 
+ #if 0
+ // Depracated...  Going...  Going...
+ struct tracer_page {
+     unsigned magic;
+     unsigned event;
+ #define TRACE_EVENT_HYPERCALL 1
+ #define TRACE_EVENT_TBP 2
+ #define TRACE_EVENT_RDTSC 3
+     unsigned dom;
+     unsigned call;
+     pcounter_t pcounter;
+ };
+ #endif
+ 
+ 
+ //TODO: Make the following used in both recording and replay.  
+ //      When events are queued up... ?
+ 
+ /* The replay ring consists of a four pointer ring of machine addresses
+    of replay buffers.  The buffers are then simple linear buffers of
+    events.
+ 
+    This somewhat complicated scheme is necessary because a simple ring
+    of events can't exceed 4k, which is only a few hundered events, and
+    we can't really get away with that little bandwidth.  At the same
+    time, each event is only a few dozen bytes, so giving each one its
+    own frame, as in the network or block rings, would be extremely
+    wasteful. */
+ 
+ /* Make a buffer just less than 4k. */
+ #define RECORD_EVENTS_PER_BUFFER (4084/sizeof(struct record_event))
+ struct record_event_buffer {
+     unsigned prod;
+     unsigned cons;
+     unsigned vcpu;
+     struct record_event events[RECORD_EVENTS_PER_BUFFER];
+ };
+ 
+ 
+ /* Rings are fairly simple: each entry is just an unsigned long giving
+    the mfn of the relevant buffer.  The logging domain is expected to
+    place empty buffers at empty_buffer_prod; Xen then takes them from
+    empty_buffer_cons when it needs them.  Xen produces full buffers at
+    full_buffer_prod, with the domain taking them off at
+    full_buffer_cons. */
+ 
+ /* Make a ring one page */
+ //#define RECORD_BUFFERS_PER_RING 1018
+ #define RECORD_BUFFERS_PER_RING 1010
+ struct record_event_ring {
+     unsigned full_buffer_prod, full_buffer_cons, full_buffer_clean;
+     unsigned empty_buffer_prod, empty_buffer_cons, empty_buffer_clean;
+     unsigned long buffers[RECORD_BUFFERS_PER_RING];
+ };
+ 
+ enum {
+     SHADOW_LOCK,
+     VCPU_WAIT_HYPERCALL_LOCK,
+     SHADOW_FAULT,
+     CREW_FAULT,
+     CREW_GET_PERM_1,
+     CREW_GET_PERM_2,
+     CREW_GET_PERM_3,
+     CREW_GET_PERM_4,
+     CREW_REMOVE_WRITE,
+     CREW_REMOVE_ALL,
+     CREW_LOG_LOCAL,
+     CREW_LOG_REMOTE,
+     CREW_LOG_DELAY,
+     PREPARE_EVENT,
+     PREPARE_EVENT_PAUSE,
+     PCTR_PCOUNTER_VAL,
+     POST_EVENT,
+     QUEUE_INSERT,
+     RECORD_COPY_INSERT,
+     QUEUE_SOFTIRQ,
+     QUEUE_UPDATE_LOG_WINDOW,
+     QUEUE_UPDATE_BUFFER_WINDOW,
+     QUEUE_CLEANER,
+     QUEUE_REPLAY_POKE,
+     QUEUE_GET_NEXT,
+     RECORD_COPY,
+     DOM0_SCHEDULE,
+     DOML_SCHEDULE,
+     USER_CLOCK_LOG_1,
+     USER_CLOCK_REPLAY_1,
+     USER_CLOCK_LOG_2,
+     USER_CLOCK_REPLAY_2,
+     DOM_MAX_CLOCKS
+ };
+ 
+ 
+ static const char *clock_names[DOM_MAX_CLOCKS]={
+     [SHADOW_LOCK]= "shadow_lock",
+     [VCPU_WAIT_HYPERCALL_LOCK]="waiting for hypercall lock",
+     [SHADOW_FAULT]= "shadow_fault",
+     [CREW_FAULT]= "CREW_FAULT",
+     [CREW_GET_PERM_1]= "crew.c:get_permission 1",
+     [CREW_GET_PERM_2]= "crew.c:get_permission 2",
+     [CREW_GET_PERM_3]= "crew.c:get_permission 3",
+     [CREW_GET_PERM_4]= "crew.c:get_permission 4",
+     [CREW_REMOVE_WRITE]= "preempt_all_read():remove_mappings_of_mfn",
+     [CREW_REMOVE_ALL]= "preempt_owner_write():remove_mappings_of_mfn",
+     [CREW_LOG_LOCAL]= "CREW_LOG_LOCAL",
+     [CREW_LOG_REMOTE]= "CREW_LOG_REMOTE",
+     [CREW_LOG_DELAY]= "CREW_LOG_DELAY",
+     [PREPARE_EVENT]="prepare_event()",
+     [PREPARE_EVENT_PAUSE]="vcpu_pause in prepare_event",
+     [PCTR_PCOUNTER_VAL]= "get_pcounter_val",
+     [POST_EVENT]="post_event()",
+     [QUEUE_INSERT]= "QUEUE_INSERT",
+     [RECORD_COPY]= "RecEvt Copy",
+     [RECORD_COPY_INSERT]= "RecEvt Copy(ins)",
+     [QUEUE_SOFTIRQ]= "QUEUE_SOFTIRQ",
+     [QUEUE_UPDATE_LOG_WINDOW]= "QUEUE_UPDATE_LOG_WINDOW",
+     [QUEUE_UPDATE_BUFFER_WINDOW]= "QUEUE_UPDATE_BUFFER_WINDOW",
+     [QUEUE_CLEANER]= "QUEUE_CLEANER",
+     [QUEUE_REPLAY_POKE]= "QUEUE_REPLAY_POKE",
+     [QUEUE_GET_NEXT]= "QUEUE_GET_NEXT",
+     [DOM0_SCHEDULE]="Domain 0",
+     [DOML_SCHEDULE]="Logging domain scheduled",
+     [USER_CLOCK_LOG_1]= "First (user, log)",
+     [USER_CLOCK_REPLAY_1]=    "First (user,rply)",    
+     [USER_CLOCK_LOG_2]= "Second(user, log)",
+     [USER_CLOCK_REPLAY_2]= "Second(user,rply)"
+ };
+ 
+ 
+ #if 0
+ #define CREW_REMOVE_WRITE 0
+ #define CREW_REMOVE_ALL 1
+ #define CREW_LOG_LOCAL 2
+ #define CREW_LOG_REMOTE 3
+ #define CREW_FAULT 4
+ #define QUEUE_INSERT 5
+ #define QUEUE_CLEANER 6
+ #define QUEUE_LOG_WINDOW 7
+ #define QUEUE_SOFTIRQ 8
+ #define QUEUE_GET_NEXT 9
+ #define QUEUE_UPDATE_LOG_WINDOW
+ #define RECORD_COPY 10
+ #define RECORD_COPY_INSERT 11
+ #define SHADOW_FAULT 12
+ #define PCTR_PCOUNTER_VAL 13
+ #endif
+ 
+ static const char *record_event_types[]={
+     [RECORD_EVT_NULL]="NULL",
+     [RECORD_EVT_HYPERRESULT]="HYPERRESULT",
+     [RECORD_EVT_DOMTIME]="DOMTIME",
+     [RECORD_EVT_EVTCHN_SET_PENDING]="EVTCHN_SET_PENDING",
+     [RECORD_EVT_CHECKPOINT]="CHECKPOINT",
+     [RECORD_EVT_RDTSC]="RDTSC",
+     [RECORD_EVT_HCALLBACK]="HCALLBACK",
+     [RECORD_EVT_NOTIFY]="NOTIFY",
+     [RECORD_EVT_CREW_EVENT]="CREW_EVENT",
+     [RECORD_EVT_CREW_CONSTRAINT]="CREW_CONSTRAINT",
+     [RECORD_EVT_BTS_STATE]="BTS_STATE",
+     [RECORD_EVT_UNPAUSE]="UNPAUSE",
+     [RECORD_EVT_REGISTER_CHECK]="REGISTER_CHECK",
+     [RECORD_EVT_SHADOW_SYNC]="SHADOW_SYNC",
+ };
+ 
+ static const char * constraint_source[] = {
+     [CSRC_DELAY]="DELAY",
+     [CSRC_FAULT]="FAULT",
+     [CSRC_SHARED_INFO_GP]="SI_GP",
+     [CSRC_SHARED_INFO_SPECIAL]="SI_SP",
+     [CSRC_SHADOW_GRAB_GPFN]="SGRAB",
+     [CSRC_BARRIER]="BARR",
+     [CSRC_FLUSH_TLB]="FLUSH",
+ };
+ 
+ static const char * register_check_where[] = {                            
+         [RCHECK_HYPERCALL]="HYPERCALL",
+         [RCHECK_PFAULT]="PFAULT",
+         [RCHECK_TRAP]="TRAP",
+         [RCHECK_GP]="GP",
+         [RCHECK_MATH]="MATH",
+         [RCHECK_INT3]="INT3",
+         [RCHECK_DEBUG]="DEBUG",
+         [RCHECK_HYPERCALL_POST]="HYPERCALL_POST",
+         [RCHECK_DA_EMULATION]="DA_EMULATION",
+         [RCHECK_SHADOW_MARK_OOS]="SHADOW_MARK_OOS",
+         [RCHECK_SHADOW_SYNC_LOCAL]="SHADOW_SYNC_LOCAL",
+         [RCHECK_SHADOW_SYNC_ATTEMPT]="SHADOW_SYNC_ATTEMPT",
+         [RCHECK_SHADOW_NORMAL_L1_PT_UPDATE]="SHADOW_NORMAL_L1_PT_UPDATE",
+         [RCHECK_SHADOW_NORMAL_L2_PT_UPDATE]="SHADOW_NORMAL_L2_PT_UPDATE",
+         [RCHECK_SHADOW_PROMOTE]="SHADOW_PROMOTE",
+         [RCHECK_CREW_FLUSH_TLB]="CREW_FLUSH_TLB",
+ };
+ static void useless(void) {
+     const char *str;
+     useless();
+     str=clock_names[0];
+     str=record_event_types[0];
+     str=register_check_where[0];
+     str=constraint_source[0];
+ }
+ 
+ #define assert_vcpu_not_running(_v)	\
+ 	ASSERT((!test_bit(_VCPUF_running, &_v->vcpu_flags)) \
+ 	       || (v->processor == smp_processor_id()))
+ 
+ #define REPLAY_BUFFER_LEN 1000
+ #define IS_REPLAY_TOOFULL(vvv)  (test_bit(_REPLAYFLAG_TOOFULL, &vvv->replay->replay_flags))
+ 
+ #define PCOUNTER_BREAK_THRESH 512
+ 
+ #endif /* __REPLAY_H__ */
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/record_module/common.h new/xen-3.3.0/tools/record_module/common.h
*** xen-3.3.0/tools/record_module/common.h	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/record_module/common.h	2009-05-26 15:59:08.000000000 -0400
***************
*** 0 ****
--- 1,184 ----
+ /******************************************************************************
+  * arch/xen/drivers/netif/backend/common.h
+  */
+ 
+ #ifndef __NETIF__BACKEND__COMMON_H__
+ #define __NETIF__BACKEND__COMMON_H__
+ 
+ #include <linux/config.h>
+ #include <linux/version.h>
+ #include <linux/module.h>
+ #include <linux/interrupt.h>
+ #include <linux/slab.h>
+ #include <linux/ip.h>
+ #include <linux/in.h>
+ //#include <linux/netdevice.h>
+ #include <linux/etherdevice.h>
+ //#include <asm-xen/ctrl_if.h>
+ #include <xen/interface/io/netif.h>
+ #include <asm/io.h>
+ #include <asm/pgalloc.h>
+ 
+ #if 0
+ #define ASSERT(_p) \
+     if ( !(_p) ) { printk("Assertion '%s' failed, line %d, file %s", #_p , \
+     __LINE__, __FILE__); *(int*)0=0; }
+ #define DPRINTK(_f, _a...) printk(KERN_ALERT "(file=%s, line=%d) " _f, \
+                            __FILE__ , __LINE__ , ## _a )
+ #else
+ #define ASSERT(_p) ((void)0)
+ #define DPRINTK(_f, _a...) ((void)0)
+ #endif
+ 
+ #define NETIF_TX_RING_SIZE 256
+ #define NETIF_RX_RING_SIZE 256
+ 
+ #define PACKED
+ 
+ #define NETIF_BE_STATUS_OKAY                0
+ /* Non-specific 'error' return. */
+ #define NETIF_BE_STATUS_ERROR               1
+ /* The following are specific error returns. */
+ #define NETIF_BE_STATUS_INTERFACE_EXISTS    2
+ #define NETIF_BE_STATUS_INTERFACE_NOT_FOUND 3
+ #define NETIF_BE_STATUS_INTERFACE_CONNECTED 4
+ #define NETIF_BE_STATUS_OUT_OF_MEMORY       5
+ #define NETIF_BE_STATUS_MAPPING_ERROR       6
+ 
+ typedef struct { 
+     /* IN */
+     domid_t    domid;         /*  0: Domain attached to new interface.   */
+     u16        __pad0;        /*  2 */
+     u32        netif_handle;  /*  4: Domain-specific interface handle.   */
+     u8         mac[6];        /*  8 */
+     u16        __pad1;        /* 14 */
+     u8         be_mac[6];     /* 16 */
+     u16        __pad2;        /* 22 */
+     /* OUT */
+     u32        status;        /* 24 */
+ } PACKED netif_be_create_t; /* 28 bytes */
+ 
+ typedef struct {
+     u8 type;     /*  0: echoed in response */
+     u8 subtype;  /*  1: echoed in response */
+     u8 id;       /*  2: echoed in response */
+     u8 length;   /*  3: number of bytes in 'msg' */
+     u8 msg[60];  /*  4: type-specific message data */
+ } PACKED control_msg_t; /* 64 bytes */
+ 
+ typedef control_msg_t ctrl_msg_t;
+ 
+ typedef unsigned int NETIF_RING_IDX;
+ typedef struct {
+     /*
+      * Frontend places packets into ring at tx_req_prod.
+      * Frontend receives event when tx_resp_prod passes tx_event.
+      * 'req_cons' is a shadow of the backend's request consumer -- the frontend
+      * may use it to determine if all queued packets have been seen by the
+      * backend.
+      */
+     NETIF_RING_IDX req_prod;       /*  0 */
+     NETIF_RING_IDX req_cons;       /*  4 */
+     NETIF_RING_IDX resp_prod;      /*  8 */
+     NETIF_RING_IDX event;          /* 12 */
+     union {                        /* 16 */
+         netif_tx_request_t  req;
+         netif_tx_response_t resp;
+     } ring[NETIF_TX_RING_SIZE];
+ } netif_tx_interface_t;
+ 
+ typedef struct {
+     /*
+      * Frontend places empty buffers into ring at rx_req_prod.
+      * Frontend receives event when rx_resp_prod passes rx_event.
+      */
+     NETIF_RING_IDX req_prod;       /*  0 */
+     NETIF_RING_IDX resp_prod;      /*  4 */
+     NETIF_RING_IDX event;          /*  8 */
+     union {                        /* 12 */
+         netif_rx_request_t  req;
+         netif_rx_response_t resp;
+     } ring[NETIF_RX_RING_SIZE];
+ } netif_rx_interface_t;
+ 
+ typedef struct netif_st {
+     /* Unique identifier for this interface. */
+     domid_t          domid;
+     unsigned int     handle;
+ 
+     u8               fe_dev_addr[6];
+ 
+     /* Physical parameters of the comms window. */
+     unsigned long    tx_shmem_frame;
+     unsigned long    rx_shmem_frame;
+     unsigned int     evtchn;
+     int              irq;
+ 
+     /* The shared rings and indexes. */
+     netif_tx_interface_t *tx;
+     netif_rx_interface_t *rx;
+ 
+     /* Private indexes into shared ring. */
+     NETIF_RING_IDX rx_req_cons;
+     NETIF_RING_IDX rx_resp_prod; /* private version of shared variable */
+     NETIF_RING_IDX tx_req_cons;
+     NETIF_RING_IDX tx_resp_prod; /* private version of shared variable */
+ 
+     /* Transmit shaping: allow 'credit_bytes' every 'credit_usec'. */
+     unsigned long   credit_bytes;
+     unsigned long   credit_usec;
+     unsigned long   remaining_credit;
+     struct timer_list credit_timeout;
+ 
+     /* Miscellaneous private stuff. */
+     enum { DISCONNECTED, DISCONNECTING, CONNECTED } status;
+     int active;
+     /*
+      * DISCONNECT response is deferred until pending requests are ack'ed.
+      * We therefore need to store the id from the original request.
+      */
+     u8               disconnect_rspid;
+     struct netif_st *hash_next;
+     struct list_head list;  /* scheduling list */
+     atomic_t         refcnt;
+     struct net_device *dev;
+     struct net_device_stats stats;
+ 
+     /* Are we supposed to notify the hypervisor whenever we do
+        something on this interface? */
+     unsigned replay_monitored:1;
+     /* Stop any changes being made to any data structures shared with
+        the remote. */
+     unsigned plugged:1;
+     struct work_struct work;
+ } netif_t;
+ 
+ /*
+ void netif_create(netif_be_create_t *create);
+ void netif_destroy(netif_be_destroy_t *destroy);
+ void netif_creditlimit(netif_be_creditlimit_t *creditlimit);
+ void netif_connect(netif_be_connect_t *connect);
+ int  netif_disconnect(netif_be_disconnect_t *disconnect, u8 rsp_id);
+ void netif_disconnect_complete(netif_t *netif);
+ netif_t *netif_find_by_handle(domid_t domid, unsigned int handle);
+ */
+ 
+ #define netif_get(_b) (atomic_inc(&(_b)->refcnt))
+ #define netif_put(_b)                             \
+     do {                                          \
+         if ( atomic_dec_and_test(&(_b)->refcnt) ) \
+             netif_disconnect_complete(_b);        \
+     } while (0)
+ 
+ void netif_interface_init(void);
+ void netif_ctrlif_init(void);
+ 
+ void netif_schedule_work(netif_t *netif);
+ void netif_deschedule_work(netif_t *netif);
+ 
+ int netif_be_start_xmit(struct sk_buff *skb, struct net_device *dev);
+ struct net_device_stats *netif_be_get_stats(struct net_device *dev);
+ irqreturn_t netif_be_int(int irq, void *dev_id, struct pt_regs *regs);
+ 
+ #endif /* __NETIF__BACKEND__COMMON_H__ */
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/record_module/interface.h new/xen-3.3.0/tools/record_module/interface.h
*** xen-3.3.0/tools/record_module/interface.h	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/record_module/interface.h	2009-06-12 15:48:50.000000000 -0400
***************
*** 0 ****
--- 1,315 ----
+ /* struct and function for netif*/
+ #include <linux/module.h>
+ 
+ typedef unsigned int NETIF_RING_IDX;
+ typedef unsigned long memory_t;
+ 
+ #define MEMORY_PADDING
+ #define NETIF_TX_RING_SIZE 256
+ #define NETIF_RX_RING_SIZE 256
+ #define NETIF_HASHSZ 1024
+ 
+ #define NETIF_HASH(_d,_h) (((int)(_d)^(int)(_h))&(NETIF_HASHSZ-1))
+ #define RECORD_IOCTL_UNPLUG _IOR('R', 2, struct record_ioctl_plug)
+ 
+ #define RECORD_IOCTL_ATTACH _IOR('R', 3, struct record_ioctl_attach)
+ 
+ typedef struct {
+     memory_t addr;   /*  0: Machine address of packet.  */
+     MEMORY_PADDING;
+     u16      csum_blank:1; /* Proto csum field blank?   */
+     u16      id:15;  /*  8: Echoed in response message. */
+     u16      size;   /* 10: Packet size in bytes.       */
+ } netif_tx_request_t; /* 12 bytes */
+ typedef struct {
+     u16      id;     /*  0 */
+     s8       status; /*  2 */
+     u8       __pad;  /*  3 */
+ } netif_tx_response_t; /* 4 bytes */
+ 
+ typedef struct {
+     u16       id;    /*  0: Echoed in response message.        */
+ } netif_rx_request_t; /* 2 bytes */
+ typedef struct {
+     memory_t addr;   /*  0: Machine address of packet.              */
+     MEMORY_PADDING;
+     u16      csum_valid:1; /* Protocol checksum is validated?       */
+     u16      id:15;  /*  8:  */
+     s16      status; /* 10: -ve: BLKIF_RSP_* ; +ve: Rx'ed pkt size. */
+ } netif_rx_response_t; /* 12 bytes */
+ 
+ typedef struct {
+     /*
+      * Frontend places packets into ring at tx_req_prod.
+      * Frontend receives event when tx_resp_prod passes tx_event.
+      * 'req_cons' is a shadow of the backend's request consumer -- the frontend
+      * may use it to determine if all queued packets have been seen by the
+      * backend.
+      */
+     NETIF_RING_IDX req_prod;       /*  0 */
+     NETIF_RING_IDX req_cons;       /*  4 */
+     NETIF_RING_IDX resp_prod;      /*  8 */
+     NETIF_RING_IDX event;          /* 12 */
+     union {                        /* 16 */
+         netif_tx_request_t  req;
+         netif_tx_response_t resp;
+     } ring[NETIF_TX_RING_SIZE];
+ } netif_tx_interface_t;
+ 
+ /* This structure must fit in a memory page. */
+ typedef struct {
+     /*
+      * Frontend places empty buffers into ring at rx_req_prod.
+      * Frontend receives event when rx_resp_prod passes rx_event.
+      */
+     NETIF_RING_IDX req_prod;       /*  0 */
+     NETIF_RING_IDX resp_prod;      /*  4 */
+     NETIF_RING_IDX event;          /*  8 */
+     union {                        /* 12 */
+         netif_rx_request_t  req;
+         netif_rx_response_t resp;
+     } ring[NETIF_RX_RING_SIZE];
+ } netif_rx_interface_t;
+ 
+ struct net_device_stats
+ {
+ 	unsigned long	rx_packets;		/* total packets received	*/
+ 	unsigned long	tx_packets;		/* total packets transmitted	*/
+ 	unsigned long	rx_bytes;		/* total bytes received 	*/
+ 	unsigned long	tx_bytes;		/* total bytes transmitted	*/
+ 	unsigned long	rx_errors;		/* bad packets received		*/
+ 	unsigned long	tx_errors;		/* packet transmit problems	*/
+ 	unsigned long	rx_dropped;		/* no space in linux buffers	*/
+ 	unsigned long	tx_dropped;		/* no space available in linux	*/
+ 	unsigned long	multicast;		/* multicast packets received	*/
+ 	unsigned long	collisions;
+ 
+ 	/* detailed rx_errors: */
+ 	unsigned long	rx_length_errors;
+ 	unsigned long	rx_over_errors;		/* receiver ring buff overflow	*/
+ 	unsigned long	rx_crc_errors;		/* recved pkt with crc error	*/
+ 	unsigned long	rx_frame_errors;	/* recv'd frame alignment error */
+ 	unsigned long	rx_fifo_errors;		/* recv'r fifo overrun		*/
+ 	unsigned long	rx_missed_errors;	/* receiver missed packet	*/
+ 
+ 	/* detailed tx_errors */
+ 	unsigned long	tx_aborted_errors;
+ 	unsigned long	tx_carrier_errors;
+ 	unsigned long	tx_fifo_errors;
+ 	unsigned long	tx_heartbeat_errors;
+ 	unsigned long	tx_window_errors;
+ 	
+ 	/* for cslip etc */
+ 	unsigned long	rx_compressed;
+ 	unsigned long	tx_compressed;
+ };
+ 
+ typedef struct netif_st {
+     /* Unique identifier for this interface. */
+     domid_t          domid;
+     unsigned int     handle;
+ 
+     u8               fe_dev_addr[6];
+ 
+     /* Physical parameters of the comms window. */
+     unsigned long    tx_shmem_frame;
+     unsigned long    rx_shmem_frame;
+     unsigned int     evtchn;
+     int              irq;
+ 
+     /* The shared rings and indexes. */
+     netif_tx_interface_t *tx;
+     netif_rx_interface_t *rx;
+ 
+     /* Private indexes into shared ring. */
+     NETIF_RING_IDX rx_req_cons;
+     NETIF_RING_IDX rx_resp_prod; /* private version of shared variable */
+     NETIF_RING_IDX tx_req_cons;
+     NETIF_RING_IDX tx_resp_prod; /* private version of shared variable */
+ 
+     /* Transmit shaping: allow 'credit_bytes' every 'credit_usec'. */
+     unsigned long   credit_bytes;
+     unsigned long   credit_usec;
+     unsigned long   remaining_credit;
+     struct timer_list credit_timeout;
+ 
+     /* Miscellaneous private stuff. */
+     enum { DISCONNECTED, DISCONNECTING, CONNECTED } status;
+     int active;
+     /*
+      * DISCONNECT response is deferred until pending requests are ack'ed.
+      * We therefore need to store the id from the original request.
+      */
+     u8               disconnect_rspid;
+     struct netif_st *hash_next;
+     struct list_head list;  /* scheduling list */
+     atomic_t         refcnt;
+     struct net_device *dev;
+     struct net_device_stats stats;
+ 
+     /* Are we supposed to notify the hypervisor whenever we do
+        something on this interface? */
+     unsigned replay_monitored:1;
+     /* Stop any changes being made to any data structures shared with
+        the remote. */
+     unsigned plugged:1;
+     struct work_struct work;
+ } netif_t;
+ 
+ static netif_t *netif_hash[NETIF_HASHSZ];
+ 
+ void netif_interface_init(void)
+ {
+     memset(netif_hash, 0, sizeof(netif_hash));
+ }
+ 
+ netif_t *netif_find_by_handle(domid_t domid, unsigned int handle)
+ {
+     netif_t *netif = netif_hash[NETIF_HASH(domid, handle)];
+     while ( (netif != NULL) && 
+             ((netif->domid != domid) || (netif->handle != handle)) )
+         netif = netif->hash_next;
+     return netif;
+ }
+ 
+ int do_plug_netif(unsigned dom, unsigned handle)
+ {
+     netif_t *netif;
+     netif = netif_find_by_handle(dom, handle);
+ 
+ 	if (!netif)
+ 		return -EINVAL;
+ 	
+ 	netif->plugged = 1;
+ 	return 0;
+ }
+ 
+ #define softirq_pending(cpu)	__IRQ_STAT((cpu), __softirq_pending)
+ #define __cpu_raise_softirq(cpu, nr) do { softirq_pending(cpu) |= 1UL << (nr); } while (0)
+ 
+ typedef struct {
+ 	unsigned int __softirq_pending;
+ 	unsigned int __local_irq_count;
+ 	unsigned int __local_bh_count;
+ 	unsigned int __syscall_count;
+ 	struct task_struct * __ksoftirqd_task; /* waitqueue is too large */
+ 	unsigned int __nmi_count;	/* arch dependent */
+ }irq_cpustat_tt;
+ 
+ irq_cpustat_tt irq_state[NR_CPUS];
+ 
+ #ifdef CONFIG_SMP
+ #define __IRQ_STATE(cpu, member)	(irq_state[cpu].member)
+ #else
+ #define __IRQ_STATE(cpu, member)	((void)(cpu), irq_state[0].member)
+ #endif	
+ #define local_irq_count(cpu)	__IRQ_STATE((cpu), __local_irq_count)
+ #define local_bh_count(cpu)	__IRQ_STATE((cpu), __local_bh_count)
+ #define ksoftirqd_task(cpu)	__IRQ_STATE((cpu), __ksoftirqd_task)
+ 
+ static inline void wakeup_softirqd(unsigned cpu)
+ {
+ 	struct task_struct * tsk = ksoftirqd_task(cpu);
+ 
+ 	if (tsk && tsk->state != TASK_RUNNING)
+ 		wake_up_process(tsk);
+ }
+ 
+ inline void cpu_raise_softirq(unsigned int cpu, unsigned int nr)
+ {
+ 	__cpu_raise_softirq(cpu, nr);
+ 
+ 	/*
+ 	 * If we're in an interrupt or bh, we're done
+ 	 * (this also catches bh-disabled code). We will
+ 	 * actually run the softirq once we return from
+ 	 * the irq or bh.
+ 	 *
+ 	 * Otherwise we wake up ksoftirqd to make sure we
+ 	 * schedule the softirq soon.
+ 	 */
+ 	if (!(local_irq_count(cpu) | local_bh_count(cpu)))
+ 		wakeup_softirqd(cpu);
+ }
+ 
+ #ifndef __cacheline_aligned
+ #ifdef MODULE
+ #define __cacheline_aligned ____cacheline_aligned
+ #else
+ #define __cacheline_aligned					\
+   __attribute__((__aligned__(SMP_CACHE_BYTES),			\
+ 		 __section__(".data.cacheline_aligned")))
+ #endif
+ #endif /* __cacheline_aligned */
+ 
+ struct tasklet_head
+ {
+ 	struct tasklet_struct *list;
+ } __attribute__ ((__aligned__(SMP_CACHE_BYTES)));
+ 
+ struct tasklet_head tasklet_vec[NR_CPUS] __cacheline_aligned;
+ 
+ void __tasklet_schedule(struct tasklet_struct *t)
+ {
+ 	int cpu = smp_processor_id();
+ 	unsigned long flags;
+ 
+ 	local_irq_save(flags);
+ 	t->next = tasklet_vec[cpu].list;
+ 	tasklet_vec[cpu].list = t;
+ 	cpu_raise_softirq(cpu, TASKLET_SOFTIRQ);
+ 	local_irq_restore(flags);
+ }
+ 
+ int do_unplug_netif(unsigned dom, unsigned handle)
+ {
+     netif_t *netif;
+     struct tasklet_struct net_rx_tasklet, net_tx_tasklet;
+     netif = netif_find_by_handle(dom, handle);
+ 
+ 	if (!netif)
+ 		return -EINVAL;
+ 
+ 	netif->plugged = 0;
+ 	tasklet_schedule(&net_rx_tasklet);
+ 	tasklet_schedule(&net_tx_tasklet);
+ 	
+ 	return 0;
+ }
+ 
+ /* record net attach */
+ int do_record_netif_attach(unsigned port, unsigned *rx_ring_mfn,
+ 			   unsigned *tx_ring_mfn, unsigned *handle)
+ {
+     netif_t *netif;
+     int x;
+ 
+     /* XXXSOS22: I can't help but feel that this needs some locks
+      * somewhere... */
+     for (x = 0; x < NETIF_HASHSZ; x++) {
+ 	    netif = netif_hash[x];
+ 	
+ 	    while (netif) {
+ 	        printk("<0>Considering returning %d.\n", netif->evtchn);
+ 	    
+ 		    if (netif->evtchn == port)
+ 		        break;
+ 	        netif = netif->hash_next;
+ 	    }
+ 
+ 	    if (netif)
+ 	        break;
+     }
+ 
+     if (netif == NULL) {
+ 	    return -EINVAL;
+     }
+     *rx_ring_mfn = netif->rx_shmem_frame;
+     *tx_ring_mfn = netif->tx_shmem_frame;
+     *handle = netif->handle;
+     if (netif->replay_monitored)
+ 	return -EBUSY;
+     netif->replay_monitored = 1;
+     return 0;
+ }
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/record_module/Makefile new/xen-3.3.0/tools/record_module/Makefile
*** xen-3.3.0/tools/record_module/Makefile	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/record_module/Makefile	2009-05-26 15:59:08.000000000 -0400
***************
*** 0 ****
--- 1,19 ----
+ #filename must be Makefile
+ 
+ ifneq ($(KERNELRELEASE),)  
+ #module-objs:=record_module.o #netif.o 
+ #obj-m:=module.o
+ obj-m:=record_module.o
+ else    
+ #KDIR:=/usr/src/kernels/2.6.23.1-42.fc8-x86_64
+ KDIR:=/lib/modules/2.6.18.8-xen/build
+ PWD:=$(shell pwd)
+      
+ default:
+ 	cp interface.h /lib/modules/2.6.18.8-xen/source/include/
+ 	$(MAKE) -C $(KDIR) M=$(PWD) modules
+ .PHONY:clean
+ clean:
+ 	rm *.o *.ko
+ endif 
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/record_module/module.mod.c new/xen-3.3.0/tools/record_module/module.mod.c
*** xen-3.3.0/tools/record_module/module.mod.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/record_module/module.mod.c	2009-05-26 15:59:08.000000000 -0400
***************
*** 0 ****
--- 1,49 ----
+ #include <linux/module.h>
+ #include <linux/vermagic.h>
+ #include <linux/compiler.h>
+ 
+ MODULE_INFO(vermagic, VERMAGIC_STRING);
+ 
+ struct module __this_module
+ __attribute__((section(".gnu.linkonce.this_module"))) = {
+  .name = KBUILD_MODNAME,
+  .init = init_module,
+ #ifdef CONFIG_MODULE_UNLOAD
+  .exit = cleanup_module,
+ #endif
+ };
+ 
+ static const struct modversion_info ____versions[]
+ __attribute_used__
+ __attribute__((section("__versions"))) = {
+ 	{ 0xad34a535, "struct_module" },
+ 	{ 0x1a5ef3d2, "phys_to_machine_mapping" },
+ 	{ 0x9b388444, "get_zeroed_page" },
+ 	{ 0x9bd5b644, "malloc_sizes" },
+ 	{ 0xef22ade, "remove_wait_queue" },
+ 	{ 0xe4a41b64, "HYPERVISOR_shared_info" },
+ 	{ 0x55526907, "xen_features" },
+ 	{ 0xefe8e01c, "end_pfn" },
+ 	{ 0xffd5a395, "default_wake_function" },
+ 	{ 0xde0bdcff, "memset" },
+ 	{ 0xdd132261, "printk" },
+ 	{ 0xbe499d81, "copy_to_user" },
+ 	{ 0x1aab7530, "kmem_cache_alloc" },
+ 	{ 0x1000e51, "schedule" },
+ 	{ 0x89c7132a, "register_chrdev" },
+ 	{ 0xda4d351, "wake_up_process" },
+ 	{ 0x19cacd0, "init_waitqueue_head" },
+ 	{ 0x6989875b, "add_wait_queue" },
+ 	{ 0x37a0cba, "kfree" },
+ 	{ 0xc192d491, "unregister_chrdev" },
+ 	{ 0x945bc6a7, "copy_from_user" },
+ 	{ 0xabc28976, "force_evtchn_callback" },
+ };
+ 
+ static const char __module_depends[]
+ __attribute_used__
+ __attribute__((section(".modinfo"))) =
+ "depends=";
+ 
+ 
+ MODULE_INFO(srcversion, "5E286057167A91FEAC280FA");
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/record_module/netif.c new/xen-3.3.0/tools/record_module/netif.c
*** xen-3.3.0/tools/record_module/netif.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/record_module/netif.c	2009-05-26 15:59:08.000000000 -0400
***************
*** 0 ****
--- 1,198 ----
+ #include <linux/kernel.h>
+ #include <linux/module.h>
+ #include "common.h"
+ 
+ #define NETIF_HASHSZ 1024
+ #define NETIF_HASH(_d,_h) (((int)(_d)^(int)(_h))&(NETIF_HASHSZ-1))
+ 
+ static netif_t *netif_hash[NETIF_HASHSZ];
+ 
+ void netif_init(void)
+ {
+     printk("hello, I am here\n");
+ }
+ 
+ static void __netif_up(netif_t *netif)
+ {
+     struct net_device *dev = netif->dev;
+     spin_lock_bh(&dev->xmit_lock);
+     netif->active = 1;
+     spin_unlock_bh(&dev->xmit_lock);
+     (void)request_irq(netif->irq, netif_be_int, 0, dev->name, netif);
+     netif_schedule_work(netif);
+ }
+ 
+ static void __netif_down(netif_t *netif)
+ {
+     struct net_device *dev = netif->dev;
+     spin_lock_bh(&dev->xmit_lock);
+     netif->active = 0;
+     spin_unlock_bh(&dev->xmit_lock);
+     free_irq(netif->irq, netif);
+     netif_deschedule_work(netif);
+ }
+ 
+ static int net_open(struct net_device *dev)
+ {
+     netif_t *netif = netdev_priv(dev);
+     if ( netif->status == CONNECTED )
+         __netif_up(netif);
+     netif_start_queue(dev);
+     return 0;
+ }
+ 
+ static int net_close(struct net_device *dev)
+ {
+     netif_t *netif = netdev_priv(dev);
+     netif_stop_queue(dev);
+     if ( netif->status == CONNECTED )
+         __netif_down(netif);
+     return 0;
+ }
+ 
+ void netif_create(netif_be_create_t *create)
+ {
+     int                err = 0;
+     domid_t            domid  = create->domid;
+     unsigned int       handle = create->netif_handle;
+     struct net_device *dev;
+     netif_t          **pnetif, *netif;
+     char               name[IFNAMSIZ] = {};
+ 
+     snprintf(name, IFNAMSIZ - 1, "vif%u.%u", domid, handle);
+     dev = alloc_netdev(sizeof(netif_t), name, ether_setup);
+     if ( dev == NULL )
+     {
+         DPRINTK("Could not create netif: out of memory\n");
+         create->status = NETIF_BE_STATUS_OUT_OF_MEMORY;
+         return;
+     }
+ 
+     netif = netdev_priv(dev);
+     memset(netif, 0, sizeof(*netif));
+     netif->domid  = domid;
+     netif->handle = handle;
+     netif->status = DISCONNECTED;
+     atomic_set(&netif->refcnt, 0);
+     netif->dev = dev;
+ 
+     netif->credit_bytes = netif->remaining_credit = ~0UL;
+     netif->credit_usec  = 0UL;
+     init_timer(&netif->credit_timeout);
+ 
+     pnetif = &netif_hash[NETIF_HASH(domid, handle)];
+     while ( *pnetif != NULL )
+     {
+         if ( ((*pnetif)->domid == domid) && ((*pnetif)->handle == handle) )
+         {
+             DPRINTK("Could not create netif: already exists\n");
+             create->status = NETIF_BE_STATUS_INTERFACE_EXISTS;
+             free_netdev(dev);
+             return;
+         }
+         pnetif = &(*pnetif)->hash_next;
+     }
+ 
+     dev->hard_start_xmit = netif_be_start_xmit;
+     dev->get_stats       = netif_be_get_stats;
+     dev->open            = net_open;
+     dev->stop            = net_close;
+     dev->features        = NETIF_F_NO_CSUM;
+ 
+     /* Disable queuing. */
+     dev->tx_queue_len = 0;
+ 
+     if ( (create->be_mac[0] == 0) && (create->be_mac[1] == 0) &&
+          (create->be_mac[2] == 0) && (create->be_mac[3] == 0) &&
+          (create->be_mac[4] == 0) && (create->be_mac[5] == 0) )
+     {
+         /*
+          * Initialise a dummy MAC address. We choose the numerically largest
+          * non-broadcast address to prevent the address getting stolen by an
+          * Ethernet bridge for STP purposes. (FE:FF:FF:FF:FF:FF)
+          */ 
+         memset(dev->dev_addr, 0xFF, ETH_ALEN);
+         dev->dev_addr[0] &= ~0x01;
+     }
+     else
+     {
+         memcpy(dev->dev_addr, create->be_mac, ETH_ALEN);
+     }
+ 
+     memcpy(netif->fe_dev_addr, create->mac, ETH_ALEN);
+ 
+     rtnl_lock();
+     err = register_netdevice(dev);
+     rtnl_unlock();
+ 
+     if ( err != 0 )
+     {
+         DPRINTK("Could not register new net device %s: err=%d\n",
+                 dev->name, err);
+         create->status = NETIF_BE_STATUS_OUT_OF_MEMORY;
+         free_netdev(dev);
+         return;
+     }
+ 
+     netif->hash_next = *pnetif;
+     *pnetif = netif;
+ 
+     DPRINTK("Successfully created netif\n");
+     create->status = NETIF_BE_STATUS_OKAY;
+ }
+ 
+ #define CMSG_NETIF_BE_CREATE      0  /* Create a new net-device interface. */
+ #define CMSG_NETIF_BE_DESTROY     1  /* Destroy a net-device interface.    */
+ #define CMSG_NETIF_BE_CONNECT     2  /* Connect i/f to remote driver.        */
+ #define CMSG_NETIF_BE_DISCONNECT  3  /* Disconnect i/f from remote driver.   */
+ #define CMSG_NETIF_BE_CREDITLIMIT 4  /* Limit i/f to a given credit limit. */
+ 
+ static void netif_ctrlif_rx(ctrl_msg_t *msg, unsigned long id)
+ {
+     DPRINTK("Received netif backend message, subtype=%d\n", msg->subtype);
+     
+     switch ( msg->subtype )
+     {
+     case CMSG_NETIF_BE_CREATE:
+         netif_create((netif_be_create_t *)&msg->msg[0]);
+         break;        
+     case CMSG_NETIF_BE_DESTROY:
+         //netif_destroy((netif_be_destroy_t *)&msg->msg[0]);
+         break;  
+     case CMSG_NETIF_BE_CREDITLIMIT:
+         //netif_creditlimit((netif_be_creditlimit_t *)&msg->msg[0]);
+         break;       
+     case CMSG_NETIF_BE_CONNECT:
+         //netif_connect((netif_be_connect_t *)&msg->msg[0]);
+         break; 
+     case CMSG_NETIF_BE_DISCONNECT:
+         //if ( !netif_disconnect((netif_be_disconnect_t *)&msg->msg[0],msg->id) )
+             //return; /* Sending the response is deferred until later. */
+         break;        
+     default:
+         DPRINTK("Parse error while reading message subtype %d, len %d\n",
+             msg->subtype, msg->length);
+         msg->length = 0;
+         break;
+     }
+ 
+     ctrl_if_send_response(msg);
+ }
+ 
+ void netif_ctrlif_init(void)
+ {
+     ctrl_msg_t cmsg;
+     netif_be_driver_status_t st;
+ 
+     (void)ctrl_if_register_receiver(CMSG_NETIF_BE, netif_ctrlif_rx,  //CMSG_NETIF_BE 3
+                                     CALLBACK_IN_BLOCKING_CONTEXT);  //1 == CALLBACK_IN_BLOCKING_CONTEXT
+ 
+     /* Send a driver-UP notification to the domain controller. */
+     cmsg.type      = CMSG_NETIF_BE;
+     cmsg.subtype   = CMSG_NETIF_BE_DRIVER_STATUS;
+     cmsg.length    = sizeof(netif_be_driver_status_t);
+     st.status      = NETIF_DRIVER_STATUS_UP;
+     memcpy(cmsg.msg, &st, sizeof(st));
+     ctrl_if_send_message_block(&cmsg, NULL, 0, TASK_UNINTERRUPTIBLE);
+ }
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/record_module/record_module.c new/xen-3.3.0/tools/record_module/record_module.c
*** xen-3.3.0/tools/record_module/record_module.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/record_module/record_module.c	2009-06-12 17:10:49.000000000 -0400
***************
*** 0 ****
--- 1,362 ----
+ #include <linux/config.h>
+ #include <linux/poll.h>
+ #include <linux/init.h>
+ #include <linux/kernel.h>
+ #include <linux/module.h>
+ #include <linux/slab.h>
+ #include <linux/errno.h>
+ #include <linux/irq.h>
+ #include <linux/fs.h>
+ #include <linux/uaccess.h>
+ #include <asm/uaccess.h>
+ #include <replay.h>
+ #include <linux/interrupt.h>
+ #include <linux/sched.h>
+ #include <linux/gfp.h>
+ #include <xen/evtchn.h>
+ #include <asm-generic/page.h>
+ #include "interface.h"
+ 
+ #define __HYPERVISOR_replay_op    39
+ 
+ #define DEVICE_NAME "module"
+ static int Major = -1;
+ static int used = 0;
+ 
+ #define machine_to_virt(m)	(__va(m))
+ 
+ static wait_queue_head_t record_waitq;
+ static wait_queue_head_t record_wait;
+ unsigned event_port;
+ 
+ struct record_event_buffer *event_buffer;
+ static struct record_event_ring *event_ring;
+ 
+ static irqreturn_t record_irq_handler(int ig, void *ignore, struct pt_regs *ign)
+ {
+     //printk("record_irq_handler\n");
+     if (event_ring == NULL)
+ 	BUG();
+     wake_up(&record_waitq);
+     return IRQ_HANDLED;
+ }
+ 
+ static void poke_logging(void) {
+     long error_code;
+     replay_op_t rop;
+     memset(&rop, 0, sizeof(rop));
+     printk("Poking...\n");
+     rop.cmd=REPLAY_POKE;  //enum cmd,REPLAY_POKE==7
+     //rop.u.poke.dom=targ;
+ 
+ 	__asm__ __volatile__ (
+         "int $0x82"
+         : "=a" (error_code)
+         : "0" (__HYPERVISOR_replay_op), 
+ 	  "b" (&rop));
+ }
+ 
+ static int get_next_event_buffer(void)
+ {
+         unsigned full_idx;
+ 	unsigned long full_mfn;
+ 	int ret = 0;
+ 	struct task_struct *tsk = current;
+ 	DECLARE_WAITQUEUE(wait, tsk);
+ 
+ 	if(event_buffer && (event_buffer->cons < event_buffer->prod))
+ 			return 0;
+ 	add_wait_queue(&record_waitq, &wait);
+ 
+ 	set_task_state(tsk, TASK_INTERRUPTIBLE);
+ 
+ 	if(!(event_ring->full_buffer_prod > event_ring->full_buffer_cons)){
+ 	    poke_logging();
+ 		wmb();
+ 	}
+ 
+ 	while(!(event_ring->full_buffer_prod > event_ring->full_buffer_cons)){
+                 schedule();
+ 		if(signal_pending(tsk)){
+ 		    ret = -EINTR;
+ 			break;
+ 		}
+ 		set_task_state(tsk, TASK_INTERRUPTIBLE);
+ 	}
+ 
+ 	__set_task_state(tsk, TASK_RUNNING);
+ 	remove_wait_queue(&record_waitq, &wait);
+ 
+ 	if (0 > ret)
+ 		return ret;
+ 
+ 	full_idx = event_ring->full_buffer_cons % RECORD_BUFFERS_PER_RING;
+ 	event_ring->full_buffer_cons++;
+ 	rmb();
+ 	full_mfn = event_ring->buffers[full_idx];
+ 
+ 	event_buffer = machine_to_virt(full_mfn << PAGE_SHIFT);
+ 
+ 	return 0;
+ }
+ 
+ static int record_open(struct inode *inode, struct file *file)
+ {
+ 
+     printk("==========\n record module open\n");
+ 
+ 	if(used)
+ 	    printk("module busy\n");
+ 
+ 	used = 1;
+ 	init_waitqueue_head(&record_wait);  //wait.h
+ 	try_module_get(THIS_MODULE);
+ 
+     return 0;
+ }
+ 
+ static ssize_t record_read(struct file *file, char *buf, 
+ 				size_t count, loff_t *pos) 
+ {
+     int ret;
+ 	unsigned int to_copy;
+ 	ret = get_next_event_buffer();
+ 	if(0 > ret)
+ 		return ret;
+ 
+     to_copy = event_buffer->prod - event_buffer->cons;
+ 	if(to_copy * sizeof(struct record_event) >= count)
+ 		to_copy = count /sizeof(struct record_event);
+ 	printk("consuming %d\n", to_copy);
+ 
+ 	if(copy_to_user(buf, &event_buffer->events[event_buffer->cons],
+ 							to_copy *sizeof(struct record_event)))
+ 			return -EFAULT;
+ 	event_buffer->cons += to_copy;
+ 
+     if(event_buffer->cons == event_buffer->prod){
+ 	    event_buffer->cons = 0;
+ 		event_buffer->prod = 0;
+ 	}
+ 
+ 
+ 	return to_copy * sizeof(struct record_event);
+ }
+ 
+ /*
+ int my_call(unsigned port)
+ {
+     unsigned c;
+     int (*p)();
+     p=0xc024a5f0;
+ 
+     c = (*p)(port);
+ 
+     return c;
+ }
+ */
+ 
+ static ssize_t record_write(struct file *file, const char *buf,
+ 				size_t count, loff_t *pos)
+ {
+ 	int i;
+ 	int targ;
+ 	int ret;
+ 	unsigned irq;
+ 
+ 	if(count != 4){
+ 		printk("Bad count in write");
+ 	    return -EINVAL;
+ 	}
+ 
+ 	printk("count:%d\n", (int)count);
+ 
+ 	if(copy_from_user(&targ, buf, sizeof(targ))){
+             printk("bad buf");
+ 		for(i=0; i<count; i++){
+ 		    printk("buf:%2x\n",buf[i]);
+ 		}
+ 		return -EFAULT;
+ 	}
+ 	
+ 	event_ring = (void *)get_zeroed_page(GFP_KERNEL);
+ 	if(NULL == event_ring){
+         printk("No memory for write\n");
+ 		return -ENOMEM;
+ 	}
+ 
+         printk("Connecting record module to domain %d\n",targ);
+ 
+         event_ring->full_buffer_prod = event_ring->full_buffer_cons = 0;
+ 
+ #define NUM_PAGES 1000
+ 	{
+ 	        void *page;
+ 		unsigned int *page_list;
+ 		replay_op_t rop;
+ 		long error_code = 0;
+ 		page_list = kmalloc(NUM_PAGES*sizeof(unsigned int),GFP_KERNEL);
+ 
+ 		for(i = 0; i < NUM_PAGES; i++){
+                 page = (void*)get_zeroed_page(GFP_KERNEL);
+ 
+ 			if(NULL == page){
+ 				printk("failed to get zeroed page\n");
+ 				return 0;
+ 			}
+ 			page_list[i] = virt_to_machine(page)>>PAGE_SHIFT;
+ 		}
+ 
+ 		rop.cmd = REPLAY_REC_CMD;  //4
+ 		rop.u.recop.cmd = REC_INIT;  //0
+ 		rop.u.recop.dom = targ;
+ 		rop.u.recop.u.rec_init.port=event_port;  //0
+ 		rop.u.recop.u.rec_init.ring_mfn = (virt_to_machine(event_ring)>>PAGE_SHIFT);
+ 		rop.u.recop.u.rec_init.addr = virt_to_machine(page_list)>>PAGE_SHIFT;
+ 		rop.u.recop.u.rec_init.len = NUM_PAGES;
+ 		rop.u.recop.u.rec_init.clean_algorithm = QUEUE_CLEAN_NEVER;
+ 		printk("replay_op in record_module: %u, %u, %u, %u, %lu, %lu, %lu, %u\n",rop.cmd, 			rop.u.recop.cmd, rop.u.recop.dom, rop.u.recop.u.rec_init.port, 				rop.u.recop.u.rec_init.ring_mfn, rop.u.recop.u.rec_init.addr, rop.u.recop.u.rec_init.len, 
+ 			rop.u.recop.u.rec_init.clean_algorithm);
+ 
+ 		printk("starting monitor\n");
+ 	    
+                 // event_port=0, changed by hypercall
+ 		asm("int $0x82"
+ 				:"=a" (error_code)
+ 				:"0" (__HYPERVISOR_replay_op),
+ 				"b" (&rop) );
+ 
+ 		printk("Returned error code %ld\n", error_code);
+ 
+ 		kfree(page_list);
+ 		event_port = rop.u.recop.u.rec_init.port;
+ 		printk("setting event port to :%d\n", event_port);	
+ 	}
+ 
+ 	if(event_port < 0){
+ 		printk("No event to listen\n");
+ 		free_page((unsigned long)event_ring);
+ 		return event_port;
+ 	}
+ 
+ 	printk("Binding event port to irq\n");
+ 
+         //irq = my_call(event_port);
+ 	//irq = bind_caller_port_to_irq(event_port);//xen-3.3.0
+ 	//irq = bind_evtchn_to_irq(event_port);   //rivert
+ 	//printk("Bound event port to irq %d\n", irq);
+ 
+         irq = 1;  //FIXME delete it
+ 	ret = request_irq(irq, record_irq_handler, SA_INTERRUPT,
+ 					"record event", "record event");
+ 	printk("request_irq returned %d\n", ret);
+ 
+     return 0;
+ }
+ 
+ static int record_release(struct inode *inode, struct file *file)
+ {
+     printk("record release\n");
+     used = 0;
+     module_put(THIS_MODULE);
+ 
+     return 0;
+ }
+ 
+ static int record_ioctl(struct inode *inode, struct file *file,
+ 				unsigned command, unsigned long data)
+ {
+     int ret;
+ 
+     switch(command){
+         case RECORD_IOCTL_PLUG:
+ 				printk("module ioctl\n");
+         case RECORD_IOCTL_UNPLUG:
+ 	    {
+ 		     struct record_ioctl_plug b;
+ 			 printk("unplugi ioctl\n");
+ 		     if(copy_from_user(&b, (const struct record_ioctl_plug *)data,
+ 									 sizeof(b)))
+ 			     return -EFAULT; 
+ 			 printk("dom id:%d\n", b.dom);
+ 
+ 			 if(RECORD_IOCTL_PLUG == command){
+ 				 return do_plug_netif(b.dom, b.handle);
+ 			 }
+ 			 else{
+ 				 int i = do_unplug_netif(b.dom, b.handle);
+ 			     printk("do unplug netif:%d\n", i);
+ 			     //return do_unplug_netif(b.dom, b.handle);
+ 				 return i;
+ 			 }
+ 	    }
+         case RECORD_IOCTL_ATTACH:
+ 		{
+ 		    struct record_ioctl_attach attach;
+ 			if(copy_from_user(&attach, (const struct record_ioctl_attach *)data,
+ 									sizeof(attach)))
+ 				return -EFAULT;
+ 			printk("copy_from_user,port:%d\n",attach.port);
+              
+ 			ret = do_record_netif_attach(attach.port, &attach.rx_ring_mfn,
+ 							&attach.tx_ring_mfn, &attach.handle);
+ 
+ 			printk("do record netif attach,ret:%d\n",ret);
+ 			if(0 > ret)
+ 				return ret;
+ 			
+ 			if(copy_to_user((struct record_ioctl_attach *)data,
+ 					(const struct record_ioctl_attach *)&attach,
+ 					sizeof(attach)))
+ 				return -EFAULT;
+ 			return 0;
+ 
+ 		}
+         default:
+ 		    return -EINVAL;
+ 	}
+ }
+ 
+ static struct file_operations ops={
+         .owner = THIS_MODULE,
+ 	.open = record_open,
+ 	.read = record_read,
+ 	.write = record_write,
+ 	.release = record_release,
+ 	.ioctl = record_ioctl
+ };
+ #define __HYPERVISOR_my_hyper_op 38
+ static int my_module_init(void)
+ {
+     int ret; 
+     int number=10;
+ 
+     Major = register_chrdev(0, DEVICE_NAME, &ops);
+     if(Major < 0)
+         printk("register error\n");
+     printk("Major number:%d\n", Major);
+ 
+     netif_interface_init();
+     //netif_init();
+ 
+     //hypercall invoke sample
+     asm("int $0x82"
+ 	:"=a" (ret)
+ 	:"0" (__HYPERVISOR_my_hyper_op),
+ 	"b" (number) );
+     printk("hypercall return:%d\n", ret);
+ 
+     return 0;
+ }
+ 
+ void my_module_exit(void)
+ {
+     unregister_chrdev(Major, DEVICE_NAME);
+     printk("my module exit\n");
+ }
+ 
+ 
+ module_init(my_module_init);
+ module_exit(my_module_exit);
+ 
+ MODULE_LICENSE("GPL");
Binary files xen-3.3.0/tools/record_module/.record_module.c.swo and new/xen-3.3.0/tools/record_module/.record_module.c.swo differ
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/tools/record_module/record_module.mod.c new/xen-3.3.0/tools/record_module/record_module.mod.c
*** xen-3.3.0/tools/record_module/record_module.mod.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/tools/record_module/record_module.mod.c	2009-06-12 17:11:09.000000000 -0400
***************
*** 0 ****
--- 1,52 ----
+ #include <linux/module.h>
+ #include <linux/vermagic.h>
+ #include <linux/compiler.h>
+ 
+ MODULE_INFO(vermagic, VERMAGIC_STRING);
+ 
+ struct module __this_module
+ __attribute__((section(".gnu.linkonce.this_module"))) = {
+  .name = KBUILD_MODNAME,
+  .init = init_module,
+ #ifdef CONFIG_MODULE_UNLOAD
+  .exit = cleanup_module,
+ #endif
+ };
+ 
+ static const struct modversion_info ____versions[]
+ __attribute_used__
+ __attribute__((section("__versions"))) = {
+ 	{ 0xf5c51545, "struct_module" },
+ 	{ 0x26e96637, "request_irq" },
+ 	{ 0x37a0cba, "kfree" },
+ 	{ 0x1a5ef3d2, "phys_to_machine_mapping" },
+ 	{ 0x1139ffc, "max_mapnr" },
+ 	{ 0x55526907, "xen_features" },
+ 	{ 0x19070091, "kmem_cache_alloc" },
+ 	{ 0xab978df6, "malloc_sizes" },
+ 	{ 0x2c2512ea, "get_zeroed_page" },
+ 	{ 0xf2a644fb, "copy_from_user" },
+ 	{ 0xabc28976, "force_evtchn_callback" },
+ 	{ 0xd49d4ee9, "HYPERVISOR_shared_info" },
+ 	{ 0xc192d491, "unregister_chrdev" },
+ 	{ 0x3adc4c7c, "remove_wait_queue" },
+ 	{ 0x4292364c, "schedule" },
+ 	{ 0xa3d44f8c, "add_wait_queue" },
+ 	{ 0x2da418b5, "copy_to_user" },
+ 	{ 0xffd5a395, "default_wake_function" },
+ 	{ 0x59968f3c, "__wake_up" },
+ 	{ 0x19cacd0, "init_waitqueue_head" },
+ 	{ 0xae873601, "wake_up_process" },
+ 	{ 0x280f9f14, "__per_cpu_offset" },
+ 	{ 0xb994c584, "per_cpu__irq_stat" },
+ 	{ 0x1b7d4074, "printk" },
+ 	{ 0xaf724678, "register_chrdev" },
+ };
+ 
+ static const char __module_depends[]
+ __attribute_used__
+ __attribute__((section(".modinfo"))) =
+ "depends=";
+ 
+ 
+ MODULE_INFO(srcversion, "84D13EA4F3CE59C1D2242BC");
Binary files xen-3.3.0/tools/record_module/.record_sample.c.swp and new/xen-3.3.0/tools/record_module/.record_sample.c.swp differ
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/arch/x86/bts.c new/xen-3.3.0/xen/arch/x86/bts.c
*** xen-3.3.0/xen/arch/x86/bts.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/arch/x86/bts.c	2009-06-10 20:24:32.000000000 -0400
***************
*** 0 ****
--- 1,225 ----
+ #include <xen/sched.h>
+ #include <xen/bts.h>
+ #include <xen/config.h>
+ #include <xen/softirq.h>
+ #include <asm/irq.h>
+ #include <asm/msr.h>
+ //#include <asm/debug.h>
+ 
+ /* FIXME -- move to perfctr.c */
+ void setup_perfctr_interrupt(void);
+ 
+ //BUILD_SMP_INTERRUPT(perfctr_interrupt, PERFCTR_VECTOR)
+ 
+ void bts_poke(struct vcpu *v)
+ {
+     static int max_index = DEFAULT_THRESH;
+     extern void bts_record(struct vcpu *v,
+ 			   unsigned long from, unsigned long to);
+     struct bts_record *a, *b;
+     struct ds_area *ds;
+     int mode_record=1;
+ 
+     if (!test_bit(_VCPUF_bts, &v->vcpu_flags))
+ 	return; 
+     ds=v->bts.ds;
+     BUG_ON(!ds);
+     mb();
+     if(ds->bts_index == ds->bts_max) {
+ 	printk("bts_poke: WARNING buffer is full, d %d v %d\n",
+ 			v->domain->domain_id, v->vcpu_id);
+         printk(" last-1: <%lx,%lx>\n last  : <%lx,%lx>\n",
+                 (ds->bts_index-1)->from,
+                 (ds->bts_index-1)->to,
+                 (ds->bts_index)->from,
+                 (ds->bts_index)->to);
+     }
+     for(a = b = ds->bts_base; b < ds->bts_index; b++) {
+         if(mode_record) {
+           /*if((test_bit(_VCPUF_monitored, &v->vcpu_flags) 
+               && !domain_runnable(v))
+              || (test_bit(_VCPUF_replaying, &v->vcpu_flags)
+                  && !bts_history_free(v)))*/ {
+            mode_record=0;
+            printk("%s: d %d v %d not runnable, switching to copy mode.\n",
+                         __func__, v->domain->domain_id, v->vcpu_id);
+           }
+         }
+ 	if (b->from < PAGE_OFFSET && b->to < PAGE_OFFSET) {
+             if(mode_record)
+                 bts_record(v, b->from, b->to);
+             else  {
+                 *a=*b;
+                 a++;
+             }
+ 	}
+     }
+     if(ds->bts_index - ds->bts_base > max_index) {
+ 	max_index = ds->bts_index - ds->bts_base;
+ 	printk("bts_poke: new max %d\n", max_index);
+     }
+ 
+     /* FIXME -- we can simplify the logic by always assigning to a; because
+      * a is set to bts_base, and if nothing is copied... but this is more clear
+      * for now. */
+     if(mode_record)
+         ds->bts_index=ds->bts_base;
+     else
+         ds->bts_index=a;
+ 
+     mb();
+     printk("  %s: done\n", __func__);
+ }
+ #if 0
+ void bts_insert(struct vcpu *v, unsigned long from, unsigned long to) {
+     struct ds_area *ds;
+     if(!test_bit(_VCPUF_bts, &v->vcpu_flags))
+  	return;
+     mb();
+     ds=v->bts.ds;
+     BUG_ON(!ds);
+     ds->bts_index->from=from;
+     ds->bts_index->to=to;
+     ds->bts_index++;
+     mb();
+ }
+ 
+ 
+ asmlinkage void smp_perfctr_interrupt(void)
+ {
+     int l;
+     extern void pcounter_breakpoint_hit(void);
+     /* BTS should already have been disabled at this point; it will be
+        reenabled when we get back to domain space */
+     ack_APIC_irq();
+ 
+     if (test_bit(_VCPUF_bts, &current->vcpu_flags))
+ 	raise_softirq(BTS_SOFTIRQ);
+ 
+     pcounter_breakpoint_hit();
+ 
+     l=apic_read(APIC_LVTPC);
+     l &= ~(APIC_LVT_MASKED);
+     apic_write_around(APIC_LVTPC, l);
+ }
+ #endif
+ static void bts_softirq_handler(void)
+ {
+     struct vcpu *v=current;
+     if(test_bit(_VCPUF_bts, &v->vcpu_flags))
+ 	bts_poke(v);
+ }
+ 
+ static void setup_softirq(void)
+ {
+     static int done=0;
+     if (done)
+ 	return;
+     done = 1;
+     open_softirq(BTS_SOFTIRQ, bts_softirq_handler);
+ }
+ 
+ 
+ #define IA32_DTES_AREA 0x600
+ 
+ int bts_trace_vcpu(struct vcpu *v)
+ {
+     struct ds_area *ds;
+     struct bts_area *bts;
+ 
+     if(v->bts.enabled) {
+ 	printk("bts_trace_vcpu: already traced!\n");
+ 	return -1;
+     }
+ 
+     printk("%s: tracing d %d v %d\n", 
+                 __func__, v->domain->domain_id, v->vcpu_id);
+ 
+     if(current != v)
+ 	vcpu_pause(v);
+ 
+ 
+     setup_softirq();
+     setup_perfctr_interrupt();
+ 
+     if(!(ds=xmalloc(struct ds_area)))
+ 	BUG();
+     if(!(bts=xmalloc(struct bts_area)))
+ 	BUG();
+     
+     /* We need to ensure that the accessed and dirty bits are set on
+        all of the relevant PTEs.  The easiest way to do this is just
+        to memset the buffers to 0, and this also makes debugging a bit
+        easier. */
+     memset(ds, 0, sizeof(*ds));
+     memset(bts, 0, sizeof(*bts));
+     ds->bts_base = &bts->data[0];
+     ds->bts_index = ds->bts_base;
+     ds->bts_max = &bts->data[NR_TRACE_RECORDS];
+     ds->bts_thresh = ds->bts_base + DEFAULT_THRESH;
+ 
+     v->bts.ds=ds;
+     v->bts.bts=bts;
+     v->bts.enabled=1;
+     set_bit(_VCPUF_bts, &v->vcpu_flags);
+     mb();
+ 
+     if(current != v)
+ 	vcpu_unpause(v);
+     else
+ 	wrmsr(IA32_DTES_AREA, ds, 0);
+ 
+     return 0;
+ }
+ 
+ #define PERFCTR_VECTOR		0xf9
+ void setup_perfctr_interrupt(void)
+ {
+     static cpumask_t done;
+     int pid = smp_processor_id();
+ 
+     if (!cpu_isset(pid, done)) {
+ 	cpu_set(pid, done);
+ 	//set_intr_gate(PERFCTR_VECTOR, perfctr_interrupt);
+ 	apic_write(APIC_LVTPC, PERFCTR_VECTOR);
+     }
+ }
+ 
+ void dump_bts_state(unsigned char key){
+   struct domain *d;
+   struct vcpu *v;
+   struct bts_record *a;
+ 
+   for_each_domain(d) {
+     printk("Domain: %2d\n",d->domain_id);
+     for_each_vcpu(d, v) {
+ 	printk("  vcpu %d\n", v->vcpu_id);
+ 	printk("    ds   : %p\n", v->bts.ds);
+ 	printk("    bts  : %p\n", v->bts.bts);
+ 	if(v->bts.ds) {
+ 	    printk("    ds->bts_base  : %p\n", v->bts.ds->bts_base);
+ 	    printk("    ds->bts_index : %p\n", v->bts.ds->bts_index);
+ 	    for(a=v->bts.ds->bts_base; a<v->bts.ds->bts_max; a++) {
+ 		printk("%c<%8lx,%8lx>\n", 
+ 			(a==v->bts.ds->bts_index)?'*':' ', a->from, a->to);
+ 	    }
+ 	}
+ 	if(v->replay && v->replay->bts) {
+     	    unsigned long j, prod, cons;
+     	    prod=v->replay->bts->prod;
+     	    cons=v->replay->bts->cons;
+     	    printk("v->replay->bts history prod %lx cons %lx\n",prod,cons);
+     	    if(prod > BTS_HISTORY_SIZE)
+     		j = prod - BTS_HISTORY_SIZE;
+     	    else
+     		j = 0;
+     	    for(; j<prod; j++) {
+     		printk("%c<%8lx,%8lx>\n", (j==cons)?'*':' ',
+     			    bts_history_index(v, j).from,
+     			    bts_history_index(v, j).to);
+     	    }
+ 	}
+     }
+   }
+ }
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/arch/x86/crew.c new/xen-3.3.0/xen/arch/x86/crew.c
*** xen-3.3.0/xen/arch/x86/crew.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/arch/x86/crew.c	2009-06-10 15:41:12.000000000 -0400
***************
*** 0 ****
--- 1,33 ----
+ #include <xen/config.h>
+ #include <xen/guest_access.h>
+ #include <xen/hypercall.h>
+ #include <xen/replay.h>
+ #include <xen/replay_queue.h>
+ #include <xen/crew.h>
+ 
+ void 
+ crew_xen_grab_shared_info(struct vcpu *v, int common)
+ {
+ 	//struct domain *d = v->domain;
+     
+ 	if(IS_REPLAYING(v->domain))
+ 		return;
+ 	
+ 	//ASSERT(shadow_lock_is_acquired(d));  //FIXME
+ }
+ 
+ void crew_boot_vcpu(struct vcpu * v) {  //invoked by domain.c replay_init_vcpu
+ /*
+     if(v->vcpu_id >= CREW_LAST_VCPU) {
+         printk("%s: v %d, CREW_LAST_VCPU %d!\n", __func__, v->vcpu_id,
+                CREW_LAST_VCPU);
+         BUG_ON(v->domain != current->domain);
+         domain_crash();
+     }
+     shadow_lock(v->domain);
+     crew_dominfo_t * di=get_crew_dominfo(v->domain);
+     di->actors[vcpu_to_actor(v->vcpu_id)].event_count=1;
+     shadow_unlock(v->domain);
+ */
+ }
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/arch/x86/domain.c new/xen-3.3.0/xen/arch/x86/domain.c
*** xen-3.3.0/xen/arch/x86/domain.c	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/arch/x86/domain.c	2009-06-12 15:17:55.000000000 -0400
***************
*** 51,58 ****
--- 51,65 ----
  #include <xen/iommu.h>
  #ifdef CONFIG_COMPAT
  #include <compat/vcpu.h>
+ #include <asm/perfctr.h> //start_performace_counters
  #endif
  
+ struct percpu_ctxt {
+     struct vcpu *curr_vcpu;
+ } __cacheline_aligned;
+ 
+ static struct percpu_ctxt percpu_ctxt[NR_CPUS];
+ 
  DEFINE_PER_CPU(struct vcpu *, curr_vcpu);
  DEFINE_PER_CPU(u64, efer);
  DEFINE_PER_CPU(unsigned long, cr4);
*************** int switch_compat(struct domain *d)
*** 277,284 ****
--- 284,330 ----
  #define release_compat_l4(v) ((void)0)
  #endif
  
+ void replay_boot_vcpu(struct vcpu * vr);
+ void record_boot_vcpu(struct vcpu *v);
+ void start_performance_counters(struct vcpu *);
+ void crew_boot_vcpu(struct vcpu * v);
+ 
+ void replay_init_vcpu(struct vcpu *v)
+ {
+     int i;
+     INIT_LIST_HEAD(&v->event_list);
+ 
+     // By default the event_info structs are turned off.
+     for (i=0;i<MAX_TIPS;i++) {
+             v->exec_info[i]=0;
+     }
+ 
+     atomic_set(&v->activated_events,0);
+ 
+     v->bts.enabled=0;
+ 
+     v->replay=xmalloc(struct replay_vcpu_state);
+     memset((char *)v->replay, 0, sizeof(struct replay_vcpu_state));
+ 
+     set_bit(_REPLAYFLAG_RUNNABLE,&v->replay->replay_flags);
+ 
+     start_performance_counters(v);
+ 
+     if(IS_MONITORED(v->domain)) {
+        record_boot_vcpu(v);
+     } else if(IS_REPLAYING(v->domain)) {
+        replay_boot_vcpu(v);
+     }
+ 
+     /* Can't enable crew until a vcpu is ready to take log records */
+     if(shadow_mode_translate(v->domain)) {
+         crew_boot_vcpu(v);
+     }
+ }
+ 
  int vcpu_initialise(struct vcpu *v)
  {
+     
      struct domain *d = v->domain;
      int rc;
  
*************** int vcpu_initialise(struct vcpu *v)
*** 325,330 ****
--- 371,379 ----
      v->arch.perdomain_ptes =
          d->arch.mm_perdomain_pt + (v->vcpu_id << GDT_LDT_VCPU_SHIFT);
  
+     printk("domain.c/replay vcpu initialize\n");
+     replay_init_vcpu(v);
+ 
      return (is_pv_32on64_vcpu(v) ? setup_compat_l4(v) : 0);
  }
  
*************** int arch_domain_create(struct domain *d,
*** 346,351 ****
--- 395,418 ----
      int i, vcpuid, pdpt_order, paging_initialised = 0;
      int rc = -ENOMEM;
  
+     printk("x86/domain.c/arch_domain_create,replay data init\n");
+ 	
+     //for replay init
+     
+     
+     if(NULL == (d->monitor = xmalloc(struct monitoring_domain_state)))
+ 	printk("****alarm:can not alloc memory for monotor****\n");
+     else
+         d->monitor->init=0;
+ 
+     if(NULL == (d->replay = xmalloc(struct replay_domain_state)))
+ 	printk("****alarm:can not alloc memory for replay****\n");
+     else{
+         //memset(d->replay, 0, sizeof(struct monitoring_domain_state));
+         set_bit(_REPLAYFLAG_RUNNABLE, &d->replay->replay_flags);
+         atomic_set(&d->replay_lock ,0);
+     }
+ 
      d->arch.hvm_domain.hap_enabled =
          is_hvm_domain(d) &&
          hvm_funcs.hap_supported &&
*************** static void __context_switch(void)
*** 1231,1237 ****
      per_cpu(curr_vcpu, cpu) = n;
  }
  
! 
  void context_switch(struct vcpu *prev, struct vcpu *next)
  {
      unsigned int cpu = smp_processor_id();
--- 1298,1304 ----
      per_cpu(curr_vcpu, cpu) = n;
  }
  
! void deliver_logpending(struct vcpu *v);
  void context_switch(struct vcpu *prev, struct vcpu *next)
  {
      unsigned int cpu = smp_processor_id();
*************** void context_switch(struct vcpu *prev, s
*** 1305,1310 ****
--- 1372,1387 ----
  #endif
      }
  
+     printk("context_switch, now goto logging\n");
+     if(test_bit(_VCPUF_monitored, &prev->vcpu_flags)
+        && (prev->replay->replay_flags & REPLAYFLAG_DELAY_SPEND)) {
+           deliver_logpending(prev);
+     }
+ 
+     if(test_bit(_VCPUF_monitored, &next->vcpu_flags)
+        && (next->replay->replay_flags & (REPLAYFLAG_DELAY_SPEND|REPLAYFLAG_DELAY_DOMTIME)))
+          raise_softirq(DELAYED_MONITOR_EVENTS_SOFTIRQ);
+ 
      schedule_tail(next);
      BUG();
  }
*************** void continue_running(struct vcpu *same)
*** 1315,1320 ****
--- 1392,1401 ----
      BUG();
  }
  
+ int __execstate_is_synced(void) {
+     return percpu_ctxt[smp_processor_id()].curr_vcpu == current;
+ }
+ 
  int __sync_lazy_execstate(void)
  {
      unsigned long flags;
*************** void sync_vcpu_execstate(struct vcpu *v)
*** 1344,1349 ****
--- 1425,1443 ----
      flush_tlb_mask(v->vcpu_dirty_cpumask);
  }
  
+ void sync_lazy_execstate_cpu(unsigned int cpu)
+ {
+     if ( cpu == smp_processor_id() )
+     {
+         (void)__sync_lazy_execstate();
+     }
+     else
+     {
+         /* Other cpus call __sync_lazy_execstate from flush ipi handler. */
+         flush_tlb_mask(cpumask_of_cpu(cpu));
+     }
+ }
+ 
  struct migrate_info {
      long (*func)(void *data);
      void *data;
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/arch/x86/Makefile new/xen-3.3.0/xen/arch/x86/Makefile
*** xen-3.3.0/xen/arch/x86/Makefile	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/arch/x86/Makefile	2009-06-10 17:03:40.000000000 -0400
*************** obj-y += crash.o
*** 52,57 ****
--- 52,64 ----
  obj-y += tboot.o
  obj-y += hpet.o
  
+ obj-y += bts.o
+ obj-y += timer.o
+ obj-y += replay_queue.o
+ obj-y += perfctr.o
+ obj-y += crew.o
+ obj-y += record.o
+ obj-y += replay.o
  obj-$(crash_debug) += gdbstub.o
  
  x86_emulate.o: x86_emulate/x86_emulate.c x86_emulate/x86_emulate.h
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/arch/x86/perfctr.c new/xen-3.3.0/xen/arch/x86/perfctr.c
*** xen-3.3.0/xen/arch/x86/perfctr.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/arch/x86/perfctr.c	2009-06-11 10:04:21.000000000 -0400
***************
*** 0 ****
--- 1,283 ----
+ #include <asm/perfctr.h>
+ #include <xen/sched.h>
+ #include <xen/perfc.h>
+ #include <asm/msr.h>
+ 
+ /* Because this modifies sched_start, it assumes that the performance
+  * counter has been sync'd already. */
+ spinlock_t update_count_lock = SPIN_LOCK_UNLOCKED;
+ 
+ #define PROC_P4
+ 
+ static inline void wrmsrll(unsigned long long v, unsigned k)
+ {
+     unsigned x1, x2;
+     x1 = v & 0xffffffff;
+     x2 = v >> 32;
+     wrmsr(k, x1, x2);
+ }
+ 
+ #ifdef PROC_ATHLON   /* 1 */
+ #error "Bitrot!"
+ #elif defined(PROC_P4)
+ 
+ #define MSR_TBPU_ESCR0 0x3c2
+ #define MSR_TBPU_ESCR1 0x3c3
+ #define MSR_MS_CCCR0 0x364
+ #define MSR_MS_CCCR2 0x366
+ #define MSR_MS_COUNTER0 0x304
+ #define MSR_MS_COUNTER2 0x306
+ 
+ #define COUNTER_NR 4
+ 
+ #define ESCR_OS_T0 (1<<3)
+ #define ESCR_OS_T1 (1<<1)
+ #define ESCR_USR_T0 (1<<2)
+ #define ESCR_USR_T1 (1<<0)
+ #define ESCR_EVSEL(x) (((x)&((1<<6)-1))<<25)
+ #define ESCR_EVMASK(x) (((x)&((1<<16)-1))<<9)
+ 
+ #define CCCR_EN (1<<12)
+ #define CCCR_RESERVED (3<<16)
+ #define CCCR_OVF_PMI_T0 (1<<26)
+ #define CCCR_OVF_PMI_T1 (1<<27)
+ #define CCCR_OVF (1<<31)
+ #define CCCR_ESCR_SEL(x) (((x)&7)<<13)
+ #define CCCR_CMP (1<<18)
+ #define CCCR_EDGE (1<<24)
+ #define CCCR_THRESHOLD(x) (((x)&0xf)<<20)
+ 
+ #ifdef REPLAY_HT_ENABLE
+ 
+ static const unsigned long ESCR_HT[2]     = {MSR_TBPU_ESCR0,
+                                              MSR_TBPU_ESCR1};
+ static const unsigned long ESCR_HT_VAL[2] = {ESCR_USR_T0 | ESCR_EVSEL(4) | ESCR_EVMASK(15),
+                                              ESCR_USR_T1 | ESCR_EVSEL(4) | ESCR_EVMASK(15)};
+ static const unsigned long CCCR_HT[2]     = {MSR_MS_CCCR0,
+                                              MSR_MS_CCCR2};
+ static const unsigned long CCCR_HT_VAL[2] = {CCCR_OVF_PMI_T0 | CCCR_EN | CCCR_RESERVED | CCCR_ESCR_SEL(2),
+                                              CCCR_OVF_PMI_T1 | CCCR_EN | CCCR_RESERVED | CCCR_ESCR_SEL(2)};
+ static const unsigned long COUNTER_HT[2]  = {MSR_MS_COUNTER0,
+                                              MSR_MS_COUNTER2};
+ 
+ #else
+ #define ESCR MSR_TBPU_ESCR0
+ #define CCCR MSR_MS_CCCR0
+ #define COUNTER MSR_MS_COUNTER0
+ #endif
+ 
+ #endif /* 1 */
+ 
+ static inline long long rdperfll(unsigned nr)
+ {
+     long long res;
+     unsigned long low, high;
+     rdmsr(nr,low,high);
+     /* Extend P4 counter from 40 bits to 64 bits */
+     if(high & 0x80)
+ 	high |= 0xFFFFFF00L;
+     res = high;
+     res <<= 32;
+     res |= low;
+     return res;
+ }
+ 
+ static inline void set_overflow_count(struct vcpu * v) {
+ 	long long remaining, t;
+ 
+         remaining = v->perfctrs.break_val - v->perfctrs.consumed;
+ 	v->perfctrs.extra = 0;
+ 
+         if (remaining <= PCOUNTER_BREAK_THRESH) {
+             /* We're near enough to the breakpoint that we need to
+                fire now. */
+             printk("set_overflow_count: executing callback\n");
+ 	    v->perfctrs.break_enabled=0;
+             v->perfctrs.callback(v, v->perfctrs.cb_data);
+         } else {
+ 	    int flags;
+ 	    /* Program the counters to fire an interrupt in (remaining
+ 	     * - PCOUNTER_BREAK_THRESH) events */
+ 	    perfc_incr(pcounter_break_reprogram);
+ 	    t = -(remaining - PCOUNTER_BREAK_THRESH);
+ 
+ 	    spin_lock_irqsave(&update_count_lock, flags);
+ #ifndef REPLAY_HT_ENABLE
+ 	    wrmsrll(t, COUNTER);
+ #else
+             wrmsrll(t, COUNTER_HT[smp_processor_id()%smp_num_siblings]);
+ #endif
+ 	    v->perfctrs.sched_start = t;
+ 	    /* This is called from perfctr_schedule() before current = to;
+ 	     * If we get an interrupt after the perfctr changes but before
+ 	     * current changes, and update_counters() is called, we need
+ 	     * sched_start to match. */
+ 	    if(v!=current)
+ 		current->perfctrs.sched_start=t;
+ 	    spin_unlock_irqrestore(&update_count_lock, flags);
+ 	}
+ }
+ 
+ static inline long long read_perf_counter(void)
+ {
+     mb();
+     perfc_incr(pcounter_reads);
+ #ifndef REPLAY_HT_ENABLE
+     return rdperfll(COUNTER);
+ #else
+     return rdperfll(COUNTER_HT[smp_processor_id()%smp_num_siblings]);
+ #endif
+ }
+ 
+ static inline void update_count(struct vcpu * from, struct vcpu * to) {
+     long long pct, pct_start;
+     int flags;
+ 
+     //static long long last_pct[10]={0,0,0,0,0,0,0,0,0};
+ 
+     ASSERT(smp_processor_id() == from->processor);
+     if(to)
+ 	ASSERT(smp_processor_id() == to->processor);
+ 
+     spin_lock_irqsave(&update_count_lock, flags);
+     pct = read_perf_counter();
+     
+     /*if ((last_pct[smp_processor_id()] & 0xf0000000) != (pct & 0xf0000000)) {
+         printk("Performance Counter %d: %llx\n",smp_processor_id(),pct);
+         last_pct[smp_processor_id()]=pct;
+         }*/
+ 
+     if(pct > 0x1000000000LL) {
+       printk("performance counter for processor %d at %llx, setting back to 1...\n",
+                                 smp_processor_id(), pct);
+       pct_start = 1;
+ #ifndef REPLAY_HT_ENABLE
+       wrmsrll(pct_start, COUNTER);
+ #else
+         wrmsrll(pct_start, COUNTER_HT[smp_processor_id()%smp_num_siblings]);
+ #endif
+     } else {
+       pct_start = pct;
+     }
+ 
+     if(!is_idle_task(from->domain)) {
+ 	if(pct < from->perfctrs.sched_start) {
+ 	    printk("update_count: pct %llx < from->perfctrs.sched_start %llx!\n",
+ 	    		pct, from->perfctrs.sched_start);
+ 	}
+ 	BUG_ON(pct < from->perfctrs.sched_start);
+ 	from->perfctrs.consumed += pct - from->perfctrs.sched_start;
+ 	from->perfctrs.sched_start = pct_start;
+     }
+     if(to && !is_idle_task(to->domain))
+ 	to->perfctrs.sched_start = pct_start;
+     spin_unlock_irqrestore(&update_count_lock, flags);
+ }
+ 
+ #define sync_count(_v) update_count(_v,NULL);
+ 
+ #define EXTRA_THRESH 32
+ void perfctr_extra(struct vcpu * v) {
+     static unsigned long long total=0;
+     /* This can be called from evtchn_set_pending now... but it should be
+      * de-scheduled.  Eventually we might be able to move it back to the
+      * stricter version. */
+     assert_vcpu_not_running(v);
+     //ASSERT(v==current);
+ 
+     if(v->perfctrs.counter_enabled) {
+ 	total++;
+ 	v->perfctrs.consumed++;
+ 	v->perfctrs.extra++;
+ 
+ 	/*DPRINTF("perfctr_extra: incrementing count (extra %llx, total %llx)\n",
+           v->perfctrs.extra, total);*/
+ 
+ 	if(v->perfctrs.break_enabled && v->perfctrs.extra > EXTRA_THRESH) {
+     	    printk("perfctr_extra: %llx extra counts (total %llx), resetting perfctr\n",
+     		    v->perfctrs.extra, total);
+ 	    sync_count(v);
+     	    set_overflow_count(v);
+        	}
+     }
+ }
+ 
+ void setup_performance_counters_cpu(void) {
+     static cpumask_t counter_programmed=CPU_MASK_NONE;
+     int pid = smp_processor_id();
+     unsigned int lo,hi;
+ 
+     if(!cpu_isset(pid, counter_programmed)) {
+         printk("Setup performance counter for cpu %d.\n",pid);
+ 	cpu_set(pid, counter_programmed);
+         /* Counter number -> 4, 5
+            ESCR event select -> 4
+            ESCR event mask -> f
+            
+            CCCR select -> 2 */
+ #ifndef REPLAY_HT_ENABLE
+         if (smp_num_siblings>1)
+             printk("\n\nWARNING: It appears you have HT enabled, but have not enabled HT replay!\n\n\n");
+ 
+         /* Set the ESCR */
+         wrmsrll(0x08001e05, ESCR);
+         /* Set the CCCR */
+         wrmsrll(0x04035000, CCCR);
+ #else
+         printk("\n\nWARNING: HT replay code is enabled.\n\n");
+ 
+         /* Set the ESCR */
+         wrmsrll(ESCR_HT_VAL[smp_processor_id()%smp_num_siblings], ESCR_HT[smp_processor_id()%smp_num_siblings]);
+         /* Set the CCCR */
+         wrmsrll(CCCR_HT_VAL[smp_processor_id()%smp_num_siblings], CCCR_HT[smp_processor_id()%smp_num_siblings]);
+ #endif
+         printk("...ESCR and CCCR setup\n");
+ 	/* 
+ 	 * Not perfctr, but important: make sure FOP code is the same
+ 	 * See ia32-1:8.1.8.1 "FOPCODE Compatability Mode"
+ 	 * and ia32-3:B-7, IA32_MISC_ENABLE
+ 	 */
+         rdmsr(MSR_IA32_MISC_ENABLE, lo, hi);
+         lo |= MSR_IA32_MISC_ENABLE_FOP_COMPAT;
+         wrmsr(MSR_IA32_MISC_ENABLE, lo, hi);
+         printk("...FOPCODE compatibility set\n");
+     }
+ }
+ 
+ void start_performance_counters(struct vcpu *v)
+ {
+     long long cur_pctr_val;
+ 
+     if (v->perfctrs.counter_enabled)
+         printk("Restarting performance counters in domain %d.\n", v->domain->domain_id);
+     else 
+         printk("Starting performance counters in domain %d.\n", v->domain->domain_id);
+     v->perfctrs.counter_enabled = 1;
+     v->perfctrs.break_enabled=0;
+ 
+     if(v == current) {
+ 	setup_performance_counters_cpu();  //FIXME seems not run
+ 	cur_pctr_val = read_perf_counter();
+     } else {
+ 	/* The perfctr val should be read in on the next schedule.  Put in
+ 	 * an extremely unlikely candidate, to make sure this assumption
+ 	 * is true. :-) */
+ 	ASSERT(!test_bit(_VCPUF_running, &v->vcpu_flags));
+ 	cur_pctr_val=0xdeadbeef;
+     }
+ 
+     v->perfctrs.consumed = PERFCTR_INIT;
+     v->perfctrs.sched_start = cur_pctr_val;
+ }
+ 
+ unsigned long long get_pcounter_val(struct vcpu *v)
+ {
+     m_start_clock(PCTR_PCOUNTER_VAL);
+     if (v == current) {
+ 	sync_count(v);
+     } else {
+ 	assert_vcpu_not_running(v);
+     }
+     m_stop_clock(PCTR_PCOUNTER_VAL);
+     return v->perfctrs.consumed;
+ }
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/arch/x86/record.c new/xen-3.3.0/xen/arch/x86/record.c
*** xen-3.3.0/xen/arch/x86/record.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/arch/x86/record.c	2009-06-12 16:44:31.000000000 -0400
***************
*** 0 ****
--- 1,819 ----
+ #include <xen/config.h>
+ #include <xen/guest_access.h>
+ #include <xen/hypercall.h>
+ #include <xen/replay.h>
+ #include <xen/replay_queue.h>
+ #include <asm/shadow.h>
+ #include <asm/event.h>
+ #include <xen/crew.h>
+ #include <asm/perfctr.h>
+ #include <asm/bitops.h>
+ #include <xen/domain_page.h>
+ #include <xen/cpumask.h>
+ #include <xen/softirq.h>
+ #include <xen/sched.h>
+ #include <xen/domain.h>
+ #include <xen/bts.h>
+ #include <xen/event.h>
+ #include <asm/spinlock.h>
+ #include <xen/timer.h>  //int user_acitve = 0;
+ //#include <asm/guest_access.h>
+ 
+ #define shadow_lock(_d)                         \
+ do {                                            \
+ s_start_clock(SHADOW_LOCK);             \
+ LOCK_BIGLOCK(_d);                           \
+ s_stop_clock(SHADOW_LOCK);             \
+ (_d)->arch.shadow_nest++;                   \
+ } while (0)
+ #define shadow_unlock(_d)                       \
+ do {                                            \
+ ASSERT(shadow_lock_is_acquired(_d));        \
+ (_d)->arch.shadow_nest--;                   \
+ UNLOCK_BIGLOCK(_d);                         \
+ } while (0)
+ 
+ #define PREINIT_EVENTS 5
+ 
+ //extern int user_acitve;
+ void post_bts(struct vcpu * v, unsigned long from, unsigned long to);
+ 
+ static void
+ __post_event(struct record_event *evt, int do_flush, struct vcpu *v);
+ 
+ /* ============================ */
+ //hypercall sample
+ int do_my_hyper_op(int number)
+ {
+     printk("my hyper_op\n");
+ 	return (number+10);
+ }
+ /* ============================ */
+ 
+ static struct record_event preinit_events[PREINIT_EVENTS];
+ static struct vcpu * preinit_event_vcpu[PREINIT_EVENTS];
+ static int preinit_event_count = 0;
+ 
+ int _event_delivery_pending[NR_CPUS] = { 0 };
+ struct record_event _event_delivery_evt[NR_CPUS];
+ #define event_delivery_pending (_event_delivery_pending[smp_processor_id()])
+ #define event_delivery_evt (_event_delivery_evt[smp_processor_id()])
+ 
+ static int nr_posts_per_pause=0;
+ static long long int total=0;
+ void inc_num_pauses(unsigned char key){
+   nr_posts_per_pause++;
+   printk("New number of posts per pause : %6d\ttotal=%10lld\n",nr_posts_per_pause,total);
+ }
+ void dec_num_pauses(unsigned char key){
+   nr_posts_per_pause--;
+   printk("New number of posts per pause : %6d\ttotal=%10lld\n",nr_posts_per_pause,total);
+ }
+ void no_pauses(unsigned char key) {
+   nr_posts_per_pause=0;
+   printk("New number of posts per pause : %6d\ttotal=%10lld\n",nr_posts_per_pause,total);
+ }
+ void reset_num_pauses(unsigned char key) {
+   nr_posts_per_pause=1;
+   printk("New number of posts per pause : %6d\ttotal=%10lld\n",nr_posts_per_pause,total);
+ }
+ void double_num_pauses(unsigned char key) {
+   nr_posts_per_pause*=2;
+   printk("New number of posts per pause : %6d\ttotal=%10lld\n",nr_posts_per_pause,total);
+ }
+ void half_num_pauses(unsigned char key) {
+   nr_posts_per_pause/=2;
+   printk("New number of posts per pause : %6d\ttotal=%10lld\n",nr_posts_per_pause,total);
+ }
+ 
+ static inline void replay_prepost_lock(struct vcpu *v) {
+     rq_lock(v->replay->rq);
+ }
+ 
+ static inline void replay_prepost_unlock(struct vcpu *v) {
+     rq_unlock(v->replay->rq);
+ }
+ 
+ static void record_softirq_handler(void);
+ 
+ /*
+  * What we want to avoid is for v to be scheduled on another processor.
+  * (_VCPUF_running && v!=current) is not a good test for this, because there's
+  * a short period of time in the scheduler where _VCPUF_running is set, but
+  * it hasn't done a context switch yet.  So instead, if v!=current, we
+  * assert that either !_VCPUF_running, or that it's running on the current
+  * cpu (in which case, it's about to be scheduled in, but hasn't context
+  * switched just yet).
+  */
+ struct cpu_user_regs *find_domain_pt_regs(struct vcpu *v)
+ {
+     struct cpu_user_regs *ret;
+    
+     if((v == current) && __execstate_is_synced()) {
+         printk("v == current\n");
+ 	ret= guest_cpu_user_regs();
+         if(ret->eip == 0x1d1ed00d) {
+             printk("%s: d%dv%d == current, synced, eip 1d1ed00d!\n",
+                 __func__, v->domain->domain_id, v->vcpu_id);
+         }
+         ASSERT(ret->eip != 0x1d1ed00d);
+     } else {
+         printk("may v != current?\n");
+ 	assert_vcpu_not_running(v);
+ 	sync_lazy_execstate_cpu(v->processor);
+ 	ret= &v->arch.guest_context.user_regs;
+         ASSERT(ret->eip != 0x01dda7a);
+     }
+     /* These should never EVER happen here -- the asserts for their
+      * respective faults should be above... */
+     ASSERT(ret->eip != 0x1d1ed00d);
+     ASSERT(ret->eip != 0x01dda7a);
+ 
+     printk("  %s: done\n", __func__);
+     return ret;
+ }
+ 
+ static void
+ post_event(struct record_event *evt,
+ 	   int do_flush, struct vcpu *v)
+ {
+     if(!v->replay || !v->replay->rq)
+         return;
+ 
+     s_start_clock(POST_EVENT);
+ 
+ #if !TESTING_MINIMAL
+     post_thing(v);
+ #endif
+ 
+     /* FIXME -- make this spin lock a much smaller area; maybe just locking
+      * to grab a new buffer */
+     replay_prepost_lock(v);
+     __post_event(evt, do_flush, v);
+     replay_prepost_unlock(v);
+ 
+     s_stop_clock(POST_EVENT);
+ }
+ 
+ static void __prepare_event(struct record_event *evt, struct cpu_user_regs *regs, long long perfctr)
+ {
+     evt->unpause_validate=0;
+     evt->ignore = 0;
+     evt->user_timer = !!(user_active);
+ 
+     if(event_delivery_pending) {
+ 	evt->when = event_delivery_evt.when;
+     } else {
+         evt->when.eip = regs->eip;
+         evt->when.ecx = regs->ecx;
+         evt->when.perfctr = perfctr;
+     }
+ #ifdef REPLAY_FULLCONTEXT
+     memcpy(&evt->ctx, regs, sizeof(struct cpu_user_regs));
+ #endif
+ }
+ 
+ static void prepare_event(struct record_event *evt, struct vcpu *v)
+ {
+     struct cpu_user_regs *regs;
+     long long perfctr;
+ 
+     int need_unpause=0;
+ 
+     m_start_clock(PREPARE_EVENT);
+ 
+     /* See note near find_domain_pt_regs */
+     if(vcpu_not_current(v)) {
+ 	printk("prepare_event: v!=current, running on processor %d (current %d), pausing vcpu\n", v->processor, smp_processor_id());
+ 	need_unpause=1;
+         m_start_clock(PREPARE_EVENT_PAUSE);
+ 	vcpu_pause(v);
+         m_stop_clock(PREPARE_EVENT_PAUSE);
+     }
+ 
+     /* We can't have interrupts turned off for these two */
+     regs=find_domain_pt_regs(v);
+     perfctr = get_pcounter_val(v);
+ #ifndef NDEBUG
+     if(regs->eip == 0x1dda7a) {
+ 	printk("prepare_event: read stale data!\n");
+ 	printk(" target: domain %d vcpu %d (_VCPUF_running %lu)\n",
+ 		v->domain->domain_id, v->vcpu_id, 
+ 		v->vcpu_flags & _VCPUF_running);
+ 	printk(" current: domain %d vcpu %d\n",
+ 		current->domain->domain_id, current->vcpu_id);
+ 	printk(" guest_cpu_user_regs()->eip %lu\n", 
+ 			(unsigned long)guest_cpu_user_regs()->eip);
+ 	ASSERT(regs->eip != 0x1dda7a);
+     }
+     if(regs->eip == 0x1d1ed00d) {
+ 	printk("prepare_event: read idle domain data!\n");
+ 	printk(" target: domain %d vcpu %d (_VCPUF_running %lu)\n",
+ 		v->domain->domain_id, v->vcpu_id, 
+ 		v->vcpu_flags & _VCPUF_running);
+ 	printk(" current: domain %d vcpu %d\n",
+ 		current->domain->domain_id, current->vcpu_id);
+ 	printk(" guest_cpu_user_regs()->eip %lu\n", 
+ 			(unsigned long)guest_cpu_user_regs()->eip);
+ 	ASSERT(regs->eip != 0x1d1ed00d);
+     }
+ #endif
+     replay_prepost_lock(v);
+     __prepare_event(evt, regs, perfctr);
+     replay_prepost_unlock(v);
+ 
+     if(need_unpause) {
+ 	printk("  unpausing v\n");
+     	vcpu_unpause(v);
+     }
+ 
+     m_stop_clock(PREPARE_EVENT);
+ }
+ 
+ void bts_record(struct vcpu *v, unsigned long from, unsigned long to)
+ {
+     struct record_event evt;
+     
+     printk("bts_record(%d, %lu, %lu)\n",v->vcpu_id,from,to);
+ 
+     if (test_bit(_VCPUF_monitored, &v->vcpu_flags)) {
+ 	evt.type = RECORD_EVT_CHECKPOINT;
+ 	prepare_event(&evt, v);
+ 	evt.u.checkpoint.from = from;
+ 	evt.u.checkpoint.to = to;
+         printk("evt.type:%d, evt from: %lu, event_to: %lu\n", evt.type, evt.u.checkpoint.from, evt.u.checkpoint.to);
+ 	post_event(&evt, 0, v);
+     } else if(test_bit(_VCPUF_replaying, &v->vcpu_flags)){
+ 	post_bts(v, from, to);
+     }
+ }
+ 
+ static void pause_vcpu(struct vcpu *v)
+ {
+     set_bit(_VCPUF_ctrl_pause, &v->vcpu_flags);
+     if (v == current) {
+ 	ASSERT(test_bit(_VCPUF_running, &v->vcpu_flags));
+ 	raise_softirq(SCHEDULE_SOFTIRQ);
+     }
+ }
+ 
+ //queue init and insert  *key queue op*
+ static void
+ __post_event(struct record_event *evt, int do_flush, struct vcpu *v)
+ {
+     int cur_free=0;
+     replay_queue_t rq=NULL;
+ 
+     if (!v) {
+         printk("WARNING: No vcpu!\n");
+     }
+ 
+     if (v && v->replay) {
+         rq=v->replay->rq;
+     } else {
+ 	printk("post_event: init 0! putting in pre-init log\n");
+ 	preinit_events[preinit_event_count] = *evt;
+ 	preinit_event_vcpu[preinit_event_count]=v;
+ 	preinit_event_count++;
+ 	ASSERT(preinit_event_count < PREINIT_EVENTS);
+ 	return; 
+     }
+ 
+     if (!rq) {
+         printk("post_event: request queue not set up yet!\n");
+         return;
+     }
+     
+     cur_free=queue_free(rq);
+     if (cur_free<REPLAY_BUFFER_LEN*2) {
+         printk("Queue is filling...\n");
+         ASSERT(v);
+         if (!test_bit(_VCPUF_ctrl_pause, &v->vcpu_flags)) {
+             if (!IS_REPLAY_TOOFULL(v) && domain_runnable(v)) {
+                 printk("Throttling domain...(event%d)\n", evt->type);
+ 		set_bit(_REPLAYFLAG_TOOFULL, &v->replay->replay_flags);
+ 		raise_softirq(SCHEDULE_SOFTIRQ);
+             } else {
+                 printk("Queue has no free entries!  Can't slow down!\n");
+             }
+         } else {
+             printk("No vcpu or in schedule...\n");
+         }
+     }
+ 
+     if (queue_free(rq)) {
+ #if !TESTING_DISABLE_EXTRAVIRT        
+         if (!in_irq() &&
+ 	    //(evt->type != RECORD_EVT_RDTSC) &&  // might slow down the system too much to boot
+             (evt->type != RECORD_EVT_REGISTER_CHECK) &&
+             (evt->type != RECORD_EVT_CHECKPOINT) &&
+             (evt->type != RECORD_EVT_CREW_EVENT) &&   // Doesn't call replay_queue_get_next
+             (evt->type != RECORD_EVT_CREW_CONSTRAINT) &&  // Doesn't call replay_queue_get_next
+             (evt->type != RECORD_EVT_BTS_STATE) &&
+             (time_cmp(v->replay->last_time,evt->when)!=0) &&
+ 	    test_bit(_REPLAYFLAG_EVENT_STEPPING,&v->replay->replay_flags) &&
+             !test_bit(_REPLAYFLAG_WAITING_FOR_UNPAUSE,&v->replay->replay_flags)) {
+             if (test_bit(_REPLAYFLAG_VALIDATING,&v->replay->replay_flags)) {
+                 evt->unpause_validate=3;
+             } else {
+                 evt->unpause_validate=1;
+             }
+             v->replay->pause_time=evt->when;
+ 	    v->replay->last_time=evt->when;
+             switch_to_replay(v,evt);
+         }
+ #endif
+         if (queue_insert(rq, evt)) {
+ #if !TESTING_DISABLE_EXTRAVIRT
+ 	    printk("Inserted\n");
+ 	    // poke the replay system if it has paused
+ 	    int i;
+ 	    for (i=0;i<v->domain->replay->replay_set->num_replay_domains;i++) {
+ 		struct vcpu *vr=v->domain->replay->replay_set->replaying_domains[i]->vcpu[v->vcpu_id];
+ 		if(!(vr && 
+                      vr->replay && 
+                      test_bit(_REPLAYFLAG_INIT, &vr->replay->replay_flags))) {
+ 		   printk("%s: replaying vcpu %d not active yet\n", 
+ 		   			__func__, v->vcpu_id);
+ 		   continue;
+ 		}
+ 		if (!atomic_read(&vr->activated_events) && 
+ 		    //!test_bit(_VCPUF_running, &vr->vcpu_flags) &&
+ 		    //queue_avail(rq)>3 &&
+                     (!test_bit(_REPLAYFLAG_EVENT_STEPPING, &v->replay->replay_flags) ||
+                      !test_bit(_REPLAYFLAG_WAITING_FOR_UNPAUSE, &vr->replay->replay_flags) ||
+                      evt->unpause_validate
+                      //  test_bit(_REPLAYFLAG_WAITING_FOR_UNPAUSE, &v->replay->replay_flags)
+                      )) {
+ 		    printk("INSERT: Poking replay...\n");
+ 		    vr->replay->needs_poke=1;
+ 		} else {
+                     printk("INSERT: Available: %lu  Activated: %d\n",queue_avail(rq),atomic_read(&vr->activated_events));
+ 		}
+ 	    }
+ #endif
+ 	} else {
+             printk("Failed to insert queue type %d, <%llx,%lx,%lx>\n",
+                 evt->type, evt->when.perfctr, evt->when.eip, evt->when.ecx);
+             BUG();
+         }
+ 	printk("Event inserted in queue...\n");
+     } else {
+         printk("Queue has no free entries!  Overran buffer, pausing.");
+         pause_vcpu(v);
+     }
+ 
+     printk("  %s: done\n", __func__);
+ }
+ 
+ 
+ int post_thing(struct vcpu *v) {
+     static int count=0;
+     count++;
+     total++;
+     if (count%100)
+         printk("post_thing: total %lld, count %d\n", total, count);
+     if((nr_posts_per_pause > 0) && (count >= nr_posts_per_pause)) {
+ 	printk("post_thing: %d since last pause (%lld total), pausing\n",
+ 		count, total);
+ 	count=0;
+ 	pause_vcpu(v);
+     }
+ 
+     printk("  %s: done\n", __func__);
+     return 0;
+ }
+ 
+ static int setup_monitoring(struct vcpu *logging_vcpu,   /* 11 */
+                             struct domain *monitored_dom,
+                             unsigned long ring_mfn) {
+ 
+     struct monitoring_domain_state *m=NULL;
+     int i;
+ 
+     if (monitored_dom)
+         m=monitored_dom->monitor;
+ 
+     m->port = evtchn_alloc_port(logging_vcpu->domain);
+     if (m->port < 0) {
+         printk("Can't allocate monitoring port.\n");
+         return m->port;
+     }
+ 
+     /* FIXME -- make this work right... */
+     if (get_page_and_type(&frame_table[ring_mfn],
+                           current->domain,
+                           PGT_writable_page) == 0) {
+         printk("%s: Bad page type for mfn %lx\n", __func__, ring_mfn);
+         return -EINVAL;
+     }
+ 
+     m->ring = map_domain_page(ring_mfn);
+     if (m->ring == NULL) {
+         /* Oops... */
+         printk("Error mapping %lx.\n", ring_mfn);
+         return -1;
+     }
+     m->cur_buffer = NULL;
+     m->vcpu = logging_vcpu;
+     m->logger_domain = logging_vcpu->domain;
+     m->cur_buffer_mfn = 0;
+ 
+ 
+     printk("Setup monitoring ring, full: %d:%d:%d, empty: %d:%d:%d.\n",
+            m->ring->full_buffer_prod,
+            m->ring->full_buffer_cons,
+            m->ring->full_buffer_clean,
+            m->ring->empty_buffer_prod,
+            m->ring->empty_buffer_cons,
+            m->ring->empty_buffer_clean);
+ 
+     record_softirq_handler();
+ 
+     for(i=0; i<preinit_event_count; i++) {
+         printk("  posting pre-init event %d\n", i);
+         post_event(&preinit_events[i], 0, preinit_event_vcpu[i]);  // !executed
+     }
+     return m->port;
+ }
+ 
+ 
+ /* This must be called "safely" -- either v->processor==smp_processor_id(),
+  * or v->processor already paused. */
+ void report_monitored_evtchn_set_pending(struct vcpu *v, int port, u8 flags) //9
+ {
+ 	struct record_event evt;
+ 	struct cpu_user_regs *regs;
+ 	long long perfctr;
+ 		
+ 	assert_vcpu_not_running(v);
+ 	
+ 	printk("dom: %5d\treport_monitored_evtchn_set_pending(%p,%d)\n",current->domain->domain_id,v,port);
+ 	evt.type = RECORD_EVT_EVTCHN_SET_PENDING;
+ 	evt.u.evtchn_set_pending.port = port;
+ 	evt.u.evtchn_set_pending.flags = flags;
+ 	regs=find_domain_pt_regs(v);
+ 	perfctr = get_pcounter_val(v);
+ 	replay_prepost_lock(v);
+ 	__prepare_event(&evt, regs, perfctr);
+ 	__post_event(&evt, 0, v);
+ 	replay_prepost_unlock(v);
+ 
+ 	printk("  %s: done\n", __func__);
+ }
+ 
+ int __evtchn_set_pending(struct vcpu *v, int port) {  //8
+ 	struct shared_info *s = v->domain->shared_info;
+ 	u8 flags=0;
+ 	int send=0;
+ 	
+     /* "flags" is a record of how far this actually got.  Because it was
+      * initialized to 0, each !test_and_set() should succeed. */
+    /* These three operations must happen in strict order. */
+ 
+ 	if ( !test_and_set_bit(port, &s->evtchn_pending[0]) && \
+ 	     !test_and_set_bit(1,&flags) && \
+ 	     !test_bit(port, &s->evtchn_mask[0])  && \
+ 	     !test_and_set_bit(2,&flags) && \
+ 	     !test_and_set_bit(port>>5, &v->vcpu_info->evtchn_pending_sel) )
+ 	{
+         /* The VCPU pending flag must be set /after/ update to evtchn-pend. */
+ 		set_bit(0, &v->vcpu_info->evtchn_upcall_pending);
+ 		set_bit(3, &flags);
+ 		send=1;
+ 	}
+ 	
+     /* This changes the state, so we need to increment so we can tell if 
+      * a particular crew event happened before or after this point... */
+ 	if(flags) {
+ 		report_monitored_evtchn_set_pending(v, port, flags);
+ 		perfctr_extra(v);
+ 	}
+ 	printk("  %s: done\n", __func__);
+ 
+ 	return send;
+ }
+ 
+ void monitored_evtchn_set_pending(struct vcpu *v, int port) { //9
+ 	int running;
+ 	int replay_unpause=0, send=0;
+ 	
+ 	if(port > 31) {
+ 		printk("%s: refusing to set port %d pending\n", __func__, port);
+ 		return;
+ 	}
+ 	
+ 	if(v==current) {
+ 		printk("%s: v==current, raising softirq\n", __func__);
+ 		v->replay->log_pending |= (1<<port);
+ 		set_bit(_REPLAYFLAG_DELAY_SPEND, &v->replay->replay_flags);
+ 		raise_softirq(DELAYED_MONITOR_EVENTS_SOFTIRQ);
+ 		return;
+ 	} else if(v->processor != smp_processor_id()) {
+ 		vcpu_pause(v);
+ 		replay_unpause=1;
+ 	} 
+ 	
+ 	shadow_lock(v->domain);
+ 	crew_xen_grab_shared_info(v, 1);
+ 	send=__evtchn_set_pending(v, port);
+ 	shadow_unlock(v->domain);
+ 	
+ 	if(send) {
+ 		evtchn_notify(v);
+ 		
+         /*
+          * NB1. 'vcpu_flags' and 'processor' must be checked /after/ update of
+          * pending flag. These values may fluctuate (after all, we hold no
+          * locks) but the key insight is that each change will cause
+          * evtchn_upcall_pending to be polled.
+          * 
+          * NB2. We save VCPUF_running across the unblock to avoid a needless
+          * IPI for domains that we IPI'd to unblock.
+          */
+ 		running = test_bit(_VCPUF_running, &v->vcpu_flags);
+ 		vcpu_unblock(v);
+ 		if ( running )
+ 			smp_send_event_check_cpu(v->processor);
+ 	}
+ 	
+ 	if(replay_unpause)
+ 		vcpu_unpause(v);
+ 	printk("  %s: done\n", __func__);
+ }
+ 
+ void deliver_logpending(struct vcpu *v) //7
+ {
+ 	int port, send=0;
+         printk("deliver_logging, grab and transfer events\n");
+ 	
+ 	ASSERT(v->processor == smp_processor_id());
+ 	
+ 	if(!v->replay->log_pending)
+ 		return;
+ 	
+ 	shadow_lock(v->domain);
+ 
+ 	crew_xen_grab_shared_info(v, 1);  /* 获取并发送事件 */
+ 	
+ 	local_irq_disable();
+ 	for(port=0; port<32; port++) {
+ 		if(!(v->replay->log_pending & (1<<port)))
+ 			continue;
+ 		if(__evtchn_set_pending(v, port))
+ 			send=1;
+ 	} 
+ 	local_irq_enable();
+ 	shadow_unlock(v->domain);
+ 	
+ 	if(send) {
+ 		evtchn_notify(v);  //FIXME event.h, null function, passing asynchronous events to guest OS
+         /* This should only happen on the processor on which v is running,
+          * so we don't need to send any IPIs to do event checks */
+ 		vcpu_unblock(v);
+ 	}
+ 	printk("  %s: done\n", __func__);
+ }
+ 
+ void
+ check_delayed_events_softirq(void)  //6
+ {
+ 	struct vcpu *v=current;
+ 	
+ 	if(!test_bit(_VCPUF_monitored, &v->vcpu_flags))
+ 		return;
+ 	
+ 	BUG_ON(v->replay->replay_flags & REPLAYFLAG_DELAY_SECTION);
+ 	
+ 	//make sure to clear this before logging crew constraints | events
+ 	v->perfctrs.extra = 0;
+ 	
+ 	if(test_bit(_REPLAYFLAG_DELAY_DOMTIME, &v->replay->replay_flags)){
+ 		//_update_dom_time(v);
+ 		update_vcpu_system_time(v);  //replace
+ 		clear_bit(_REPLAYFLAG_DELAY_DOMTIME, &v->replay->replay_flags);
+ 	}
+ 	if(_REPLAYFLAG_DELAY_SPEND, &v->replay->replay_flags){
+ 		clear_bit(_REPLAYFLAG_DELAY_SPEND, &v->replay->replay_flags);
+ 		deliver_logpending(v);
+ 	}
+ 	printk("  %s: done\n", __func__);
+ }
+ 
+ static void record_softirq_handler(void)  //5 importent,begin wirte to replay_queue
+ {
+ 	replay_queue_t rq=NULL;
+ 	struct domain *d;
+ 	struct vcpu   *v;
+ 	struct monitoring_domain_state *m;
+ 	printk("record_softirq_handler");
+ 	
+ 	for_each_domain(d){
+ 		m = d->monitor;
+ 		for_each_vcpu(d,v){
+ 			if(!v->replay)
+ 				continue;
+ 			rq=v->replay->rq;
+ 			
+ 			if(m->init && rq){
+ 				//--importent, begin wirte to replay_queue--
+ 				queue_soft_irq_handler(rq);  //FIXME
+ 			}else if(IS_MONITORED(d)){
+ 				if(!m->init)
+ 					printk("Monitor notinitialized:%d",d->domain_id);
+ 				if(!rq)
+ 					printk("Replay Queue not initialized:%d\n",d->domain_id);
+ 			}
+ 		
+ 		}
+ 	}
+ 	printk("  %s: done\n", __func__);
+ }
+ 
+ void
+ init_softirq(void)  //4
+ {
+ 	static int done;
+ 	printk("  %s: begin\n", __func__);
+         printk(" module write 4\n");
+ 	
+ 	if(done)
+ 		return;
+ 
+ 	done = 1;
+ 	open_softirq(RECORD_SOFTIRQ, record_softirq_handler);  //5
+ 	//DELAYED_MONITOR_EVENTS_SOFTIRQ = 7, in rivert it's 10
+ 	open_softirq(DELAYED_MONITOR_EVENTS_SOFTIRQ, check_delayed_events_softirq);
+ 	printk("  %s: done\n", __func__);
+ }
+ 
+ 
+ void record_boot_vcpu(struct vcpu *v)  //10
+ {
+ 
+     struct cpu_user_regs *regs;
+     struct domain *d=v->domain;
+     int list_len = d->replay->list_len;
+     int pervcpu_len = d->replay->pervcpu_len;
+     int pervcpu_extra = d->replay->pervcpu_extra, l;
+     replay_queue_t h_rq;
+ 
+     printk("%s: d %d v %d\n", __func__, v->domain->domain_id, v->vcpu_id); 
+ 
+ 
+     /* Init the replay queue -- should probably lock... */
+     /* FIXME -- need a lock here. */
+     l = pervcpu_len;
+     if(v->vcpu_id < pervcpu_extra)
+ 	l++;
+ 
+     ASSERT(d->replay->list_start + l <= d->replay->mfn_list + list_len);
+     printk("Initializing queue for logging domain v %d got %d tot %d\n",
+ 		    v->vcpu_id, l, list_len);
+     printk("d->list->replay->list_start: %lu,%d\n", *d->replay->list_start, l);
+ 
+     if (NULL == (h_rq =queue_init(d->replay->list_start, l, v)))  //key func
+         printk("queue init error\n");
+ 
+     v->replay->rq = h_rq;  //v->replay->rq = h_rq;
+     d->replay->list_start +=l;
+ 
+     /* Start other stuff */
+ 
+     start_performance_counters(v);  //FIXME something is wrong?
+     ASSERT(get_pcounter_val(v) == PERFCTR_INIT);
+ 
+     regs=find_domain_pt_regs(v);
+     v->replay->last_time.eip = regs->eip;
+     v->replay->last_time.ecx = regs->ecx;
+     v->replay->last_time.perfctr = get_pcounter_val(v);
+     printk("register value, eip:%lu, ecx:%lu, perfctr:%llu", v->replay->last_time.eip, v->replay->last_time.ecx ,v->replay->last_time.perfctr);
+ 
+     set_bit(_VCPUF_monitored, &v->vcpu_flags);
+     set_bit(_REPLAYFLAG_INIT, &v->replay->replay_flags);
+ 
+     printk("v->cpu_id: %d \n", v->vcpu_id);
+     printk(" %s: enabling bts?\n", __func__);
+     if(v->vcpu_id > 0) {
+         struct vcpu * v0 = v->domain->vcpu[0];
+         if(v0->bts.enabled) {
+                 printk("  yes!\n");
+                 bts_trace_vcpu(v);
+         } else
+                 printk("  no, not enabled on vcpu 0.\n");
+     } else
+         printk("  no, vcpuid 0\n");  //FIXME, should be enabled?
+ 
+     printk("  %s: done\n", __func__);
+     return;
+ }
+ 
+ long record_init(unsigned dom, int *port, unsigned long ring_mfn, unsigned long list_mfn, unsigned list_len)  //3
+ {
+ 	struct domain *rd;
+ 	unsigned long *temp_page;
+ 	unsigned long *mfn_list;
+ 	int i;
+ 	int nr_vcpus=0;
+ 	struct monitoring_domain_state *m = NULL;
+ 	struct vcpu *v;
+ 	
+         printk("  %s: begin\n", __func__);
+         printk(" module write 3\n");
+ 
+ 	rd = get_domain_by_id(dom);
+ 	printk("get domain by id, tot_pages: %d\n", rd->tot_pages);
+ 
+ 	if(NULL == rd)
+ 		return -ESRCH;
+ 	printk("domain info:\n");
+ 	
+ 	if(!(mfn_list = xmalloc_array(unsigned long, list_len))){
+ 		put_domain(rd);
+ 		return -ENOMEM;
+ 	}
+ 	
+ 	//Getting pages for record events
+ 	temp_page = map_domain_page(list_mfn);
+ 	printk("Pages for record: %lu \n", *temp_page);
+ 
+ 	for(i=0; i<list_len; i++){
+ 		if(0 == (i%(PAGE_SIZE/sizeof(unsigned long)))){
+ 			unmap_domain_page(temp_page);
+ 			temp_page = map_domain_page(list_mfn+(i/PAGE_SIZE)/sizeof(unsigned long));
+ 		}
+ 		if(NULL == temp_page){
+ 			put_domain(rd);
+ 			xfree(mfn_list);
+ 			return -ENOMEM;
+ 		}
+ 		
+ 		mfn_list[i]=temp_page[i%(PAGE_SIZE/sizeof(unsigned long))];
+ 		//printk("processing: mfn(%d): %lx\n", i, mfn_list[i]);
+ 	}
+ 	unmap_domain_page(temp_page);
+ 	
+ 	//nr_vcpus=rd->shared_info->n_vcpu;  should be this?
+ 	nr_vcpus = num_online_cpus();  /* FIXME */
+ 	printk("nr_vcpus: %d\n",nr_vcpus);
+ 
+ 	rd->replay->list_len = list_len;
+ 	rd->replay->list_start = rd->replay->mfn_list = mfn_list;
+ 	rd->replay->pervcpu_len = list_len/nr_vcpus;
+ 	rd->replay->pervcpu_extra = list_len%nr_vcpus;
+ 
+ 	/* set up the per-domain monitor state */
+ 	// set the magic bit on the target domain
+ 	m = rd->monitor;
+ 
+ 	init_softirq();  //4,set qoftirq_handler func array, executed by do_softirq
+ 
+ 	spin_lock_init(&m->lock);
+ 
+ 	rd->replay->replay_set = xmalloc(replay_set_t);
+ 	rd->replay->replay_set->logging_domain = rd;
+ 	rd->replay->replay_set->num_replay_domains = 0;
+ 
+ 	set_bit(_DOMF_monitored, &rd->domain_flags);
+ 
+ 	m->init = 1;
+ 
+ 	for_each_vcpu(rd, v){
+             record_boot_vcpu(v);   /* FIXME important */
+ 	}
+ 
+ 	*port = setup_monitoring(current, rd, ring_mfn);    /* FIXME important */
+ 	printk("port to setup_monitoring: %d \n", *port);
+ 
+         /* Release resources belonging to task */
+         put_domain(rd);
+ 
+ 	return *port;
+ }
+ 
+ long do_rec_op(replay_record_op_t *op)  //2
+ {
+     long ret = 0;
+     printk("  %s: begin\n", __func__);
+     printk(" module write 2\n");
+ 	switch (op->cmd){
+ 	    case REC_INIT:
+ 		printk("REC_INIT\n");
+ 		ret = record_init(op->dom,
+ 		                 &op->u.rec_init.port, op->u.rec_init.ring_mfn,
+ 		                  op->u.rec_init.addr, op->u.rec_init.len);
+ 		break;
+ 	    case REC_CLEAN_SWITCH:
+ 		printk("REC_CLEAN_SWITCH\n");
+ 		//ret = record_set_cleaner(op->dom,op->u.rec_clean_switch.clean_algorithm);
+ 		break;
+ 	    default:
+ 		printk("default\n");
+                 ret=-ESRCH;
+ 		break;
+ 	}
+ 
+     return ret;
+ }
+ 
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/arch/x86/replay.c new/xen-3.3.0/xen/arch/x86/replay.c
*** xen-3.3.0/xen/arch/x86/replay.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/arch/x86/replay.c	2009-06-12 14:57:46.000000000 -0400
***************
*** 0 ****
--- 1,182 ----
+ #include <xen/config.h>
+ #include <xen/guest_access.h>
+ #include <xen/hypercall.h>
+ #include <xen/errno.h>
+ #include <xen/sched.h>
+ #include <xen/softirq.h>
+ #include <xen/replay.h>  /* xen-3.3.0/xen/include/xen/ */
+ 
+ void dump_bts_state(unsigned char key);  //bts.c
+ 
+ unsigned int full_debug=0;
+ 
+ void do_poke(void)
+ {
+ 	struct domain *d;
+ 	struct vcpu *v;
+ 	printk("module read: do_poke\n");
+ 
+ 	for_each_domain(d){
+ 	    if(IS_LOGGING(d)){
+ 	        for_each_vcpu(d,v){
+ 	            if(v->replay)
+ 		        v->replay->needs_poke=1;
+ 	        }
+ 	    }
+ 	}
+ 	raise_softirq(RECORD_SOFTIRQ);
+ }
+ 
+ void vcpu_pause_delay(struct vcpu *v)
+ {
+     atomic_inc(&v->pausecnt);
+     if ( v == current )
+     	raise_softirq(SCHEDULE_SOFTIRQ);
+     else
+         vcpu_sleep_sync(v);
+ }
+ 
+ int dump_bts_history(struct vcpu *v, unsigned long cfrom, unsigned long cto)
+ {
+      int j;
+      record_event_t e;
+      printk("  History:\n");
+      if(v->replay->bts->prod > BTS_HISTORY_SIZE)
+        	j=v->replay->bts->prod - BTS_HISTORY_SIZE;
+      else
+        	j=0;
+      /* First, print as much of the correct history as we have */
+      for(; j<v->replay->bts->cons; j++) {
+     	printk(" <%8lx,%8lx> -\n", bts_history_index(v, j).from,
+  	     		bts_history_index(v, j).to);
+      }
+      /* Then print where we are now */
+      if(bts_history_unconsumed(v)) {
+  	printk("*<%8lx,%8lx> <%8lx,%8lx>\n",
+      		cfrom, cto, bts_history_index(v, j).from, 
+  		bts_history_index(v,j).to);
+  	j++;
+      } else {
+  	printk("*<%8lx,%8lx>\n", cfrom, cto);
+      }
+      /* Then print the logged history, with the replayed history */
+      while(1) {
+  	if(!queue_get_next(v->replay->rq, v->replay->cur_checkpoint,
+  		RECORD_EVT_CHECKPOINT, &e)) {
+  		break;
+  	}
+  	if(e.type != RECORD_EVT_CHECKPOINT)
+  	    break;
+  	if(j < v->replay->bts->prod)
+  	    printk(" <%8lx,%8lx> <%8lx,%8lx>\n",
+  			e.u.checkpoint.from, e.u.checkpoint.to,
+  			bts_history_index(v, j).from,
+  			bts_history_index(v, j).to);
+  	else
+  	    printk(" <%8lx,%8lx>\n", e.u.checkpoint.from, e.u.checkpoint.to);
+      }
+      /* And if we have any extra history, print that too */
+     for(; j<v->replay->bts->prod; j++) {
+  	printk("                     <%8lx,%8lx>\n",
+  		bts_history_index(v, j).from,
+  		bts_history_index(v, j).to);
+     }
+     return 0;
+ }
+ 
+ int bts_verify(struct vcpu *v, unsigned long from, unsigned long to)
+ {
+      int ret=1;
+      if(bts_history_unconsumed(v)) {
+  	if(bts_history_cons(v).from != from || bts_history_cons(v).to != to) {
+  	    printk("bts_verify: expecting <%lx,%lx>, got <%lx,%lx>!\n",
+  	   		from, to, bts_history_cons(v).from, bts_history_cons(v).to);
+  	    ret=0;
+  	} else
+ 	    v->replay->bts->cons++;
+      } else {
+  	printk("bts_verify: no bts to consume!\n");
+  	ret=0;
+      }
+      if(!ret) {
+      	dump_bts_history(v, from, to);
+  	dump_bts_state('b');
+         	printk("BTS trace error!  Pausing self...\n");
+  	vcpu_pause_delay(v);
+      }
+      return ret;
+ }
+ 
+ void post_bts(struct vcpu * v, unsigned long from, unsigned long to)
+ {
+      record_event_t e;
+  
+      while(bts_history_unconsumed(v) && queue_get_next(v->replay->rq,
+      		v->replay->cur_checkpoint, RECORD_EVT_CHECKPOINT, &e)) {
+  	bts_verify(v, e.u.checkpoint.from, e.u.checkpoint.to);
+      }
+  
+      if(bts_history_free(v)) {
+  	bts_history_prod(v).from = from;
+  	bts_history_prod(v).to = to;
+  	v->replay->bts->prod++;
+  	if(v->replay->bts->prod > (BTS_HISTORY_SIZE * 100)) {
+  	    v->replay->bts->prod -= (BTS_HISTORY_SIZE * 99);
+  	    v->replay->bts->cons -= (BTS_HISTORY_SIZE * 99);
+  	}
+      } else {
+  	printk("post_bts: no room!\n");
+  	vcpu_pause_delay(v);
+      }
+ }
+ 
+ 
+ void replay_boot_vcpu(struct vcpu * vr)
+ {
+ 
+ }
+ 
+ long do_rec_op(replay_record_op_t *);  //record.c
+ /*
+ long do_replay_op(replay_op_t *uop)
+ {
+     long i=10;
+ 
+     printk("do_record and replay hypercall\n");
+     return i;
+ }
+ */
+ long do_replay_op(replay_op_t *uop)    //1
+ {
+     long ret = 0;
+     replay_op_t op;
+     printk("do_record_replay_op begin\n");
+ 
+     if(copy_from_user(&op, uop, sizeof(op)))
+ 	return -EFAULT;
+ 
+ 	printk("replay op: cmd %u on %u , recop.cmd:%u, port: %u\n",op.cmd, op.u.lockdomain.domain,
+ 	       op.u.recop.cmd, op.u.recop.u.rec_init.port);
+     if(full_debug)
+ 	printk("replay_op:%u on %u", op.cmd, op.u.lockdomain.domain);
+ 
+     switch(op.cmd){
+ 	case REPLAY_LOCKDOMAIN:
+ 	    //ret=replay_lockdomain(op.u.lockdomain.domain);
+ 	    break;
+ 	case REPLAY_REC_CMD:
+             printk("REPLAY_REC_CMD\n");
+ 	    ret = do_rec_op(&op.u.recop);  //2
+ 	    copy_to_user(uop, &op, sizeof(op));
+ 	    break;
+         case REPLAY_POKE:
+ 	    printk("REPLAY_POKE\n");
+ 	    do_poke();
+ 	    break;
+         default:
+             printk("default\n");
+ 	    break;
+     }
+ 
+     return ret;
+ }
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/arch/x86/replay_queue.c new/xen-3.3.0/xen/arch/x86/replay_queue.c
*** xen-3.3.0/xen/arch/x86/replay_queue.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/arch/x86/replay_queue.c	2009-06-11 10:38:31.000000000 -0400
***************
*** 0 ****
--- 1,727 ----
+ #include <xen/replay_queue.h>
+ #include <xen/sched.h>
+ #include <xen/domain_page.h>
+ #include <xen/event.h>
+ #include <xen/timer.h>
+ 
+ const char * clean_algorithm_name[] = {
+     "QUEUE_CLEAN_LOGGED",
+     "QUEUE_CLEAN_REPLAYED",
+     "QUEUE_CLEAN_VERIFIED",
+     "QUEUE_CLEAN_NEVER"
+ };
+ 
+ unsigned long queue_free(replay_queue_t rq){
+     long unsigned int 
+ 	tp = rq->tail->idx,
+ 	cp = rq->cleaned->idx;
+     if (cp <= tp)
+ 	cp+=(rq->num_pages*EVENTS_PER_PAGE);
+     return (cp-tp);
+ }
+ 
+ long int h_abs(long int x)
+ {
+     if(x<0)
+ 	    x = -x;
+     
+     return x;
+ }
+ 
+ int ri_dist(replay_iterator_t l, replay_iterator_t r) {
+ 	ASSERT(l->q==r->q);
+ 	ASSERT(l->version==r->version);
+ 
+ 	return h_abs((long int)l->idx-(long int)r->idx);
+ }
+ 
+ int ri_pagedist(replay_iterator_t l, replay_iterator_t r) {
+ 	ASSERT(l->q==r->q);
+ 	ASSERT(l->version==r->version);
+ 
+ 	return h_abs((long int)l->idx-(long int)r->idx)/EVENTS_PER_PAGE;
+ }
+ 
+ unsigned long ri_mfn(replay_iterator_t it){
+ 	return it->q->page_mfns[ITER_TO_PAGE_IDX(it)];
+ }
+ 
+ int ri_cmp(replay_iterator_t l, replay_iterator_t r){
+     if (l->idx < r->idx) {
+ 	return -1;
+     } else if (l->idx > r->idx) {
+ 	return  1;
+     } else {
+ 	return  0;
+     }
+ }
+ 
+ void ri_new(replay_iterator_t *i, replay_queue_t rq) {
+     DPRINTFMAX("ri_new\n");
+     (*i)=xmalloc(struct replay_iterator);
+     DPRINTFMAX("*i: %p\n", *i);
+     (*i)->idx=0;
+     DPRINTFMAX("*i: %llu\n", (*i)->idx);
+     (*i)->q=rq;
+     DPRINTFMAX("*i: %p\n", (*i)->q);
+     if (rq)
+ 	(*i)->version=rq->version;
+ }
+ 
+ #define ri_next(_it) ((_it)->idx++)
+ #define ri_prev(_it) ((_it)->idx--)
+ 
+ void __rq_lock(replay_queue_t rq, int line) {
+     int flags;
+ #ifndef NDEBUG
+     int proc = smp_processor_id();
+     if(spin_is_locked(&rq->lock)) {
+         if(rq->lock_processor == proc) {
+                 printk("recursive locking, line %d then line %d!\n",
+                         rq->lock_line, line);
+         }
+     	ASSERT(rq->lock_processor != proc);
+     }
+ #endif
+     spin_lock_irqsave(&rq->lock, flags); 
+     rq->lock_flags = flags;
+ #ifndef NDEBUG
+     rq->lock_processor = proc;
+     rq->lock_line = line;
+ #endif
+ }
+ 
+ void rq_unlock(replay_queue_t rq) {
+     int flags;
+ #ifndef NDEBUG
+     ASSERT(rq->lock_processor == smp_processor_id());
+     rq->lock_processor = -1;
+     rq->lock_line = -1;
+ #endif
+     flags = rq->lock_flags;
+     spin_unlock_irqrestore(&rq->lock, flags);
+ }
+ 
+ void __rq_lock(replay_queue_t rq, int line);
+ #define rq_lock(x) __rq_lock(x, __LINE__)
+ void rq_unlock(replay_queue_t rq);
+ 
+ void dump_it(replay_iterator_t it){
+     if (it) {
+ 	unsigned int idx=ITER_TO_PAGE_IDX(it);
+ 	unsigned int ofs=ITER_TO_PAGE_OFFSET(it);
+ 	printk("<%llu(%u,%u),%u>",it->idx,idx,ofs,it->version);
+     }
+ }
+ 
+ static inline void queue_map_page(struct replay_queue *rq, page_id_t idx) {
+ 	if (in_irq()) {
+ 		return;
+ 	}
+ 
+ 	ASSERT(spin_is_locked(&rq->lock));
+ 	ASSERT(idx < rq->num_pages);
+ 
+ 	if (!rq->mapped_pages[idx]) {
+ 		rq->mapped_pages[idx]=map_domain_page(rq->page_mfns[idx]);
+ 		if(rq->mapped_pages[idx]) {
+ 			rq->num_mapped++;
+ 		} else {
+ 			printk("queue_map_page: WARNING failed to map idx %d!\n", (int)idx);
+ 		}
+ 	}
+ }
+ 
+ static inline void queue_unmap_page(replay_queue_t rq, page_id_t idx) {
+ 	ASSERT(spin_is_locked(&rq->lock));
+ 	if (rq->mapped_pages[idx]) {
+ 		unmap_domain_page(rq->mapped_pages[idx]);
+ 		rq->mapped_pages[idx]=0;
+ 		rq->num_mapped--;
+ 	}
+ }
+ 
+ void queue_dump(replay_queue_t rq) {
+     int i;
+     if (!rq) {
+ 	printk("    Queue: not initialized.\n");
+ 	return;
+     } else {
+ 	printk("    Queue: initialized.\n");
+     }
+     printk("        Pages        : %d\n",rq->num_pages);
+     printk("        Pages mapped : %d\n",rq->num_mapped);
+     /*if (full_debug>2) {
+       for (i=0;i<MAX_PAGES;i++){
+       if (i%32==0) {
+       if (i!=0)
+       printk("\n");
+       printk("        ");
+       }
+       printk("%1d ",!(rq->mapped_pages[i]==0));
+       }
+       }*/
+     printk("\n");
+     //printk("        head_idx     : %u\n",rq->head_idx);
+     //printk("        tail_idx     : %u\n",rq->tail_idx);
+     printk("        cleaned      : ");
+     dump_it(rq->cleaned); 
+     printk("\n");
+     printk("        replayed     : ");
+     dump_it(rq->replayed); 
+     printk("\n");
+     printk("        verified     : ");
+     dump_it(rq->verified); 
+     printk("\n");
+     printk("        head         : ");
+     dump_it(rq->head);
+     printk("\n");
+     printk("        tail         : ");
+     dump_it(rq->tail);
+     printk("\n");
+     printk("        log_sent     : ");
+     dump_it(rq->log_sent);
+     printk("\n");
+     printk("        log_finished : ");
+     dump_it(rq->log_finished);
+     printk("\n");
+     printk("        version      : %u\n",rq->version);
+     printk("        buf_len      : %d\n",rq->buf_len);
+ 
+     if (!(rq->vcpu->domain && rq->vcpu->domain->monitor && rq->vcpu->domain->monitor->ring)) {
+ 	printk("        RING: Unaccessible\n");
+     } else {
+ 	printk("        RING:\n");
+ 	printk("            full (prod,cons,clean): (%u,%u,%u)\n",
+ 	       rq->vcpu->domain->monitor->ring->full_buffer_prod,
+ 	       rq->vcpu->domain->monitor->ring->full_buffer_cons,
+ 	       rq->vcpu->domain->monitor->ring->full_buffer_clean);
+ 	printk("            empty(prod,cons,clean): (%u,%u,%u)\n",
+ 	       rq->vcpu->domain->monitor->ring->empty_buffer_prod,
+ 	       rq->vcpu->domain->monitor->ring->empty_buffer_cons,
+ 	       rq->vcpu->domain->monitor->ring->empty_buffer_clean);
+     }
+     printk("        cleaner      : %s\n",clean_algorithm_name[rq->clean_algorithm]);
+     printk("        replay_doms  : ");
+     for (i=0;i<rq->num_replay_vcpus;i++) {
+ 	printk(" %d",rq->replay_vcpu[i]->domain->domain_id);
+     }
+     printk("\n");
+ 
+     dump_clock(QUEUE_INSERT);
+     dump_clock(QUEUE_CLEANER);
+     dump_clock(QUEUE_UPDATE_LOG_WINDOW);
+     dump_clock(QUEUE_SOFTIRQ);
+     dump_clock(QUEUE_GET_NEXT);    
+ }
+ 
+ void queue_clean_page(replay_queue_t rq, page_id_t idx){
+     queue_map_page(rq,idx);
+     rq->mapped_pages[idx]->prod=0;
+     rq->mapped_pages[idx]->cons=0;
+     queue_unmap_page(rq,idx);
+ }
+ 
+ int ri_next_laps(replay_iterator_t it1, replay_iterator_t it2) {
+     unsigned int 
+ 	cidx1=( ( (it1->idx/EVENTS_PER_PAGE) % it2->q->num_pages ) ),
+ 	nidx1=( ( ((it1->idx+1)/EVENTS_PER_PAGE) % it1->q->num_pages ) ),
+ 	idx2=( ( (it2->idx/EVENTS_PER_PAGE) % it2->q->num_pages ) );
+     if (it1->idx < it2->idx) {
+ 	queue_dump(it1->q);
+     }
+     ASSERT(it1->idx >= it2->idx);
+     ASSERT(it1->q == it2->q);
+     if (cidx1!=idx2 && nidx1 == idx2)  // don't let them end up pointing to the same page.
+ 	return 1;
+     return 0;
+ }
+ 
+ void queue_cleaner(replay_queue_t rq) {
+ 	unsigned idx;
+ 	static int warn_always=0;
+ 
+ 	m_start_clock(QUEUE_CLEANER);
+ 	rq_lock(rq);
+ 	switch (rq->clean_algorithm) {
+ 		case QUEUE_CLEAN_LOGGED:
+ 			printk("Using QUEUE_CLEAN_LOGGED cleaner...\n");
+ 	    
+ 			rq->cleaned->idx = rq->log_finished->idx;
+ 
+ 			break;
+ 		case QUEUE_CLEAN_REPLAYED:
+ 			printk("Using QUEUE_CLEAN_REPLAYED cleaner...\n");
+ 			if (rq && rq->replay_vcpu[0] && rq->replay_vcpu[0]->replay) {
+ 				unsigned long idx;
+ 
+ #if 1
+ 				idx = (rq->replayed->idx < rq->log_finished->idx) ?
+ 						rq->replayed->idx : rq->log_finished->idx;
+ #else
+ 				idx = rq->replayed->idx;
+ #endif
+ 				/* Give us some space to push stuff back on the queue */
+ 				if(rq->cleaned->idx < idx)
+ 					idx--;
+ 
+ 				ASSERT(rq->cleaned->idx <= idx);
+ 
+ 				rq->cleaned->idx = idx;
+ 			} else {
+ 				printk("    NO REPLAYING DOMAINS YET!\n");
+ 			}
+ 			break;
+ 		case QUEUE_CLEAN_VERIFIED:
+ 			while (ri_cmp(rq->cleaned,rq->verified)<0) {
+ 				idx=ITER_TO_PAGE_IDX(rq->cleaned);
+ 				ri_next(rq->cleaned);
+ 			}
+ 			break;
+ 		case QUEUE_CLEAN_NEVER:
+ 			break;
+ 		case QUEUE_CLEAN_ALWAYS:
+ 			if(!warn_always) {
+ 				printk("WARNING: using CLEAN_ALWAYS\n");
+ 				warn_always=1;
+ 			}
+ 			/* FIXME -- virtualize */
+ 			rq->cleaned->idx = rq->tail->idx;
+ 			break;
+ 		default:
+ 			printk("ERROR: Failure to find appropriate queue cleaner!\n");
+ 			break;
+ 	}
+ #if !TESTING_DISABLE_EXTRAVIRT
+ 	if (rq && rq->vcpu && IS_REPLAY_TOOFULL(rq->vcpu)) {
+ 		if(queue_free(rq)>REPLAY_BUFFER_LEN*2)  {
+ 			clear_toofull(rq->vcpu);
+ 		}
+ 	}
+ #endif
+ 	rq_unlock(rq);
+ 	m_stop_clock(QUEUE_CLEANER);
+ }
+ 
+ void queue_update_buffer_window(replay_queue_t rq) {
+ 	unsigned long new_start, new_end, i;
+ 	m_start_clock(QUEUE_UPDATE_BUFFER_WINDOW);
+ 
+ 	rq_lock(rq);
+     
+ 	/* Find out where "here" is */
+ 	new_start = ITER_TO_PAGE_NUMBER(rq->tail);
+ 
+ 	/* Update buffer window size if necessary */
+ 	if ((rq->buf_len < MAX_MAPPED/4)
+ 		    && ((rq->bw_end - ITER_TO_PAGE_IDX(rq->tail)) < rq->buf_len / 2)) {
+ 		rq->buf_len *= 2;
+ 		    }
+ 
+ 		    new_end = new_start + rq->buf_len;
+ 
+ 		    /* Unmap from bw_start to here */
+ 		    for(i=rq->bw_start; i < new_start ; i++) 
+ 			    queue_unmap_page(rq, i % rq->num_pages);
+    
+ 		    rq->bw_start = new_start;
+ 
+ 		    /* Map from current end to new end */
+ 		    for(i = rq->bw_end ; i < new_end ; i++)
+ 			    queue_map_page(rq, i % rq->num_pages);
+ 
+ 		    rq->bw_end = new_end;
+  
+ 		    rq_unlock(rq);
+ 		    m_stop_clock(QUEUE_UPDATE_BUFFER_WINDOW);
+ }
+ 
+ static void clear_toofull(struct vcpu *vl) {
+ 	struct vcpu *vp;
+ 
+ 	clear_bit(_REPLAYFLAG_TOOFULL, &vl->replay->replay_flags);
+ 
+ 	/* FIXME -- think about this some more */
+ 	vl->replay->needs_poke=1;
+ 
+ 	for_each_vcpu(vl->domain, vp) {
+ 		if((vp == vl) 
+ 				  || !test_bit(_VCPUF_initialised, &vp->vcpu_flags)
+ 				  || !vp->replay)
+ 			continue;
+ 		if(vp->replay->replay_flags & REPLAYFLAG_PEER_TOOFULL) {
+ 			printk("%s: waking vcpu %d's peer %d\n",
+ 				__func__, vl->vcpu_id, vp->vcpu_id);
+ 			clear_bit(_REPLAYFLAG_PEER_TOOFULL, &vp->replay->replay_flags);
+ 			vl->replay->needs_poke=1;
+ 		} 
+ 	}
+ }
+ 
+ 
+ void queue_update_log_window(replay_queue_t rq){
+     unsigned long buf_mfn;
+     unsigned idx;
+     struct monitoring_domain_state *m;
+     struct record_event_buffer *reb;
+     struct domain *d = rq->vcpu->domain;
+     replay_queue_t cur_rq=NULL;
+     
+     printk("queue_update_log_window\n");
+     if(IS_LOGGING(d) || d->monitor->ring == NULL){
+         return;
+     }
+     
+     m_start_clock(&m->lock);
+     
+     m=d->monitor;
+     spin_lock(&m->lock);
+     
+     while(m->ring->full_buffer_clean < m->ring->full_buffer_cons) {
+         //Find the index, and make sure the page is actually valid
+ 	idx = m->ring->full_buffer_clean % RECORD_BUFFERS_PER_RING;
+ 	buf_mfn = m->ring->buffers[idx];
+ 	ASSERT(buf_mfn != 0);
+ 	
+ 	//update the log_finished
+ 	reb = map_domain_page(buf_mfn);  //domain_page.h
+ 	ASSERT(reb->vcpu != 0);
+ 	
+ 	if(cur_rq != d->vcpu[reb->vcpu-1]->replay->rq){
+ 	    if(cur_rq){
+ 	        rq_unlock(cur_rq);
+ 	    }
+ 	    
+ 	    cur_rq = d->vcpu[reb->vcpu-1]->replay->rq;
+ 	    rq_lock(cur_rq);
+ 	}
+ 	
+ 	cur_rq->log_finished->idx += EVENTS_PER_PAGE;
+ 	cur_rq->pages_sent--;
+ 	
+ 	//update log_finished and pages sent
+ 	unmap_domain_page(reb);
+ 	reb=NULL;
+ 	
+ 	//assumes that the window of exposed state is always shown
+ 	//between the full_buffer_producer/consumer
+ 	m->ring->buffers[idx]=0;
+ 	
+ 	m->ring->full_buffer_clean++;
+     }
+     
+     if(cur_rq != rq){
+         if(cur_rq)
+ 		rq_unlock(cur_rq);
+ 	rq_lock(rq);
+     }
+     cur_rq=NULL;
+     
+     if(rq && rq->vcpu && IS_REPLAY_TOOFULL(rq->vcpu)){
+         if(queue_free(rq)>REPLAY_BUFFER_LEN*2){
+ 	    clear_toofull(rq->vcpu);
+ 	}
+     }
+     
+         // update the pages available to the full buffer
+     while (rq->pages_sent<100 &&
+ 	   (ri_pagedist(rq->log_sent,rq->tail)>0)) {
+ 
+ 	    idx = m->ring->full_buffer_prod % RECORD_BUFFERS_PER_RING;
+ 
+ 	    ASSERT(m->ring->buffers[idx] == 0);
+ 
+ 	    m->ring->buffers[idx]=ri_mfn(rq->log_sent);
+ 
+ 	    rq->log_sent->idx += EVENTS_PER_PAGE;
+ 
+ 	    m->ring->full_buffer_prod++;
+ 	    rq->pages_sent++;
+ 
+ 	    h_evtchn_set_pending(m->logger_domain->vcpu[0], m->port);   //event.h
+ 	   }
+ 	     
+ 	   wmb();
+ 	   rq_unlock(rq);
+ 	   spin_unlock(&m->lock);
+ 	   m_stop_clock(QUEUE_UPDATE_LOG_WINDOW);
+     printk("queue_update_log_window done...\n");
+ }
+ 
+ void queue_soft_irq_handler(replay_queue_t rq)
+ {
+     ASSERT(local_irq_is_enabled());
+     s_start_clock(QUEUE_SOFTIRQ);
+         
+     queue_update_log_window(rq);
+     printk("queue_update_log_window\n");
+     queue_update_buffer_window(rq);
+     queue_cleaner(rq);
+     
+ #if !TESTING_DISABLE_EXTRAVIRT
+     queue_replay_poke(rq);
+     printk("replay_queue.c queue_soft_irq_handler/queue_replay_poke\n");
+ #endif
+     s_stop_clock(QUEUE_SOFTIRQ);
+     ASSERT(local_irq_is_enabled());
+ }
+ 
+ long queue_register_replay_vcpu(replay_queue_t rq, struct vcpu *v) {
+     if (!rq) {
+ 	printk("ERROR: Attempt to register a replaying domain with a nonexistant replay queue!\n");
+ 	return 0;
+     } 
+ 
+     rq_lock(rq);
+     
+     rq->replay_vcpu[rq->num_replay_vcpus++]=v;
+     ASSERT(v->replay->rq==NULL);
+     v->replay->rq=rq;
+ 
+     rq_unlock(rq);
+     return 1;
+ }
+ 
+ replay_queue_t queue_init(unsigned long *pages, int n_pages, struct vcpu *v) {
+     int i;
+     replay_queue_t rq;
+ 
+     printk("queue_init\n");
+     rq=xmalloc(struct replay_queue);
+ 
+     spin_lock_init(&rq->lock);
+ 
+     ASSERT(sizeof(replay_page_t)<=4096);
+ 
+     rq_lock(rq);
+ 
+     rq->num_pages  =n_pages;
+     rq->num_mapped =0;
+     rq->buf_len    =10;
+     rq->version    =0;
+ 
+     rq->bw_start = rq->bw_end = 0;
+ 
+     /* 
+      * If this is the first time, start with NEVER, to make sure that
+      * if there /is/ a replaying domain, things stick around until it
+      * gets there.  If we're bringing up another vcpu, just copy vcpu 0.
+      */
+     if(v->vcpu_id == 0) 
+         rq->clean_algorithm = QUEUE_CLEAN_NEVER;  // NEVER should be default as it is the most conservative
+     else 
+         rq->clean_algorithm = v->domain->vcpu[0]->replay->rq->clean_algorithm;
+ 
+     for (i=0;i<MAX_REPLAY_VCPUS;i++) {
+ 	rq->replay_vcpu[i]=NULL;
+     }
+     rq->num_replay_vcpus=0;
+ 
+     for (i=0;i<MAX_PAGES;i++) {
+ 	rq->page_mfns[i]=0;
+ 	rq->mapped_pages[i]=0;
+     }
+ 
+     for (i=0;i<n_pages && i<MAX_PAGES;i++) {
+ 	rq->page_mfns[i]=pages[i];
+ 	queue_clean_page(rq,i);
+     }
+ 
+     printk("Initializing vcpu #%d on %d pages (%d events per page)\n",
+                 v->vcpu_id,rq->num_pages,EVENTS_PER_PAGE);
+     for (i=0;i<rq->num_pages;i++) {
+ 	queue_map_page(rq,i);
+ 	rq->mapped_pages[i]->vcpu=(v->vcpu_id+1);
+ 	queue_unmap_page(rq,i);
+     }
+ 
+ //next test
+     printk("mapping pages...\n");
+     for (i=0;i<rq->buf_len;i++)
+ 	queue_map_page(rq,i);
+ 
+     rq->vcpu=v;
+ 
+     ri_new(&rq->cleaned,rq);
+     rq->cleaned->idx  = 0;
+ 
+     ri_new(&rq->replayed,rq);
+     rq->cleaned->idx  = 0;
+ 
+     ri_new(&rq->verified,rq);
+     rq->cleaned->idx  = 0;
+ 
+     ri_new(&rq->head, rq);
+     rq->head->idx     = 0;
+ 
+     ri_new(&rq->tail, rq);
+     rq->tail->idx     = 0;
+ 
+     ri_new(&rq->log_sent, rq);
+     rq->log_sent->idx = 0;
+ 
+     ri_new(&rq->log_finished, rq);
+     rq->log_finished->idx   = 0;
+ 
+     rq->pages_sent=0;
+ 
+     if (IS_REPLAYING(rq->vcpu->domain)) {
+ 	queue_register_replay_vcpu(rq,rq->vcpu);
+     }
+     
+     queue_dump(rq);
+ 
+     rq_unlock(rq);
+ 
+     return rq;
+ }
+ 
+ int queue_insert(replay_queue_t rq, record_event_t *re){
+     int ret=1;
+     unsigned int idx, ofs;
+ 
+     m_start_clock(QUEUE_INSERT);
+     ASSERT(rq);
+ 
+     idx=ITER_TO_PAGE_IDX(rq->tail);
+     ofs=ITER_TO_PAGE_OFFSET(rq->tail);
+ 
+     if (ri_next_laps(rq->tail,rq->cleaned)) {  // if we will lap cleaned, then fail.
+ 	printk("Queue for domain %d is full!\n",rq->vcpu->vcpu_id);
+         ret=0;
+         goto out;
+     }
+ 
+     queue_map_page(rq,idx);    
+ 
+ #if !TESTING_MINIMAL
+     if(!rq->mapped_pages[idx]) {
+ 	printk("queue_insert: queue_map_page(%x) failed!\n", idx);
+ 	BUG();
+     }
+ #endif
+ 
+     m_start_clock(RECORD_COPY_INSERT);
+     rq->mapped_pages[idx]->events[ofs]=*re;
+     m_stop_clock(RECORD_COPY_INSERT);
+ 
+     rq->mapped_pages[idx]->prod++;
+ 
+     ri_next(rq->tail);
+ 
+     if(!ofs)
+       raise_softirq(RECORD_SOFTIRQ);
+ 
+    out:
+     m_stop_clock(QUEUE_INSERT);
+     return ret;
+ }
+ 
+ int queue_get_next(replay_queue_t rq, replay_checkpoint_t *c, int type, record_event_t *e){
+     printk("REPLAY_QUEUE NEED TO FIX\n");
+ #if 0
+     record_event_t *rep;
+     replay_iterator_t it;
+ 
+     if (!rq) {
+ 	return 0;
+     } 
+ 
+     m_start_clock(QUEUE_GET_NEXT);
+     rq_lock(rq);
+ 
+     /* HACK to have only one "following" iterator */
+     if(type == NEXT_ASYNC)
+ 	it=c->iterators[type];
+     else {
+         ASSERT(rq->num_replay_vcpus == 1);
+ 	it=rq->replayed;
+     }
+ 
+     if (!ri_valid(it)) {
+         goto out;
+     }
+ 
+ 
+     if (type==NEXT_ASYNC || type==CONS_ASYNC) {
+ 	while (ri_valid(it) 
+                && (ri_pvalue(it)->ignore
+                    || !queue_is_async(ri_pvalue(it)->type))) {
+ 	    ri_next(it);
+ 	}
+ #if 0
+ #error Make sure to free readahead below if you bring it back.
+         ri_clone(&readahead, it);
+         /* Look ahead for out-of-order CREW events */
+         while (ri_valid(readahead) 
+                && ri_pvalue(readahead)->when.eip == ri_pvalue(it)->when.eip) {
+             int type=ri_pvalue(readahead)->type;
+             /* We already know the eip is the same from the check above.
+              * If the pcounter is less, take it. */
+             if(((type == NEXT_CREW) || (type == NEXT_CREW_CONST))
+                && ri_pvalue(readahead)->when.perfctr < ri_pvalue(it)->when.perfctr) {
+                 printk("%s: %llx < %llx, using read-ahead value %llx\n",
+                         __func__, ri_pvalue(readahead)->when.perfctr,
+                         ri_pvalue(it)->when.perfctr, readahead->idx);
+                 /* WARNING think about this carefully */
+                 it = readahead;
+                 break;
+             }
+             ri_next(readahead);
+         }
+ #endif
+     } else {
+ 	/* Skip entries that don't pull anything */
+ 	rep=ri_pvalue(it);
+ 	while(ri_valid(it) 
+               && ((rep->type == NEXT_CREW)
+ 	          || (rep->type == NEXT_CREW_CONST)
+ 	          || (rep->type == RECORD_EVT_NULL)
+                   || (rep->ignore))) {
+ 	    ri_next(it);
+ 	    rep=ri_pvalue(it);
+ 	}
+ 
+ 	/* Return either type, or CHECKPOINT */
+ 	if(rep->type != type
+ 	   && rep->type != RECORD_EVT_CHECKPOINT) {
+ 	   /* If we're looking for a bts entry but dont' have it yet, don't
+ 	    * cause trouble. */
+ 	   if(type != RECORD_EVT_CHECKPOINT) {
+ 	    printk("queue_get_next: wanted type %s(%d), got type %s(%d)(@%llx)!\n",
+ 		   record_event_types[type],type, 
+ 		   record_event_types[rep->type],rep->type,rep->when.perfctr);
+             set_bit(_VCPUF_ctrl_pause, &rq->replay_vcpu[0]->vcpu_flags);
+             raise_softirq(SCHEDULE_SOFTIRQ);
+             rq_unlock(rq);
+             queue_flush(rq);
+ 	    dump_breakpoints('B');
+ 	    m_stop_clock(QUEUE_GET_NEXT);
+             return 0;
+ 	   }
+ 	   goto out;
+ 
+ 	}
+     }
+     if (ri_valid(it)) {  // we found an appropriate entry
+         m_start_clock(RECORD_COPY);
+ 	*e=*ri_pvalue(it);
+         m_stop_clock(RECORD_COPY);
+ 	ri_next(it);
+ 	if (!(type==NEXT_ASYNC || type==CONS_ASYNC)) {
+ 	    raise_softirq(SCHEDULE_SOFTIRQ);
+ 	}
+ 	rq_unlock(rq);
+ 	queue_cleaner(rq);
+ 	m_stop_clock(QUEUE_GET_NEXT);
+ 	return 1;
+     }
+ 
+    out:
+     rq_unlock(rq);
+     m_stop_clock(QUEUE_GET_NEXT);
+ #endif
+     return 0;
+ }
+ 
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/arch/x86/setup.c new/xen-3.3.0/xen/arch/x86/setup.c
*** xen-3.3.0/xen/arch/x86/setup.c	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/arch/x86/setup.c	2009-06-09 19:25:25.000000000 -0400
*************** void __init __start_xen(unsigned long mb
*** 983,988 ****
--- 983,989 ----
  
      if ( opt_watchdog ) 
          watchdog_enable();
+     printk("arch/x86/setup.c: watchdog_enable\n");
  
      /* Create initial domain 0. */
      dom0 = domain_create(0, 0, DOM0_SSIDREF);
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/arch/x86/timer.c new/xen-3.3.0/xen/arch/x86/timer.c
*** xen-3.3.0/xen/arch/x86/timer.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/arch/x86/timer.c	2009-06-10 17:02:25.000000000 -0400
***************
*** 0 ****
--- 1,7 ----
+ #include <xen/timer.h>
+ 
+ int clocks_initialized=0;
+ int user_active=0;
+ unsigned long long clock_cpu_freq=0;
+ struct clock clocks[DOM_MAX_CLOCKS][MAX_CLOCK_CPUS];
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/arch/x86/x86_32/record.c new/xen-3.3.0/xen/arch/x86/x86_32/record.c
*** xen-3.3.0/xen/arch/x86/x86_32/record.c	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/arch/x86/x86_32/record.c	2009-05-31 20:58:42.000000000 -0400
***************
*** 0 ****
--- 1,9 ----
+ #include <xen/config.h>
+ #include <xen/guest_access.h>
+ #include <xen/hypercall.h>
+ 
+ int do_my_hyper_op(int number)
+ {
+     printk("my hyper_op\n");
+ 	return (number+10);
+ }
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/common/event_channel.c new/xen-3.3.0/xen/common/event_channel.c
*** xen-3.3.0/xen/common/event_channel.c	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/common/event_channel.c	2009-06-08 14:27:21.000000000 -0400
*************** static int get_free_port(struct domain *
*** 119,124 ****
--- 119,139 ----
      return port;
  }
  
+ int evtchn_alloc_port(struct domain *d)  //prototype xen/event.h
+ {
+     struct evtchn *chn;
+     int r;
+ 
+     spin_lock(&d->evtchn_lock);
+     r = get_free_port(d);
+     if (r >= 0) {
+     	chn = evtchn_from_port(d, r);
+ 	chn->state = ECS_UNBOUND;
+         chn->u.unbound.remote_domid = d->domain_id;
+     }
+     spin_unlock(&d->evtchn_lock);
+     return r;
+ }
  
  static long evtchn_alloc_unbound(evtchn_alloc_unbound_t *alloc)
  {
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/common/softirq.c new/xen-3.3.0/xen/common/softirq.c
*** xen-3.3.0/xen/common/softirq.c	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/common/softirq.c	2009-06-12 14:54:46.000000000 -0400
*************** asmlinkage void do_softirq(void)
*** 42,47 ****
--- 42,48 ----
              break;
  
          i = find_first_set_bit(pending);
+         printk("do softirq_handler: %d\n", i);
          clear_bit(i, &softirq_pending(cpu));
          (*softirq_handlers[i])();
      }
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/common/timer.c new/xen-3.3.0/xen/common/timer.c
*** xen-3.3.0/xen/common/timer.c	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/common/timer.c	2009-06-03 13:47:25.000000000 -0400
***************
*** 1,3 ****
--- 1,4 ----
+ 
  /******************************************************************************
   * timer.c
   * 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/asm-x86/bitops.h new/xen-3.3.0/xen/include/asm-x86/bitops.h
*** xen-3.3.0/xen/include/asm-x86/bitops.h	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/include/asm-x86/bitops.h	2009-06-04 16:16:34.000000000 -0400
*************** static inline void set_bit(int nr, volat
*** 46,55 ****
--- 46,57 ----
          : "=m" (ADDR)
          : "Ir" (nr), "m" (ADDR) : "memory");
  }
+ #if 0
  #define set_bit(nr, addr) ({                            \
      if ( bitop_bad_size(addr) ) __bitop_bad_size();     \
      set_bit(nr, addr);                                  \
  })
+ #endif
  
  /**
   * __set_bit - Set a bit in memory
*************** static inline int test_and_set_bit(int n
*** 181,190 ****
--- 183,194 ----
          : "Ir" (nr), "m" (ADDR) : "memory");
      return oldbit;
  }
+ #if 0
  #define test_and_set_bit(nr, addr) ({                   \
      if ( bitop_bad_size(addr) ) __bitop_bad_size();     \
      test_and_set_bit(nr, addr);                         \
  })
+ #endif
  
  /**
   * __test_and_set_bit - Set a bit and return its old value
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/asm-x86/domain.h new/xen-3.3.0/xen/include/asm-x86/domain.h
*** xen-3.3.0/xen/include/asm-x86/domain.h	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/include/asm-x86/domain.h	2009-06-03 14:54:38.000000000 -0400
*************** struct arch_domain
*** 263,268 ****
--- 263,271 ----
      struct list_head relmem_list;
  
      cpuid_input_t cpuids[MAX_CPUID_INPUT];
+ 
+     //for record and replay
+     unsigned int shadow_nest;
  } __cacheline_aligned;
  
  #define has_arch_pdevs(d)    (!list_empty(&(d)->arch.pdev_list))
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/asm-x86/event.h new/xen-3.3.0/xen/include/asm-x86/event.h
*** xen-3.3.0/xen/include/asm-x86/event.h	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/include/asm-x86/event.h	2009-06-04 14:59:11.000000000 -0400
***************
*** 10,15 ****
--- 10,62 ----
  #define __ASM_EVENT_H__
  
  #include <xen/shared.h>
+ #include <xen/sched.h>
+ 
+ static inline void evtchn_notify(struct vcpu *v)
+ {
+ }
+ 
+ #if 0
+ static inline void evtchn_set_pending(struct vcpu *v, int port)
+ {
+     struct domain *d = v->domain;
+     struct shared_info *s = d->shared_info;
+     int running;
+ 
+     extern void monitored_evtchn_set_pending(struct vcpu *ed,int port);
+ 
+     if ( test_bit(_VCPUF_replaying, &v->vcpu_flags) )
+         return;
+     if ( test_bit(_VCPUF_monitored, &v->vcpu_flags) ) {
+         monitored_evtchn_set_pending(v, port);
+         return;
+     }
+ 
+     /* These three operations must happen in strict order. */
+     if ( !test_and_set_bit(port,    &s->evtchn_pending[0]) &&
+          !test_bit        (port,    &s->evtchn_mask[0])    &&
+          !test_and_set_bit(port>>5, &v->vcpu_info->evtchn_pending_sel) )
+     {
+         /* The VCPU pending flag must be set /after/ update to evtchn-pend. */
+         set_bit(0, &v->vcpu_info->evtchn_upcall_pending);
+         evtchn_notify(v);
+ 
+         /*
+          * NB1. 'vcpu_flags' and 'processor' must be checked /after/ update of
+          * pending flag. These values may fluctuate (after all, we hold no
+          * locks) but the key insight is that each change will cause
+          * evtchn_upcall_pending to be polled.
+          * 
+          * NB2. We save VCPUF_running across the unblock to avoid a needless
+          * IPI for domains that we IPI'd to unblock.
+          */
+         running = test_bit(_VCPUF_running, &v->vcpu_flags);
+         vcpu_unblock(v);
+         if ( running )
+             smp_send_event_check_cpu(v->processor);
+     }
+ }
+ #endif
  
  static inline void vcpu_kick(struct vcpu *v)
  {
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/asm-x86/hypercall.h new/xen-3.3.0/xen/include/asm-x86/hypercall.h
*** xen-3.3.0/xen/include/asm-x86/hypercall.h	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/include/asm-x86/hypercall.h	2009-06-01 10:19:09.000000000 -0400
***************
*** 7,12 ****
--- 7,13 ----
  
  #include <public/physdev.h>
  #include <xen/types.h>
+ #include </usr/include/replay.h>
  
  /*
   * Both do_mmuext_op() and do_mmu_update():
*************** extern int
*** 108,113 ****
--- 109,122 ----
  do_kexec(
      unsigned long op, unsigned arg1, XEN_GUEST_HANDLE(void) uarg);
  
+ /* for replay begin */
+ extern int do_my_hyper_op(
+     int number
+ );
+ 
+ extern long do_replay_op(replay_op_t *uop);
+ /* for replay end */
+ 
  #ifdef __x86_64__
  
  extern long
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/asm-x86/msr.h new/xen-3.3.0/xen/include/asm-x86/msr.h
*** xen-3.3.0/xen/include/asm-x86/msr.h	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/include/asm-x86/msr.h	2009-06-08 16:07:30.000000000 -0400
***************
*** 8,13 ****
--- 8,18 ----
  #include <xen/smp.h>
  #include <xen/percpu.h>
  
+ #define MSR_IA32_MISC_ENABLE_FOP_COMPAT   (1<<2)
+ #define MSR_IA32_MISC_ENABLE_PERF_AVAIL   (1<<7)
+ #define MSR_IA32_MISC_ENABLE_BTS_UNAVAIL  (1<<11)
+ #define MSR_IA32_MISC_ENABLE_PEBS_UNAVAIL (1<<12)
+ 
  #define rdmsr(msr,val1,val2) \
       __asm__ __volatile__("rdmsr" \
  			  : "=a" (val1), "=d" (val2) \
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/asm-x86/perfctr.h new/xen-3.3.0/xen/include/asm-x86/perfctr.h
*** xen-3.3.0/xen/include/asm-x86/perfctr.h	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/include/asm-x86/perfctr.h	2009-06-04 10:44:54.000000000 -0400
***************
*** 0 ****
--- 1,13 ----
+ #ifndef __ASM_PERFCTR_H
+ #define __ASM_PERFCTR_H
+ #include <xen/sched.h>
+ unsigned long long get_pcounter_val(struct vcpu *);
+ void setup_performance_counters_cpu(void);
+ void start_performance_counters(struct vcpu *);
+ void perfctr_extra(struct vcpu *);
+ void request_perfctr_interrupt(struct vcpu *, unsigned long long, void *, 
+ 	void(*)(struct vcpu*,void *));
+ void do_perfctrs_scheduler(struct vcpu *, struct vcpu *);
+ #define PERFCTR_INIT 1
+ #endif
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/asm-x86/shadow.h new/xen-3.3.0/xen/include/asm-x86/shadow.h
*** xen-3.3.0/xen/include/asm-x86/shadow.h	2008-08-22 05:49:09.000000000 -0400
--- new/xen-3.3.0/xen/include/asm-x86/shadow.h	2009-06-10 17:17:57.000000000 -0400
***************
*** 30,36 ****
--- 30,87 ----
  #include <asm/flushtlb.h>
  #include <asm/paging.h>
  #include <asm/p2m.h>
+ #include <xen/replay.h>
  
+ /*
+ static inline
+ void init_clocks() {
+     memset(clocks,0,sizeof(struct clock)*DOM_MAX_CLOCKS*MAX_CLOCK_CPUS);
+     clock_cpu_freq=dom0->shared_info->cpu_freq;
+ }
+ 
+ static inline
+ void stop_clock(int name) {
+     struct clock * d=get_clock(name);
+     if(name >= USER_CLOCK_LOG_1)
+         user_active--;
+ 
+     atomic_dec(&d->startcount);
+     d->count++;
+     
+     if(d->rdtsc_start != -1 && atomic_read(&d->startcount)==0) {
+         rdtscll(d->rdtsc_stop);
+         d->sum += d->rdtsc_stop - d->rdtsc_start;
+     }
+ 
+     if(name >= USER_CLOCK_LOG_1 && !user_active) {
+ 	 dump_clocks('C');
+     }
+ }
+ 
+ static inline
+ void start_clock(int name) {
+     if (!clocks_initialized) {
+ 	init_clocks();
+ 	clocks_initialized=1;
+     }
+     struct clock * d=get_clock(name);;
+     if(name >= USER_CLOCK_LOG_1) 
+         user_active++;
+  
+     rdtscll(d->rdtsc_start);
+     
+     if((name >= USER_CLOCK_LOG_1) && (user_active == 1)) {
+       clear_clocks(d->rdtsc_start);
+     }
+ 
+     atomic_inc(&d->startcount);
+     d->count++;
+     
+     if(!user_active)
+       d->rdtsc_start = -1;
+ }
+ 
+ */
  /*****************************************************************************
   * Macros to tell which shadow paging mode a domain is in*/
  
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/public/xen.h new/xen-3.3.0/xen/include/public/xen.h
*** xen-3.3.0/xen/include/public/xen.h	2008-08-22 05:49:10.000000000 -0400
--- new/xen-3.3.0/xen/include/public/xen.h	2009-06-01 09:42:26.000000000 -0400
*************** DEFINE_XEN_GUEST_HANDLE(xen_pfn_t);
*** 91,96 ****
--- 91,100 ----
  #define __HYPERVISOR_sysctl               35
  #define __HYPERVISOR_domctl               36
  #define __HYPERVISOR_kexec_op             37
+ /* for record and replay op*/
+ #define __HYPERVISOR_my_hyper_op           38
+ #define __HYPERVISOR_replay_op               39
+ 
  
  /* Architecture-specific hypercall definitions. */
  #define __HYPERVISOR_arch_0               48
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/xen/bts.h new/xen-3.3.0/xen/include/xen/bts.h
*** xen-3.3.0/xen/include/xen/bts.h	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/include/xen/bts.h	2009-06-08 13:39:32.000000000 -0400
***************
*** 0 ****
--- 1,10 ----
+ #ifndef __XEN_BTS_H
+ #define __XEN_BTS_H
+ #include <xen/sched.h>
+ int bts_trace_vcpu(struct vcpu *v);
+ int bts_untrace_vcpu(struct vcpu * v);
+ void bts_poke(struct vcpu * v);
+ void bts_insert(struct vcpu *v, unsigned long from, unsigned long to);
+ void do_bts_scheduler(struct vcpu *from, struct vcpu *to);
+ void dump_bts_state(unsigned char key);
+ #endif
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/xen/crew.h new/xen-3.3.0/xen/include/xen/crew.h
*** xen-3.3.0/xen/include/xen/crew.h	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/include/xen/crew.h	2009-06-10 15:40:15.000000000 -0400
***************
*** 0 ****
--- 1,3 ----
+ 
+ void crew_xen_grab_shared_info(struct vcpu *v, int common);
+ void crew_boot_vcpu(struct vcpu * v);
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/xen/domain.h new/xen-3.3.0/xen/include/xen/domain.h
*** xen-3.3.0/xen/include/xen/domain.h	2008-08-22 05:49:10.000000000 -0400
--- new/xen-3.3.0/xen/include/xen/domain.h	2009-06-08 14:06:05.000000000 -0400
*************** void arch_dump_domain_info(struct domain
*** 54,59 ****
--- 54,63 ----
  
  void arch_vcpu_reset(struct vcpu *v);
  
+ int __execstate_is_synced(void);
+ 
+ void sync_lazy_execstate_cpu(unsigned int cpu);
+ 
  extern unsigned int xen_processor_pmbits;
  
  #endif /* __XEN_DOMAIN_H__ */
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/xen/event.h new/xen-3.3.0/xen/include/xen/event.h
*** xen-3.3.0/xen/include/xen/event.h	2008-08-22 05:49:10.000000000 -0400
--- new/xen-3.3.0/xen/include/xen/event.h	2009-06-11 13:46:25.000000000 -0400
***************
*** 13,26 ****
  #include <xen/sched.h>
  #include <xen/smp.h>
  #include <xen/softirq.h>
! #include <asm/bitops.h>
  #include <asm/event.h>
  
  /*
   * send_guest_vcpu_virq: Notify guest via a per-VCPU VIRQ.
   *  @v:        VCPU to which virtual IRQ should be sent
   *  @virq:     Virtual IRQ number (VIRQ_*)
   */
  void send_guest_vcpu_virq(struct vcpu *v, int virq);
  
  /*
--- 13,72 ----
  #include <xen/sched.h>
  #include <xen/smp.h>
  #include <xen/softirq.h>
! #include <xen/bitops.h>
  #include <asm/event.h>
+ #include <asm/bitops.h>
  
  /*
   * send_guest_vcpu_virq: Notify guest via a per-VCPU VIRQ.
   *  @v:        VCPU to which virtual IRQ should be sent
   *  @virq:     Virtual IRQ number (VIRQ_*)
   */
+ 
+ int evtchn_alloc_port(struct domain *d);
+ 
+ static inline void h_evtchn_set_pending(struct vcpu *v, int port)
+ {
+     struct domain *d = v->domain;
+     struct shared_info *s = d->shared_info;
+     int            running;
+ 
+     extern void monitored_evtchn_set_pending(struct vcpu *ed,int port);
+ 
+     printk("h_evtchn_set_pending\n");
+     if ( test_bit(_VCPUF_replaying, &v->vcpu_flags) )
+         return;
+     if ( test_bit(_VCPUF_monitored, &v->vcpu_flags) ) {
+         monitored_evtchn_set_pending(v, port);
+         return;
+     }
+ 
+     /* These three operations must happen in strict order. */
+     if ( !test_and_set_bit(port,    &s->evtchn_pending[0]) &&
+          !test_bit        (port,    &s->evtchn_mask[0])    &&
+          !test_and_set_bit(port>>5, &v->vcpu_info->evtchn_pending_sel) )
+     {
+         /* The VCPU pending flag must be set /after/ update to evtchn-pend. */
+         set_bit(0, &v->vcpu_info->evtchn_upcall_pending);
+         evtchn_notify(v);
+ 
+         /*
+          * NB1. 'vcpu_flags' and 'processor' must be checked /after/ update of
+          * pending flag. These values may fluctuate (after all, we hold no
+          * locks) but the key insight is that each change will cause
+          * evtchn_upcall_pending to be polled.
+          * 
+          * NB2. We save VCPUF_running across the unblock to avoid a needless
+          * IPI for domains that we IPI'd to unblock.
+          */
+         running = test_bit(_VCPUF_running, &v->vcpu_flags);
+         vcpu_unblock(v);
+         if ( running )
+             smp_send_event_check_cpu(v->processor);
+     }
+     printk("h_evtchn_set_pending done...\n");
+ }
+ 
  void send_guest_vcpu_virq(struct vcpu *v, int virq);
  
  /*
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/xen/replay.h new/xen-3.3.0/xen/include/xen/replay.h
*** xen-3.3.0/xen/include/xen/replay.h	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/include/xen/replay.h	2009-06-12 17:08:58.000000000 -0400
***************
*** 0 ****
--- 1,722 ----
+ #ifndef __REPLAY_H__
+ #define __REPLAY_H__
+ 
+ /* Maximum number of vcpus to log. */
+ #define CREW_LAST_VCPU 2
+ /* This is limited by the size available in the lower 12 bits of the
+  * p2m table.  */
+ #define CREW_MAX_ACTORS (CREW_LAST_VCPU+3)
+ 
+ #if (CREW_MAX_ACTORS > 8)
+ # error "CREW_MAX_ACTORS cannot be more than 8!" 
+ #endif
+ 
+ #define vcpu_to_actor(x) ((x)+1)
+ #define actor_to_vcpu(x) ((x)-1)
+ //#define tip_to_actor(x) ((x)+CREW_LAST_VCPU+1)
+ //#define tip_to_actor(x) (_tip_to_actor[x])
+ #define is_vcpu(x) ((x) <= CREW_LAST_VCPU)
+ #define is_proc(x) (!is_vcpu(x))
+ #define can_preempt(x) is_vcpu(x)
+ 
+ #define CREW_TIP_EVENT        20
+ #define CREW_TIP_CONSTRAINT   21
+ #define DTIME_TIP             22
+ #define BREAK_TIP             23
+ #define SPEND_TIP             24
+ #define DELIVER_TIP           25
+ #define NOTIFY_TIP            26
+ #define	BTS_UPDATE_TIP        27
+ #define UNPAUSE_TIP           28
+ #define NULL_TIP              29
+ #define SHADOW_SYNC_TIP       30
+ 
+ //#define __HYPERVISOR_replay_op  43
+ 
+ //#define	SA_INTERRUPT	0x20000000
+ 
+ #define crew_v2e(_a, _v) ((_v).event[(_a)-1])
+ 
+ typedef unsigned long u_32;
+ 
+ #define vcpu_not_current(_v)	\
+ 		(((_v)!=current) && ((_v)->processor != smp_processor_id()))
+ 
+ #define TESTING_LOG_IGNORE 0
+ #define TESTING_MINIMAL 1
+ #define TESTING_STRICTLY_MINIMAL 1
+ /* These will cause extravirt and/or replay not to work */
+ #define TESTING_DISABLE_EXTRAVIRT 1
+ #define TESTING_DISABLE_CREW 0
+ #define TESTING_FILTER_GPFNS 0
+ 
+ #if TESTING_STRICTLY_MINIMAL
+ # define s_start_clock(_x)
+ # define s_stop_clock(_x)
+ #else
+ # define s_start_clock(_x) start_clock(_x)
+ # define s_stop_clock(_x) stop_clock(_x)
+ #endif
+ 
+ #if TESTING_MINIMAL
+ # define register_check(_x...)
+ # define m_start_clock(_x)
+ # define m_stop_clock(_x)
+ #else
+ # define register_check __register_check
+ # define m_start_clock(_x) start_clock(_x)
+ # define m_stop_clock(_x) stop_clock(_x)
+ #endif
+ 
+ /*
+  * Per-domain flags (domain_flags).
+  */
+  /* Has the guest OS been fully built yet? */
+ #define _DOMF_constructed      0
+ #define DOMF_constructed       (1UL<<_DOMF_constructed)
+  /* Is this one of the per-CPU idle domains? */
+ #define _DOMF_idle_domain      1
+ #define DOMF_idle_domain       (1UL<<_DOMF_idle_domain)
+  /* Is this domain privileged? */
+ #define _DOMF_privileged       2
+ #define DOMF_privileged        (1UL<<_DOMF_privileged)
+  /* May this domain do IO to physical devices? */
+ #define _DOMF_physdev_access   3
+ #define DOMF_physdev_access    (1UL<<_DOMF_physdev_access)
+  /* Guest shut itself down for some reason. */
+ #define _DOMF_shutdown         4
+ #define DOMF_shutdown          (1UL<<_DOMF_shutdown)
+  /* Guest is in process of shutting itself down (becomes DOMF_shutdown). */
+ #define _DOMF_shuttingdown     5
+ #define DOMF_shuttingdown      (1UL<<_DOMF_shuttingdown)
+  /* Death rattle. */
+ #define _DOMF_dying            6
+ #define DOMF_dying             (1UL<<_DOMF_dying)
+ 
+ /* Replay flags */
+ #define  _DOMF_monitored   16 /* Monitor timings of return to userspace      */
+ #define  _DOMF_unplug      17 /* Unplug net ring as soon as this schedules*/ 
+ #define  _DOMF_replaying   18 /* In progress of replaying this domain */ 
+ //#define  _DOMF_HYPERCALL   19 /* Domain in process of doing a hypercall */
+ #define DOMF_monitored             (1UL<<_DOMF_monitored)
+ #define DOMF_unplug             (1UL<<_DOMF_unplug)
+ #define DOMF_replaying             (1UL<<_DOMF_replaying)
+ 
+ #define IS_REPLAYING(ddd)       (test_bit(_DOMF_replaying, &ddd->domain_flags))
+ #define IS_MONITORED(ddd)       (test_bit(_DOMF_monitored, &ddd->domain_flags))
+ #define IS_LOGGING(ddd)         (test_bit(_DOMF_monitored, &ddd->domain_flags))
+ 
+ #define RECORD_SOFTIRQ         8
+ 
+ struct record_ioctl_plug {
+ 		    unsigned dom;
+ 			    unsigned handle;
+ };
+ 
+ struct record_ioctl_attach {
+ 		    unsigned port;
+ 			    unsigned rx_ring_mfn;
+ 				    unsigned tx_ring_mfn;
+ 					    unsigned handle;
+ };
+ 
+ #define RECORD_IOCTL_ATTACH _IOR('R', 3, struct record_ioctl_attach)
+ #define RECORD_IOCTL_PLUG _IOR('R', 1, struct record_ioctl_plug)
+ #define RECORD_IOCTL_UNPLUG _IOR('R', 2, struct record_ioctl_plug)
+ 
+ typedef struct {
+     unsigned long event[CREW_MAX_ACTORS-1];
+ } crew_vector_t;
+ #define CREW_MAX_EVENT_COUNT (((unsigned long)0)-1)
+ 
+ 
+ /* General #defines */
+ #define    MAX_TIPS           32
+ #define    SYNCHRONOUS_EVENT   0
+ 
+ typedef unsigned long long pcounter_t;
+ 
+ /* A representation of a point in time.  */
+ struct time_location {
+     unsigned long eip;
+     unsigned long ecx; /* Needed for string opcodes */
+     pcounter_t perfctr;
+ };
+ 
+ static inline int time_cmp(struct time_location a, struct time_location b) {
+     if (a.perfctr<b.perfctr)
+ 	return -1;
+     if (a.perfctr>b.perfctr)
+ 	return 1;
+     if (a.eip<b.eip)
+ 	return -1;
+     if (a.eip>b.eip)
+ 	return 1;
+     if (a.ecx<b.ecx)
+ 	return -1;
+     if (a.ecx>b.ecx)
+ 	return 1;
+     return 0;  // otherwise they are ==
+ }
+ 
+ /*static inline int time_cmp(struct time_location t1, struct time_location t2) {
+     if (t1.perfctr < t2.perfctr) 
+ 	return -1;
+     if (t1.perfctr == t2.perfctr &&
+ 	t1.eip < t2.eip)
+ 	return -1;
+     if (t1.perfctr == t2.perfctr &&
+ 	t1.eip == t2.eip &&
+ 	t1.ecx < t2.ecx) 
+ 	return -1;
+     if (t1.perfctr == t2.perfctr &&
+         t1.eip == t2.eip &&
+         t1.ecx == t2.ecx)
+         return 0;
+     return 1;
+     }*/
+ 
+ /* Breakpoint request structure */
+ struct breakpoint {
+   struct time_location    time;
+   int                     dom_id;
+   int                     vcpu;
+   int                     tip_num;  // -1 for none
+ };
+ 
+ //typedef unsigned long long tsc_timestamp_t;
+ 
+ typedef struct record_event {
+    enum {
+         RECORD_EVT_NULL=0,
+         RECORD_EVT_SCHED,
+         RECORD_EVT_HYPERRESULT,
+         RECORD_EVT_DOMTIME,
+         RECORD_EVT_EVTCHN_SET_PENDING,
+         RECORD_EVT_CHECKPOINT, /* 5 */
+         RECORD_EVT_RDTSC,
+         RECORD_EVT_HCALLBACK,
+         RECORD_EVT_NOTIFY,
+         RECORD_EVT_CREW_EVENT,
+         RECORD_EVT_CREW_CONSTRAINT, /* 10 */
+         RECORD_EVT_BTS_STATE,
+         RECORD_EVT_UNPAUSE,
+         RECORD_EVT_REGISTER_CHECK,
+         RECORD_EVT_SHADOW_SYNC,
+         RECORD_EVT_MAX,
+    } type:4; /* 4 bits */
+     unsigned unpause_validate:2; /* 6 bits */
+     unsigned ignore:1; /* 7 bits */
+     unsigned user_timer:1; /* 8 bits */
+     struct time_location when; /* 17 bytes */
+ //#define REPLAY_FULLCONTEXT
+ #ifdef REPLAY_FULLCONTEXT
+     cpu_user_regs_t ctx; /* lots */
+ #endif
+     union {
+ 	struct {
+ 	    unsigned call;
+ 	    unsigned result;
+ 	    unsigned result_ecx;
+ 	} hyperresult; /* 12 bytes */
+ 	struct {
+ 	    unsigned long long cpu_freq;
+ 	    unsigned long long system_time;
+ 	    unsigned long wc_sec;
+ 	    unsigned long wc_usec;
+ 	    unsigned long long tsc_timestamp;
+ 	    unsigned version;
+ 	} domtime; /* 36 bytes */
+ 	struct {
+ 	    int port;
+             unsigned char flags;
+ 	} evtchn_set_pending; /* 5 bytes */
+ 	struct {
+ 	    unsigned long from,to;
+ 	} checkpoint; /* 8 bytes */
+ 	struct {
+ 	    unsigned long low, high;
+ 	} rdtsc; /* 8 bytes */
+         struct {
+ 	    int      tip;
+ 	} notify; /* 12 bytes */
+         struct {
+ 	    int      enable;
+ 	} bts_state; /* 12 bytes */
+ 	struct {
+ 	    unsigned long gpfn;
+             unsigned long event_count;
+ 	} crew_event; /* 8 bytes */
+ 	struct {
+ 	    unsigned actor; 
+ 	    crew_vector_t vector;
+ 	    unsigned long gpfn;
+             unsigned long cr2;
+             enum {
+                 CSRC_DELAY,
+                 CSRC_FAULT,
+                 CSRC_SHARED_INFO_GP,
+                 CSRC_SHARED_INFO_SPECIAL,
+                 CSRC_SHADOW_GRAB_GPFN,
+                 CSRC_BARRIER,
+                 CSRC_FLUSH_TLB,
+             } source:3;
+ 	} crew_constraint; /* 28 bytes + 3 bits*/
+ 	struct {
+ 	    enum {
+ 	       RCHECK_HYPERCALL,  /* 0 */
+ 	       RCHECK_PFAULT,
+ 	       RCHECK_TRAP,
+ 	       RCHECK_GP,
+ 	       RCHECK_MATH,
+ 	       RCHECK_INT3, /* 5 */
+ 	       RCHECK_DEBUG,
+ 	       RCHECK_HYPERCALL_POST,
+ 	       RCHECK_DA_EMULATION,
+ 	       RCHECK_SHADOW_MARK_OOS,
+                RCHECK_SHADOW_SYNC_LOCAL, /* 10 */
+                RCHECK_SHADOW_SYNC_ATTEMPT,
+                RCHECK_SHADOW_NORMAL_L1_PT_UPDATE,
+                RCHECK_SHADOW_NORMAL_L2_PT_UPDATE,
+                RCHECK_SHADOW_PROMOTE,
+                RCHECK_CREW_FLUSH_TLB,
+ 	    } where:4;
+ 	    unsigned long extra;
+ 	} register_check; /* 4 bytes + 4 bits */
+     } u; /* 57 bytes + ctxt size */
+ } record_event_t;
+ 
+ 
+ //#define REPLAY_LOCKDOMAIN      0
+ typedef struct {
+   /* IN parameters. */
+   unsigned short domain;
+ } replay_lockdomain_t;
+ 
+ //#define REPLAY_UNLOCKDOMAIN    1
+ typedef struct {
+   /* IN parameters. */
+   unsigned short domain;
+ } replay_unlockdomain_t;
+ 
+ typedef struct {
+   /* IN parameters. */
+   unsigned short domain;
+   int     vcpu;    // -1 for all vcpu's
+   int     enable;
+ } replay_bts_state_t;
+ 
+ typedef struct {
+     enum {
+ 	CREW_GRAB_GPFN,
+ 	CREW_RELEASE_GPFN,
+ 	CREW_GRAB_GREF,
+ 	CREW_RELEASE_GREF,
+ 	CREW_GRAB_EVENT,
+ 	CREW_RELEASE_EVENT,
+ 	CREW_RELEASE_ACTOR,
+ 	CREW_SET_VCPU_CONSTRAINT,
+ 	CREW_GRAB_BARRIER,
+ 	CREW_RELEASE_BARRIER,
+ 	CREW_FILTER_GPFN,
+     } cmd;
+     int actor;
+     int dom;
+     union {
+ 	struct {
+ 	    crew_vector_t * vector_p;
+ 	} grab_barrier;
+ 	struct {
+ 	    crew_vector_t * vector_p;
+ 	} release_barrier;
+ 	struct {
+ 	    int gpfn;
+ 	    int rw;
+ 	    crew_vector_t * vector_p;
+ 	} grab_gpfn;
+ 	struct {
+ 	    int gpfn;
+ 	    int rw;
+ 	} release_gpfn;
+ 	struct {
+ 	    int gref;
+ 	    int rw;
+ 	    crew_vector_t * vector_p;
+ 	} grab_gref;
+ 	struct {
+ 	    int gref;
+ 	    int rw;
+ 	} release_gref;
+ 	struct {
+ 	    /* event count is redundant; it's in the vector */
+ 	    crew_vector_t vector;
+ 	} grab_event;
+ 	struct {
+ 	} release_event;
+ 	struct {
+ 	    crew_vector_t vector;
+ 	} set_vcpu_constraint;
+         struct {
+ 	  unsigned long gpfn;
+         } filter_gpfn;
+     } u;
+     struct {
+ 	int dom;
+ 	int port;
+     } notify;
+ } replay_crew_op_t;
+ 
+ typedef enum {
+     QUEUE_CLEAN_LOGGED,
+     QUEUE_CLEAN_REPLAYED,
+     QUEUE_CLEAN_VERIFIED,
+     QUEUE_CLEAN_NEVER,
+     QUEUE_CLEAN_ALWAYS,
+ } clean_algorithm_t;
+ 
+ 
+ typedef struct {
+     enum {
+ 	REC_INIT,
+ 	REC_CLEAN_SWITCH
+     } cmd;
+     int dom;
+     union {
+ 	struct {
+ 	    int                port;     // notification port   (OUTPUT)
+ 	    unsigned long      ring_mfn; // mfn of the shared ring
+ 	    unsigned long      addr;     // start of the list
+ 	    unsigned long      len;      // number of mfns to read
+ 	    clean_algorithm_t  clean_algorithm;
+ 	} rec_init;
+ 	struct {
+ 	    clean_algorithm_t  clean_algorithm;
+ 	} rec_clean_switch;
+     } u;
+ } replay_record_op_t;
+ 
+ typedef struct {
+     enum {
+ 	REP_INIT,
+ 	REP_PUSH_RECORD,
+ 	REP_CONNECT
+     } cmd;
+     int dom;
+     union {
+ 	struct {
+ 	    int                port;       // notification port   (OUTPUT)
+             unsigned long      buf_vaddr;  // tip page for replay
+             unsigned long      list_vaddr; // start of the list
+             unsigned long      len;        // number of pages
+ 	} rep_init;
+ 	struct {
+ 	    record_event_t event;
+ 	} rep_push_record;
+ 	struct {
+ 	    int record_dom;
+ 	} rep_connect;
+     } u;
+ } replay_replay_op_t;
+ 
+ typedef struct {
+     unsigned long gpfn;
+     char * exec;
+     int len;
+ } replay_feedback_op_t;
+ 
+ typedef struct {
+     int dom;
+ } replay_poke_op_t;
+ 
+ typedef struct {
+   enum {
+     REPLAY_LOCKDOMAIN,
+     REPLAY_UNLOCKDOMAIN,
+     REPLAY_CREW_CMD,
+     REPLAY_BTS_UPDATE,
+     REPLAY_REC_CMD,
+     REPLAY_REP_CMD,
+     REPLAY_FEEDBACK_CMD,
+     REPLAY_POKE
+   }cmd;
+   union {
+     replay_lockdomain_t       lockdomain;
+     replay_unlockdomain_t     unlockdomain;
+     replay_crew_op_t	      crewop;
+     replay_record_op_t        recop;
+     replay_replay_op_t        repop;
+     replay_bts_state_t        bts_state;
+     replay_feedback_op_t      feedback;
+     replay_poke_op_t          poke;
+   } u;
+ } replay_op_t;
+ 
+ 
+ // Definition for per vcpu information.
+ //   we break down per vcpu to try to insure that
+ //   there won't be any contention between information in case
+ //   each vcpu ends upon a different processor...
+ struct tracer_exec_info {
+   // Event channel information if we need to notify on an event
+   long                    notify_port;
+   struct vcpu      *notify_domain;
+   
+   // Tracing information about which dom/exec_dom is being traced
+   unsigned      traced_domain;
+   unsigned      traced_vcpu;
+ 
+   pcounter_t    pcounter;
+ 
+   // Defines for the current state of the tracer  (needs work...)
+   unsigned long state;
+ #define       TI_LIVE          0xbeefcafe
+ #define       TI_DEAD          0x0
+ 
+   // Defines for what to do on an activation.
+   u_32           t_flags;
+ #define       TF_PAUSE        0
+ #define       TF_NOTIFY       1
+ 
+   // Current event's type
+   u_32           event;
+   u_32		count;
+ #define TRACE_EVENT_NONE      0
+ #define TRACE_EVENT_HYPERCALL 1
+ #define TRACE_EVENT_TBP       2
+ #define TRACE_EVENT_RDTSC     3
+   
+   union {
+     
+   } event_info;  
+ };
+ 
+ //
+ // Definition for the shared memory page.
+ //   If we want more than MAX_VCPUS, then we will need a multiple
+ //   pages.  May want to try to insure that each processor's info
+ //   maps to a different cache line?
+ //
+ // NOTE:  tip# 0 automatically gets notified for all synchronous events
+ //
+ 
+ #define MAX_VCPUS    (4088/sizeof(struct tracer_exec_info))
+ struct tracer_info_page {
+   unsigned                 tip_state;
+   #define                  TIP_LIVE          0xbeefcafe;
+   #define                  TIP_DEAD          0x0;
+ 
+   unsigned                 tip_num;
+   struct tracer_exec_info  exec_info[MAX_VCPUS];
+ };
+ 
+ #if 0
+ // Depracated...  Going...  Going...
+ struct tracer_page {
+     unsigned magic;
+     unsigned event;
+ #define TRACE_EVENT_HYPERCALL 1
+ #define TRACE_EVENT_TBP 2
+ #define TRACE_EVENT_RDTSC 3
+     unsigned dom;
+     unsigned call;
+     pcounter_t pcounter;
+ };
+ #endif
+ 
+ 
+ //TODO: Make the following used in both recording and replay.  
+ //      When events are queued up... ?
+ 
+ /* The replay ring consists of a four pointer ring of machine addresses
+    of replay buffers.  The buffers are then simple linear buffers of
+    events.
+ 
+    This somewhat complicated scheme is necessary because a simple ring
+    of events can't exceed 4k, which is only a few hundered events, and
+    we can't really get away with that little bandwidth.  At the same
+    time, each event is only a few dozen bytes, so giving each one its
+    own frame, as in the network or block rings, would be extremely
+    wasteful. */
+ 
+ /* Make a buffer just less than 4k. */
+ #define RECORD_EVENTS_PER_BUFFER (4084/sizeof(struct record_event))
+ struct record_event_buffer {
+     unsigned prod;
+     unsigned cons;
+     unsigned vcpu;
+     struct record_event events[RECORD_EVENTS_PER_BUFFER];
+ };
+ 
+ 
+ /* Rings are fairly simple: each entry is just an unsigned long giving
+    the mfn of the relevant buffer.  The logging domain is expected to
+    place empty buffers at empty_buffer_prod; Xen then takes them from
+    empty_buffer_cons when it needs them.  Xen produces full buffers at
+    full_buffer_prod, with the domain taking them off at
+    full_buffer_cons. */
+ 
+ /* Make a ring one page */
+ //#define RECORD_BUFFERS_PER_RING 1018
+ #define RECORD_BUFFERS_PER_RING 1010
+ struct record_event_ring {
+     unsigned full_buffer_prod, full_buffer_cons, full_buffer_clean;
+     unsigned empty_buffer_prod, empty_buffer_cons, empty_buffer_clean;
+     unsigned long buffers[RECORD_BUFFERS_PER_RING];
+ };
+ 
+ enum {
+     SHADOW_LOCK,
+     VCPU_WAIT_HYPERCALL_LOCK,
+     SHADOW_FAULT,
+     CREW_FAULT,
+     CREW_GET_PERM_1,
+     CREW_GET_PERM_2,
+     CREW_GET_PERM_3,
+     CREW_GET_PERM_4,
+     CREW_REMOVE_WRITE,
+     CREW_REMOVE_ALL,
+     CREW_LOG_LOCAL,
+     CREW_LOG_REMOTE,
+     CREW_LOG_DELAY,
+     PREPARE_EVENT,
+     PREPARE_EVENT_PAUSE,
+     PCTR_PCOUNTER_VAL,
+     POST_EVENT,
+     QUEUE_INSERT,
+     RECORD_COPY_INSERT,
+     QUEUE_SOFTIRQ,
+     QUEUE_UPDATE_LOG_WINDOW,
+     QUEUE_UPDATE_BUFFER_WINDOW,
+     QUEUE_CLEANER,
+     QUEUE_REPLAY_POKE,
+     QUEUE_GET_NEXT,
+     RECORD_COPY,
+     DOM0_SCHEDULE,
+     DOML_SCHEDULE,
+     USER_CLOCK_LOG_1,
+     USER_CLOCK_REPLAY_1,
+     USER_CLOCK_LOG_2,
+     USER_CLOCK_REPLAY_2,
+     DOM_MAX_CLOCKS
+ };
+ 
+ 
+ static const char *clock_names[DOM_MAX_CLOCKS]={
+     [SHADOW_LOCK]= "shadow_lock",
+     [VCPU_WAIT_HYPERCALL_LOCK]="waiting for hypercall lock",
+     [SHADOW_FAULT]= "shadow_fault",
+     [CREW_FAULT]= "CREW_FAULT",
+     [CREW_GET_PERM_1]= "crew.c:get_permission 1",
+     [CREW_GET_PERM_2]= "crew.c:get_permission 2",
+     [CREW_GET_PERM_3]= "crew.c:get_permission 3",
+     [CREW_GET_PERM_4]= "crew.c:get_permission 4",
+     [CREW_REMOVE_WRITE]= "preempt_all_read():remove_mappings_of_mfn",
+     [CREW_REMOVE_ALL]= "preempt_owner_write():remove_mappings_of_mfn",
+     [CREW_LOG_LOCAL]= "CREW_LOG_LOCAL",
+     [CREW_LOG_REMOTE]= "CREW_LOG_REMOTE",
+     [CREW_LOG_DELAY]= "CREW_LOG_DELAY",
+     [PREPARE_EVENT]="prepare_event()",
+     [PREPARE_EVENT_PAUSE]="vcpu_pause in prepare_event",
+     [PCTR_PCOUNTER_VAL]= "get_pcounter_val",
+     [POST_EVENT]="post_event()",
+     [QUEUE_INSERT]= "QUEUE_INSERT",
+     [RECORD_COPY]= "RecEvt Copy",
+     [RECORD_COPY_INSERT]= "RecEvt Copy(ins)",
+     [QUEUE_SOFTIRQ]= "QUEUE_SOFTIRQ",
+     [QUEUE_UPDATE_LOG_WINDOW]= "QUEUE_UPDATE_LOG_WINDOW",
+     [QUEUE_UPDATE_BUFFER_WINDOW]= "QUEUE_UPDATE_BUFFER_WINDOW",
+     [QUEUE_CLEANER]= "QUEUE_CLEANER",
+     [QUEUE_REPLAY_POKE]= "QUEUE_REPLAY_POKE",
+     [QUEUE_GET_NEXT]= "QUEUE_GET_NEXT",
+     [DOM0_SCHEDULE]="Domain 0",
+     [DOML_SCHEDULE]="Logging domain scheduled",
+     [USER_CLOCK_LOG_1]= "First (user, log)",
+     [USER_CLOCK_REPLAY_1]=    "First (user,rply)",    
+     [USER_CLOCK_LOG_2]= "Second(user, log)",
+     [USER_CLOCK_REPLAY_2]= "Second(user,rply)"
+ };
+ 
+ 
+ #if 0
+ #define CREW_REMOVE_WRITE 0
+ #define CREW_REMOVE_ALL 1
+ #define CREW_LOG_LOCAL 2
+ #define CREW_LOG_REMOTE 3
+ #define CREW_FAULT 4
+ #define QUEUE_INSERT 5
+ #define QUEUE_CLEANER 6
+ #define QUEUE_LOG_WINDOW 7
+ #define QUEUE_SOFTIRQ 8
+ #define QUEUE_GET_NEXT 9
+ #define QUEUE_UPDATE_LOG_WINDOW
+ #define RECORD_COPY 10
+ #define RECORD_COPY_INSERT 11
+ #define SHADOW_FAULT 12
+ #define PCTR_PCOUNTER_VAL 13
+ #endif
+ 
+ static const char *record_event_types[]={
+     [RECORD_EVT_NULL]="NULL",
+     [RECORD_EVT_HYPERRESULT]="HYPERRESULT",
+     [RECORD_EVT_DOMTIME]="DOMTIME",
+     [RECORD_EVT_EVTCHN_SET_PENDING]="EVTCHN_SET_PENDING",
+     [RECORD_EVT_CHECKPOINT]="CHECKPOINT",
+     [RECORD_EVT_RDTSC]="RDTSC",
+     [RECORD_EVT_HCALLBACK]="HCALLBACK",
+     [RECORD_EVT_NOTIFY]="NOTIFY",
+     [RECORD_EVT_CREW_EVENT]="CREW_EVENT",
+     [RECORD_EVT_CREW_CONSTRAINT]="CREW_CONSTRAINT",
+     [RECORD_EVT_BTS_STATE]="BTS_STATE",
+     [RECORD_EVT_UNPAUSE]="UNPAUSE",
+     [RECORD_EVT_REGISTER_CHECK]="REGISTER_CHECK",
+     [RECORD_EVT_SHADOW_SYNC]="SHADOW_SYNC",
+ };
+ 
+ static const char * constraint_source[] = {
+     [CSRC_DELAY]="DELAY",
+     [CSRC_FAULT]="FAULT",
+     [CSRC_SHARED_INFO_GP]="SI_GP",
+     [CSRC_SHARED_INFO_SPECIAL]="SI_SP",
+     [CSRC_SHADOW_GRAB_GPFN]="SGRAB",
+     [CSRC_BARRIER]="BARR",
+     [CSRC_FLUSH_TLB]="FLUSH",
+ };
+ 
+ static const char * register_check_where[] = {                            
+         [RCHECK_HYPERCALL]="HYPERCALL",
+         [RCHECK_PFAULT]="PFAULT",
+         [RCHECK_TRAP]="TRAP",
+         [RCHECK_GP]="GP",
+         [RCHECK_MATH]="MATH",
+         [RCHECK_INT3]="INT3",
+         [RCHECK_DEBUG]="DEBUG",
+         [RCHECK_HYPERCALL_POST]="HYPERCALL_POST",
+         [RCHECK_DA_EMULATION]="DA_EMULATION",
+         [RCHECK_SHADOW_MARK_OOS]="SHADOW_MARK_OOS",
+         [RCHECK_SHADOW_SYNC_LOCAL]="SHADOW_SYNC_LOCAL",
+         [RCHECK_SHADOW_SYNC_ATTEMPT]="SHADOW_SYNC_ATTEMPT",
+         [RCHECK_SHADOW_NORMAL_L1_PT_UPDATE]="SHADOW_NORMAL_L1_PT_UPDATE",
+         [RCHECK_SHADOW_NORMAL_L2_PT_UPDATE]="SHADOW_NORMAL_L2_PT_UPDATE",
+         [RCHECK_SHADOW_PROMOTE]="SHADOW_PROMOTE",
+         [RCHECK_CREW_FLUSH_TLB]="CREW_FLUSH_TLB",
+ };
+ static void useless(void) {
+     const char *str;
+     useless();
+     str=clock_names[0];
+     str=record_event_types[0];
+     str=register_check_where[0];
+     str=constraint_source[0];
+ }
+ 
+ #define assert_vcpu_not_running(_v)	\
+ 	ASSERT((!test_bit(_VCPUF_running, &_v->vcpu_flags)) \
+ 	       || (v->processor == smp_processor_id()))
+ 
+ #define REPLAY_BUFFER_LEN 1000
+ #define IS_REPLAY_TOOFULL(vvv)  (test_bit(_REPLAYFLAG_TOOFULL, &vvv->replay->replay_flags))
+ 
+ #define PCOUNTER_BREAK_THRESH 512
+ 
+ #endif /* __REPLAY_H__ */
+ 
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/xen/replay_queue.h new/xen-3.3.0/xen/include/xen/replay_queue.h
*** xen-3.3.0/xen/include/xen/replay_queue.h	1969-12-31 19:00:00.000000000 -0500
--- new/xen-3.3.0/xen/include/xen/replay_queue.h	2009-06-10 14:46:11.000000000 -0400
***************
*** 0 ****
--- 1,183 ----
+ #include <xen/replay.h>
+ #include <xen/spinlock.h>
+ #include <asm/system.h>
+ 
+ #ifndef REPLAY_QUEUE
+ #define REPLAY_QUEUE
+ // Use this #define to use the replay queue
+ #define USE_REPLAY_QUEUE
+ #define USE_REPLAY_QUEUE_RECORD
+ #define USE_REPLAY_QUEUE_REPLAY
+ 
+ #define MAX_PAGES 10000
+ 
+ #define EVENTS_PER_PAGE (4084/sizeof(struct record_event))
+ typedef unsigned long page_id_t;
+ 
+ // This is to be sized to take *exactly* 1 page.
+ #define EVENTS_PER_PAGE (4084/sizeof(struct record_event))
+ 
+ #define MAX_MAPPED         100
+ #define MAX_CP_ITERATORS    32
+ #define MAX_REPLAY_VCPUS     4
+ 
+ #define ITER_TO_PAGE_IDX(iter)      ( ( (iter->idx/EVENTS_PER_PAGE) % iter->q->num_pages ) )
+ #define ITER_TO_PAGE_OFFSET(iter)    ( (iter->idx % EVENTS_PER_PAGE) )
+ #define ITER_TO_PAGE_NUMBER(iter)    ( (iter->idx / EVENTS_PER_PAGE) )
+ 
+ struct replay_iterator;
+ struct replay_queue;
+ struct replay_checkpoint;
+ 
+ typedef struct replay_queue* replay_queue_t;
+ typedef struct replay_iterator* replay_iterator_t;
+ typedef struct replay_checkpoint replay_checkpoint_t;
+ 
+ typedef struct replay_page {
+     unsigned prod;  //used while logging    // iterators can perform these functions.
+     unsigned cons;  //used while replaying
+     unsigned vcpu;
+     record_event_t events[EVENTS_PER_PAGE];
+ } replay_page_t;
+ 
+ struct replay_iterator {
+     replay_queue_t  q;       // the queue which we are iterating
+     unsigned long long  idx;
+     unsigned int   version;
+ };
+ 
+ struct replay_checkpoint {
+     replay_iterator_t iterators[MAX_CP_ITERATORS]; // uses NEXT_XXX as index into list.
+ };
+ 
+ struct replay_queue{
+     spinlock_t       lock;
+     int              lock_flags;
+     int              lock_processor;
+     int              lock_line;
+     //atomic_t         lock;                       // lock within hypervisor
+ 
+     unsigned long    page_mfns[MAX_PAGES];       // kept in the order they were initialized in
+     int              num_pages;
+ 
+     replay_page_t   *mapped_pages[MAX_PAGES];    // NULL if unmapped, cleaner run when num_mapped gets too high.
+     int              num_mapped;                 // number of mappings (xen space taken up)
+ 
+     page_id_t          head_idx;
+     page_id_t          tail_idx;
+     replay_iterator_t  cleaned;                  // entries between tail and cleaned (after wrapping) are empty
+     replay_iterator_t  head;                     // start of queue 
+     replay_iterator_t  tail;                     // end of queue  (points one past the end)
+     replay_iterator_t  log_sent;                 // pointer to the next entry to be consumed by the logger
+     replay_iterator_t  log_finished;             // pointer to the last consumed entry. (log_sent==log_finished=> all sent are logged)
+     replay_iterator_t  replayed;                 // pointer to the most recently replayed event
+     replay_iterator_t  verified;                 // pointer to the most recently verified event
+     // NOTE: replayed is generated in the appropriate cleaner and can't be relied upon to be accurate outside of
+     //       the cleaner.
+     //       verified isn't currently implemented yet, but the verifier will likely do a batch update, something like:
+     //         queue_verified_chkpoint(verified_checkpoint)
+     //       at which point the cleaner will use that checkpoint to recreate verified and then clean up to the most
+     //       recent checkpoint. (again likely not current outside of the cleaner)
+     
+     
+     // Basically, you should always have...
+     //  cleaned <= head <= log_sent <= tail
+ 
+     unsigned int     version;                    // version of the queue (used to invalidate iterators)
+ 
+     int              buf_len;  // number of pages past the end of the queue to keep mapped in
+                                 // (in case of an interrupt, during which we can't map new pages)
+     int              bw_start; /* First page mapped in current buffer window */
+     int              bw_end;   /* Last page + 1 mapped by current buffer window */
+ 
+     //replay_checkpoint_t  cur_checkpoint;
+     
+     // Link back to the logging domain...
+     struct vcpu         *vcpu;
+ 
+     // Links back to replaying domains
+     struct vcpu         *replay_vcpu[MAX_REPLAY_VCPUS];
+     int                  num_replay_vcpus;
+ 
+     // sent to logger
+     unsigned             pages_sent;
+     clean_algorithm_t    clean_algorithm;
+ };
+ 
+ // insert/remove from queue...
+ replay_queue_t     queue_init(unsigned long *pages, int n_pages, struct vcpu *v);
+ 
+ long               queue_register_replay_vcpu(replay_queue_t rq, struct vcpu *v);
+ void               queue_soft_irq_handler(replay_queue_t rq);
+ void               queue_set_cleaner(replay_queue_t rq, clean_algorithm_t clean);
+ void               queue_begin(replay_queue_t, replay_iterator_t);
+ int                queue_insert(replay_queue_t, record_event_t *re);
+ void               queue_flush(replay_queue_t rq);
+ void               queue_pop_front(replay_queue_t );
+ unsigned long      queue_free(replay_queue_t);
+ unsigned long      queue_avail(replay_queue_t);
+ 
+ void               queue_erase_before(replay_iterator_t);
+ //void               queue_erase_after(replay_iterator_t);
+ 
+ void dump_cp(replay_checkpoint_t *cp);
+ void queue_dump(replay_queue_t);
+ 
+ #define        NEXT_DTIME        RECORD_EVT_DOMTIME
+ #define        NEXT_SPEND        RECORD_EVT_EVTCHN_SET_PENDING
+ #define        NEXT_RDTSC        RECORD_EVT_RDTSC
+ #define        NEXT_BREAK        RECORD_EVT_NOTIFY
+ #define        NEXT_DELIVER      RECORD_EVT_HCALLBACK
+ #define        NEXT_CREW         RECORD_EVT_CREW_EVENT
+ #define        NEXT_CREW_CONST   RECORD_EVT_CREW_CONSTRAINT
+ #define        NEXT_UNPAUSE      RECORD_EVT_UNPAUSE
+ 
+ #define        NEXT_ASYNC    20
+ 
+ // used for checkpointing?
+ #define        CONS_ASYNC    21
+ 
+ int queue_get_next(replay_queue_t, replay_checkpoint_t *c, int type, record_event_t *e);
+ replay_checkpoint_t *checkpoint_alloc(replay_queue_t rq);
+ 
+ static inline int cp_next_is_used(int next_type) {
+     return(next_type == NEXT_ASYNC);
+ }
+ 
+ static inline int queue_is_async(int record_evt_type) {
+     if (record_evt_type!=RECORD_EVT_NULL
+         && record_evt_type!=RECORD_EVT_RDTSC
+         && record_evt_type!=RECORD_EVT_CHECKPOINT
+ 	&& record_evt_type!=RECORD_EVT_REGISTER_CHECK) {
+ 	return 1;
+     }
+     /*if ((record_evt_type==RECORD_EVT_DOMTIME) ||
+ 	(record_evt_type==RECORD_EVT_EVTCHN_SET_PENDING) ||
+ 	(record_evt_type==RECORD_EVT_HCALLBACK) || 
+ 	(record_evt_type==RECORD_EVT_NOTIFY) ||
+ 	(record_evt_type==RECORD_EVT_CREW_EVENT) ||
+ 	(record_evt_type==RECORD_EVT_CREW_CONSTRAINT) ||
+ 	(record_evt_type==RECORD_EVT_BTS_STATE)) {
+ 	return 1;
+ 	}*/ 
+     return 0;
+ }
+ 
+ 
+ 
+ 
+ // Iterator manipulators
+ void               ri_new(replay_iterator_t*, replay_queue_t);
+ void               ri_delete(replay_iterator_t*);
+ int                ri_cmp(replay_iterator_t l, replay_iterator_t r);
+ int                ri_mapped(replay_iterator_t );
+ int                ri_valid(replay_iterator_t );
+ record_event_t     ri_value(replay_iterator_t );
+ void               ri_next(replay_iterator_t );
+ void               ri_prev(replay_iterator_t );
+ 
+ void __rq_lock(replay_queue_t rq, int line);
+ #define rq_lock(x) __rq_lock(x, __LINE__)
+ void rq_unlock(replay_queue_t rq);
+ 
+ #endif
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/xen/sched.h new/xen-3.3.0/xen/include/xen/sched.h
*** xen-3.3.0/xen/include/xen/sched.h	2008-08-22 05:49:10.000000000 -0400
--- new/xen-3.3.0/xen/include/xen/sched.h	2009-06-12 16:43:10.000000000 -0400
***************
*** 20,31 ****
--- 20,42 ----
  #include <xen/rcupdate.h>
  #include <xen/irq.h>
  
+ //for record and replay
+ #include <xen/replay.h>
+ #include <xen/replay_queue.h>
+ //#include <xen/sched.h>
+ 
  #ifdef CONFIG_COMPAT
  #include <compat/vcpu.h>
  DEFINE_XEN_GUEST_HANDLE(vcpu_runstate_info_compat_t);
  #endif
  
  /* A global pointer to the initial domain (DOM0). */
+ struct domian;
+ struct replay_vcpu_state;
+ struct vcpu;
+ struct replay_domain_state;
+ struct monitoring_domain_state;
+ 
  extern struct domain *dom0;
  
  #ifndef CONFIG_COMPAT
*************** struct evtchn
*** 66,74 ****
--- 77,303 ----
  #endif
  };
  
+ #define LOCK_BIGLOCK(_d) spin_lock_recursive(&(_d)->big_lock)
+ #define UNLOCK_BIGLOCK(_d) spin_unlock_recursive(&(_d)->big_lock)
+ 
+ //#define CONFIG_PAGE_OFFSET     0xC0000000
+ //#define __PAGE_OFFSET		CONFIG_PAGE_OFFSET
+ #define PAGE_OFFSET		((unsigned long)__PAGE_OFFSET)
+ 
+ #define DPRINTFMAX printk
+ 
  int  evtchn_init(struct domain *d);
  void evtchn_destroy(struct domain *d);
  
+ 
+ #define MAX_REPLAY_DOMAINS   4
+ 
+ typedef struct replay_set {
+     // Link back to the logging domain (if any)
+     struct domain       *logging_domain;
+     
+     // Links back to other replaying domains (if any) (includes self)
+     struct domain       *replaying_domains[MAX_REPLAY_DOMAINS];
+     int                  num_replay_domains;
+ } replay_set_t;
+ 
+ struct replay_domain_state{
+ 	//replay states(domain level)
+ 	unsigned long replay_flags;
+ 	//vcpu_id+1
+ 	int hypercall_lock_owner;
+ 	unsigned long *mfn_list, *list_start;
+ 	int pervcpu_len,pervcpu_extra;
+ 	unsigned list_len;
+    
+ 	//The set of logging and replaying domains which are linked together
+         replay_set_t *replay_set;
+ };
+ 
+ #define _REPLAYFLAG_RUNNABLE               0
+ #define REPLAYFLAG_RUNNABLE                (1UL<<_REPLAYFLAG_RUNNABLE)
+ #define _REPLAYFLAG_EMPTY                  1
+ #define REPLAYFLAG_EMPTY                   (1UL<<_REPLAYFLAG_EMPTY)
+ #define _REPLAYFLAG_SYNC                   2
+ #define REPLAYFLAG_SYNC                    (1UL<<_REPLAYFLAG_SYNC)
+ 
+ 
+ // Used to lock-step an event at a time
+ #define _REPLAYFLAG_EVENT_STEPPING         3
+ #define REPLAYFLAG_EVENT_STEPPING          (1UL<<_REPLAYFLAG_EVENT_STEPPING)
+ // Used to lock-step an instruction at a time
+ #define _REPLAYFLAG_SINGLE_STEPPING        4
+ #define REPLAYFLAG_SINGLE_STEPPING         (1UL<<_REPLAYFLAG_SINGLE_STEPPING)
+ // Set if we validating the state of the replaying/logging machine
+ #define _REPLAYFLAG_VALIDATING             5
+ #define REPLAYFLAG_VALIDATING              (1UL<<_REPLAYFLAG_VALIDATING)
+ // Set when the logging domain has reached a point where it needs validated.  
+ //   This implies that the logging domain has been paused and is in a validatable state
+ // Cleared when the validation has completed.
+ #define _REPLAYFLAG_NEEDS_VALIDATED        6
+ #define REPLAYFLAG_NEEDS_VALIDATED         (1UL<<_REPLAYFLAG_NEEDS_VALIDATED)
+ // Set when the replaying domain has reached the same execution point as the logging domain
+ // Cleared when the validation has completed.
+ #define _REPLAYFLAG_READY_FOR_VALIDATION   7
+ #define REPLAYFLAG_READY_FOR_VALIDATION    (1UL<<_REPLAYFLAG_READY_FOR_VALIDATION)
+ // Set when we have verified that the state is identical to the logging domain
+ // Cleared when the set of domains are unpaused
+ #define _REPLAYFLAG_VALIDATED              8
+ #define REPLAYFLAG_VALIDATED               (1UL<<_REPLAYFLAG_VALIDATED)
+ // Set when the logging domain is waiting for the replaying domains to catch up
+ // Cleared when the replaying domains have caught up
+ #define _REPLAYFLAG_WAITING_FOR_REPLAY     9
+ #define REPLAYFLAG_WAITING_FOR_REPLAY      (1UL<<_REPLAYFLAG_WAITING_FOR_REPLAY)
+ // Set when the replaying domain has reached the same execution point as the logging domain (& not validating)
+ // Cleared when the set of domains are unpaused
+ #define _REPLAYFLAG_WAITING_FOR_LOGGING    10
+ #define REPLAYFLAG_WAITING_FOR_LOGGING     (1UL<<_REPLAYFLAG_WAITING_FOR_LOGGING)
+ // Set when the queue is too full and the logging system has been paused as a result
+ #define _REPLAYFLAG_TOOFULL                11
+ #define REPLAYFLAG_TOOFULL                 (1UL<<_REPLAYFLAG_TOOFULL)
+ // Set on all replaying vcpu's when we pause the logging domain and insert a need_unpause event
+ #define _REPLAYFLAG_WAITING_FOR_UNPAUSE    12
+ #define REPLAYFLAG_WAITING_FOR_UNPAUSE     (1UL<<_REPLAYFLAG_WAITING_FOR_UNPAUSE)
+ #define _REPLAYFLAG_INIT    13
+ #define REPLAYFLAG_INIT     (1UL<<_REPLAYFLAG_INIT)
+ #define _REPLAYFLAG_CREW_WAIT    14
+ #define REPLAYFLAG_CREW_WAIT     (1UL<<_REPLAYFLAG_CREW_WAIT)
+ #define _REPLAYFLAG_CREW_WOKEN   15
+ #define REPLAYFLAG_CREW_WOKEN     (1UL<<_REPLAYFLAG_CREW_WOKEN)
+ /* Set to indicate that a domtime should happen in softirq time */
+ #define _REPLAYFLAG_DELAY_DOMTIME   16
+ #define REPLAYFLAG_DELAY_DOMTIME     (1UL<<_REPLAYFLAG_DELAY_DOMTIME)
+ #define _REPLAYFLAG_DELAY_SPEND    17
+ #define REPLAYFLAG_DELAY_SPEND     (1UL<<_REPLAYFLAG_DELAY_SPEND)
+ #define _REPLAYFLAG_IN_SHADOW_FAULT    18
+ #define REPLAYFLAG_IN_SHADOW_FAULT     (1UL<<_REPLAYFLAG_IN_SHADOW_FAULT)
+ #define _REPLAYFLAG_PEER_TOOFULL    19
+ #define REPLAYFLAG_PEER_TOOFULL     (1UL<<_REPLAYFLAG_PEER_TOOFULL)
+ #define _REPLAYFLAG_HYPERCALL_LOCK    20
+ #define REPLAYFLAG_HYPERCALL_LOCK     (1UL<<_REPLAYFLAG_HYPERCALL_LOCK)
+ #define _REPLAYFLAG_DELAY_CONSTRAINT    21
+ #define REPLAYFLAG_DELAY_CONSTRAINT     (1UL<<_REPLAYFLAG_DELAY_CONSTRAINT)
+ #define _REPLAYFLAG_DELAY_EVENT    22
+ #define REPLAYFLAG_DELAY_EVENT     (1UL<<_REPLAYFLAG_DELAY_EVENT)
+ #define _REPLAYFLAG_DELAY_SECTION    23
+ #define REPLAYFLAG_DELAY_SECTION     (1UL<<_REPLAYFLAG_DELAY_SECTION)
+ #define _REPLAYFLAG_WAITING_FOR_LOCK    24
+ #define REPLAYFLAG_WAITING_FOR_LOCK     (1UL<<_REPLAYFLAG_WAITING_FOR_LOCK)
+ 
+ 
+ /*
+  * Per-VCPU flags (vcpu_flags).
+  */
+  /* Has the FPU been initialised? */
+ #define _VCPUF_fpu_initialised 0
+ #define VCPUF_fpu_initialised  (1UL<<_VCPUF_fpu_initialised)
+  /* Has the FPU been used since it was last saved? */
+ #define _VCPUF_fpu_dirtied     1
+ #define VCPUF_fpu_dirtied      (1UL<<_VCPUF_fpu_dirtied)
+  /* Domain is blocked waiting for an event. */
+ #define _VCPUF_blocked         3
+ #define VCPUF_blocked          (1UL<<_VCPUF_blocked)
+  /* Domain is paused by controller software. */
+ #define _VCPUF_ctrl_pause      4
+ #define VCPUF_ctrl_pause       (1UL<<_VCPUF_ctrl_pause)
+  /* Currently running on a CPU? */
+ #define _VCPUF_running         5
+ #define VCPUF_running          (1UL<<_VCPUF_running)
+  /* Disables auto-migration between CPUs. */
+ #define _VCPUF_cpu_pinned      6
+ #define VCPUF_cpu_pinned       (1UL<<_VCPUF_cpu_pinned)
+  /* Domain migrated between CPUs. */
+ #define _VCPUF_cpu_migrated    7
+ #define VCPUF_cpu_migrated     (1UL<<_VCPUF_cpu_migrated)
+  /* Initialization completed. */
+ #define _VCPUF_initialised     8
+ #define VCPUF_initialised      (1UL<<_VCPUF_initialised)
+  /* VCPU is not-runnable */
+ #define _VCPUF_down            9
+ #define VCPUF_down             (1UL<<_VCPUF_down)
+ 
+ 
+ // for record and replay, replay flags
+ #define _VCPUF_monitored 16
+ #define _VCPUF_monitored   16 /* Monitor timings of return to userspace */
+ #define _VCPUF_replaying   18 /* In progress of replaying this domain   */
+ #define _VCPUF_in_hypercall   19 /* Domain in process of doing a hypercall */
+ #define _VCPUF_bts         20 /* Domain is being branch-traced(copy of domain_flags) */
+ #define _VCPUF_request_perfctr_init  22 /* Initialize the performance counter on next sched */
+ #define _VCPUF_branch_trap        23 /* If TF set, vcpu traps only on branches  */
+ #define _VCPUF_in_exception_handler 24 /* vcpu in process of handling exception */
+ 
+ /* OK, this is annoying...  We want these flags available in entry.S.
+  * And we also want to specify "UL" in .c files to make sure we have
+  * the right type/ enough bits.
+  * However, UL causes compilation errors.  So, define it twice...
+  */
+ #ifdef FROM_ASSEMBLY
+ #define VCPUF_monitored      (1<<_VCPUF_monitored)
+ #define VCPUF_replaying      (1<<_VCPUF_replaying)
+ #define VCPUF_in_hypercall (1<<_VCPUF_in_hypercall)
+ #define VCPUF_bts      (1<<_VCPUF_bts)
+ #define VCPUF_request_perfctr_init      (1<<_VCPUF_request_perfctr_init)
+ #define VCPUF_branch_trap      (1<<_VCPUF_branch_trap)
+ #define VCPUF_in_exception_handler (1<<_VCPUF_in_exception_handler)
+ #else
+ #define VCPUF_monitored      (1UL<<_VCPUF_monitored)
+ #define VCPUF_replaying      (1UL<<_VCPUF_replaying)
+ #define VCPUF_in_hypercall (1UL<<_VCPUF_in_hypercall)
+ #define VCPUF_bts      (1UL<<_VCPUF_bts)
+ #define VCPUF_request_perfctr_init      (1UL<<_VCPUF_request_perfctr_init)
+ #define VCPUF_branch_trap      (1UL<<_VCPUF_branch_trap)
+ #define VCPUF_in_exception_handler (1UL<<_VCPUF_in_exception_handler)
+ #endif
+ 
+ #define IDLE_DOMAIN_ID   (0x7FFFU)
+ #define is_idle_task(_d) (test_bit(_DOMF_idle_domain, &(_d)->domain_flags))
+ 
+ struct bts_record {
+     unsigned long from;
+     unsigned long to;
+     unsigned long flags;
+ };
+ #define NR_TRACE_RECORDS 2048
+ #define DEFAULT_THRESH 128
+ //#define NR_TRACE_RECORDS 1365
+ struct bts_area {
+     struct bts_record data[NR_TRACE_RECORDS];
+ };
+ 
+ struct ds_area {
+     struct bts_record *bts_base;	/* Actual start of bts_area */
+     struct bts_record *bts_index; 	/* Next bts_record to write */
+     struct bts_record *bts_max;
+     struct bts_record *bts_thresh;
+     void *pebs_base;
+     unsigned long pebs_index;
+     void *pebs_maximum;
+     void *pebs_intr_thresh;
+     unsigned long pebs_reset1;
+     unsigned char pebs_reset2;
+ } __attribute__((packed));
+ 
+ struct vcpu_bts_data {
+     unsigned enabled:1;
+     struct ds_area *ds;
+     struct bts_area *bts;
+ };
+ 
+ struct vcpu_perfctr_data
+ {
+ 	unsigned counter_enabled:1;
+ 	unsigned break_enabled:1;
+ 	long long sched_start;
+ 	unsigned long long consumed;
+ 	unsigned long long extra;
+ 	unsigned long long break_val;
+ 	void (*callback)(struct vcpu *, void *);
+ 	void *cb_data;
+ };
+ 
+ #define NUM_NOTIFY_ENTRIES 32
+ 
  struct vcpu 
  {
      int              vcpu_id;
*************** struct vcpu 
*** 150,155 ****
--- 379,394 ----
      cpumask_t        vcpu_dirty_cpumask;
  
      struct arch_vcpu arch;
+ 	
+     //for record and replay
+         struct replay_vcpu_state *replay;
+ 	unsigned long vcpu_flags;
+ 	struct vcpu_perfctr_data perfctrs;
+ 	struct vcpu_bts_data bts;
+ 	atomic_t pausecnt;
+ 	atomic_t activated_events;
+ 	struct list_head  event_list;
+ 	struct tracer_exec_info *exec_info[NUM_NOTIFY_ENTRIES];  // should be per domain, not vcpu?
  };
  
  /* Per-domain lock can be recursively acquired in fault handlers. */
*************** struct vcpu 
*** 157,162 ****
--- 396,460 ----
  #define domain_unlock(d) spin_unlock_recursive(&(d)->domain_lock)
  #define domain_is_locked(d) spin_is_locked(&(d)->domain_lock)
  
+ struct monitoring_domain_state{
+ 	spinlock_t lock;
+ 	struct record_event_ring *ring;
+ 	struct record_event_buffer *cur_buffer, *next_buffer;
+ 	unsigned long cur_buffer_mfn, next_buffer_mfn;
+ 	
+ 	struct record_event_buffer *mapped[RECORD_BUFFERS_PER_RING];
+ 	unsigned long buffer_mfn[RECORD_BUFFERS_PER_RING];
+ 	
+ 	struct domain *logger_domain;
+ 	struct vcpu *vcpu;
+ 	int port;
+ 	
+ 	int init;
+ };
+ 
+ #define BTS_HISTORY_PAGE_ORDER 3
+ /* >>3 == /8 */
+ #define BTS_HISTORY_SIZE ((((4096<<BTS_HISTORY_PAGE_ORDER)-8))>>3)
+ #define bts_history_unconsumed(_v) \
+ 	((_v)->replay->bts->prod - (_v)->replay->bts->cons)
+ #define bts_history_free(__v) \
+ 	(BTS_HISTORY_SIZE - bts_history_unconsumed(__v))
+ 
+ #define bts_history_cons(_v)  \
+ 	((_v)->replay->bts->record[(_v)->replay->bts->cons % BTS_HISTORY_SIZE])
+ #define bts_history_prod(_v)  \
+ 	((_v)->replay->bts->record[(_v)->replay->bts->prod % BTS_HISTORY_SIZE])
+ #define bts_history_index(_v, _i) \
+ 	((_v)->replay->bts->record[_i % BTS_HISTORY_SIZE])
+ 
+ struct bts_history {
+     unsigned long prod, cons;
+     struct {
+ 	unsigned long from, to;
+     } record[BTS_HISTORY_SIZE];
+ };
+ 
+ struct replay_vcpu_state{
+     	
+ 	unsigned long replay_flags;
+ 
+     	unsigned long log_pending;
+ 
+      	struct vcpu  *pause_vcpu;
+ 	struct time_location pause_time;
+ 	
+         //the last time record insertion
+ 	struct time_location last_time;
+ 	
+ 	//replay queue if it is active
+ 	struct replay_queue *rq;
+ 	
+ 	replay_checkpoint_t *cur_checkpoint;
+ 	
+ 	struct bts_history *bts;
+     	int needs_poke;
+ };
+ 
  struct domain
  {
      domid_t          domain_id;
*************** struct domain
*** 229,235 ****
      int              suspend_evtchn;
  
      atomic_t         pause_count;
! 
      unsigned long    vm_assist;
  
      atomic_t         refcnt;
--- 527,533 ----
      int              suspend_evtchn;
  
      atomic_t         pause_count;
!  
      unsigned long    vm_assist;
  
      atomic_t         refcnt;
*************** struct domain
*** 260,265 ****
--- 558,570 ----
  
      /* VRAM dirty support. */
      struct sh_dirty_vram *dirty_vram;
+ 
+     /* struct and data for record and replay*/
+     unsigned long domain_flags;
+     struct replay_domain_state  *replay;
+     struct monitoring_domain_state *monitor;
+     spinlock_t big_lock;
+     atomic_t replay_lock; //xen/domian.c/arch_domain_create  init
  };
  
  struct domain_setup_info
*************** extern struct vcpu *idle_vcpu[NR_CPUS];
*** 298,303 ****
--- 603,623 ----
   * Use this when you don't have an existing reference to @d. It returns
   * FALSE if @d is being destroyed.
   */
+ 
+ static inline int domain_runnable(struct vcpu *v)
+ {
+     return !( ( atomic_read(&v->pausecnt) ) 
+               || (v->vcpu_flags & (VCPUF_blocked|VCPUF_ctrl_pause|VCPUF_down))
+ 	      || (v->domain->domain_flags & (DOMF_shutdown|DOMF_shuttingdown))
+ 	      || ( ( v->vcpu_flags & (VCPUF_monitored|VCPUF_replaying) )
+ 	           && ( v->replay ) 
+ 		   && ( ( v->replay->replay_flags & (REPLAYFLAG_WAITING_FOR_UNPAUSE|REPLAYFLAG_TOOFULL|REPLAYFLAG_PEER_TOOFULL) )   
+ 		       || ! (v->replay->replay_flags & REPLAYFLAG_RUNNABLE) ) )
+ 
+ 	      || ( (v->vcpu_flags & VCPUF_replaying) && !atomic_read(&v->activated_events) ) 
+ 	    );
+ }
+ 
  static always_inline int get_domain(struct domain *d)
  {
      atomic_t old, new, seen = d->refcnt;
*************** extern enum cpufreq_controller {
*** 547,553 ****
  } cpufreq_controller;
  
  #endif /* __SCHED_H__ */
- 
  /*
   * Local variables:
   * mode: C
--- 867,872 ----
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/xen/softirq.h new/xen-3.3.0/xen/include/xen/softirq.h
*** xen-3.3.0/xen/include/xen/softirq.h	2008-08-22 05:49:10.000000000 -0400
--- new/xen-3.3.0/xen/include/xen/softirq.h	2009-06-08 10:28:34.000000000 -0400
*************** enum {
*** 10,15 ****
--- 10,18 ----
      RCU_SOFTIRQ,
      STOPMACHINE_SOFTIRQ,
      TASKLET_SOFTIRQ,
+     DELAYED_MONITOR_EVENTS_SOFTIRQ,
+     BTS_SOFTIRQ,
+     
      NR_COMMON_SOFTIRQS
  };
  
diff --show-c-function --exclude='*.o' --exclude='*.ko' --exclude='*.a' --exclude='*.map' --exclude='*.cmd' --exclude='*.mod' --exclude='*.s' --exclude='*.S' --exclude=System.map-2.6.18.8-xen --exclude='*.bak' --exclude='*~' --exclude='*.gz' --unidirectional-new-file -rc xen-3.3.0/xen/include/xen/timer.h new/xen-3.3.0/xen/include/xen/timer.h
*** xen-3.3.0/xen/include/xen/timer.h	2008-08-22 05:49:10.000000000 -0400
--- new/xen-3.3.0/xen/include/xen/timer.h	2009-06-10 17:20:36.000000000 -0400
***************
*** 11,16 ****
--- 11,98 ----
  #include <xen/spinlock.h>
  #include <xen/time.h>
  #include <xen/string.h>
+ #include <xen/replay.h>
+ 
+ #define MAX_CLOCK_CPUS 4
+ /*
+ extern struct domain *dom0;
+ typedef unsigned long long u_64;
+ extern int clocks_initialized;
+ extern u_64 clock_cpu_freq;
+ */
+ extern int user_active;
+ extern unsigned long long clock_cpu_freq;
+ 
+ typedef unsigned long long u_64;
+ struct clock{
+ 	u_64 sum;
+ 	unsigned long count;
+ 	u_64 rdtsc_start;
+ 	u_64 rdtsc_stop;
+ 	atomic_t startcount;
+ };
+ 
+ extern struct clock clocks[DOM_MAX_CLOCKS][MAX_CLOCK_CPUS];
+ 
+ static inline
+ struct clock * get_clock(int name) {
+     ASSERT(smp_processor_id() <= MAX_CLOCK_CPUS);
+     ASSERT(name <= DOM_MAX_CLOCKS);
+     return &clocks[name][smp_processor_id()];
+ }
+ 
+ /*
+ void init_clocks(void)
+ static inline
+ void init_clocks() {
+     memset(clocks,0,sizeof(struct clock)*DOM_MAX_CLOCKS*MAX_CLOCK_CPUS);
+     clock_cpu_freq=dom0->shared_info->cpu_freq;
+ }
+ 
+ static inline
+ void stop_clock(int name) {
+     struct clock * d=get_clock(name);
+     if(name >= USER_CLOCK_LOG_1)
+         user_active--;
+ 
+     atomic_dec(&d->startcount);
+     d->count++;
+     
+     if(d->rdtsc_start != -1 && atomic_read(&d->startcount)==0) {
+         rdtscll(d->rdtsc_stop);
+         d->sum += d->rdtsc_stop - d->rdtsc_start;
+     }
+ 
+     if(name >= USER_CLOCK_LOG_1 && !user_active) {
+ 	 dump_clocks('C');
+     }
+ }
+ */
+ static inline
+ void start_clock(int name) {
+ /*    if (!clocks_initialized) {
+ 	init_clocks();
+ 	clocks_initialized=1;
+     }
+     struct clock * d=get_clock(name);;
+     if(name >= USER_CLOCK_LOG_1) 
+         user_active++;
+  
+     rdtscll(d->rdtsc_start);
+     
+     if((name >= USER_CLOCK_LOG_1) && (user_active == 1)) {
+       clear_clocks(d->rdtsc_start);
+     }
+ 
+     atomic_inc(&d->startcount);
+     d->count++;
+     
+     if(!user_active)
+       d->rdtsc_start = -1;
+ */
+ }
+ 
+ 
  
  struct timer {
      /* System time expiry value (nanoseconds since boot). */
*************** DECLARE_PER_CPU(s_time_t, timer_deadline
*** 108,113 ****
--- 190,221 ----
  /* Arch-defined function to reprogram timer hardware for new deadline. */
  extern int reprogram_timer(s_time_t timeout);
  
+ static inline
+ void dump_clock(int name) {
+   int i;
+   for(i=0; i<MAX_CLOCK_CPUS; i++) {
+     struct clock *d=&clocks[name][i];
+     if(!d->count)
+       continue;
+     if((d->count/2)) {
+       if(d->count%2 && d->rdtsc_start != -1) {
+ 	rdtscll(d->rdtsc_stop);
+ 	d->sum += d->rdtsc_stop - d->rdtsc_start;
+ 	d->rdtsc_start = d->rdtsc_stop;
+       }
+       printk("%20s cpu %d: %lld %ld %lld %s   (%lld.%03lld.%03llds)\n", 
+ 	     clock_names[name],
+ 	     i,
+ 	     d->sum, d->count/2,
+ 	     (d->sum/(d->count/2)),
+ 	     ((d->count%2)?"ACTIVE":""),
+ 	     d->sum/clock_cpu_freq,
+ 	     (d->sum*1000/clock_cpu_freq)%1000,
+ 	     (d->sum*1000000/clock_cpu_freq)%1000);
+     }
+   }
+ }
+ 
  #endif /* _TIMER_H_ */
  
  /*
